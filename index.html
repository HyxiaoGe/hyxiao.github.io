<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-夯实基础了解LLm大语言模型" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/04/23/%E5%A4%AF%E5%AE%9E%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3LLm%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time class="dt-published" datetime="2025-04-23T03:02:04.031Z" itemprop="datePublished">2025-04-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>大语言模型(Large Language Model, LLM)是一种基于深度学习的人工智能模型，能够理解和生成人类语言。它通过海量文本数据训练，可以执行对话、写作、翻译等多种自然语言处理任务。</p>
<h1 id="企业价值与市场需求"><a href="#企业价值与市场需求" class="headerlink" title="企业价值与市场需求"></a>企业价值与市场需求</h1><ul>
<li><p>提升效率</p>
<ul>
<li>自动化客户服务和支持</li>
<li>加速内容创作和文档处理</li>
</ul>
</li>
<li><p>降低成本</p>
<ul>
<li>减少人工处理时间</li>
<li>优化资源配置</li>
</ul>
</li>
<li><p>创新机遇</p>
<ul>
<li>开发新的产品和服务</li>
<li>改善用户体验</li>
</ul>
</li>
</ul>
<h1 id="企业落地案例"><a href="#企业落地案例" class="headerlink" title="企业落地案例"></a>企业落地案例</h1><ul>
<li><p>智能客服</p>
<ul>
<li>24&#x2F;7全天候服务</li>
<li>多语言支持</li>
</ul>
</li>
<li><p>内容生成</p>
<ul>
<li>营销文案创作</li>
<li>产品描述撰写</li>
</ul>
</li>
<li><p>知识管理</p>
<ul>
<li>智能文档分析</li>
<li>自动报告生成</li>
</ul>
</li>
</ul>
<h1 id="大语言模型的工作流程"><a href="#大语言模型的工作流程" class="headerlink" title="大语言模型的工作流程"></a>大语言模型的工作流程</h1><p><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/4d514fab-2492-4877-a269-a017b8992bb6/fd26e416-cc3e-44a7-b538-c284bc08aed9/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466SUWPQFS5/20250214/us-west-2/s3/aws4_request&X-Amz-Date=20250214T120615Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAIaCXVzLXdlc3QtMiJHMEUCIAldq/0bde78FsAysjUIAkdp1ziZwOYYXe4PhmbPS9urAiEAx3RmP33hXfIy2e7AA9LYkly9vT/L7W7alwcnOE4e+bIq/wMIKxAAGgw2Mzc0MjMxODM4MDUiDL6ykPBZM9Ny5QqnwyrcA7fWkX0wZJIP7yeb1XBO8p+2ZAAkR0K2R5rxtFjmdjJ/BAufEhKO0P1ZUfqMvhrrjKrMLOzwStyuu5wQXcdSF+yL7KdiBH8+v9CV+APRAxK0Sg3WSbeBeD76XPQ7FAYzcTBzrL+iL9ABX+eCYEhzhnb2EHX/IQRFphptWfs+slEyjb4xiQD1RAqtZLcanjZRt/k/MnTWdNZA4kp/VBdBlWKwSZ243JW5AEVgNR3nCjZGX5+r7p1H4XmxUoPmDJ89x/bqHPBGc7+mSlQrwMPUMa1k/Vd5+9Qe2KUHQC4Qu3/ViTiUmkg8pIerIzpZQgYpxSXPBdLpME2NFDbB/Ssk7b+DZqBavb6BlDU3F+X5vjEnLo81wAtefK5UmzS/K6DhMBr7s4Nfsa3Z5BEQfsjUYxbRXJR3q9jvoHyuPoQMTp89Pf5KfOGQX3DHxuz3t2q8liCB11PxR2UcwNe6bkTIpZERhYlgY2XWbNDnPrh5ANo6PaT0elw9lnqpmo0caKFHIXf7tE+MDikuLUQ8K/U6YoWDFLjwGbtSPb8RamZ8ovxQZaboyCilAa0m7ZLqymTXXblVvQQ7RWrsUB8hWhtxYOjMXoW85F9PfilPERmffriz0G7D8DF5g6/wb4jwMPiuvL0GOqUBCWJa4JX/QS5icwaCGW7Bnjn7BmHXDL72iiYzaPztsDg6KGxAZUFA0yZiPPZjiMpKEriOHgt1bav8BBv+F1unrWSMc25AKB5iHPTIDF23rSeTONIePdCEd/UwgmxBcnmK8F2ktoTYtPU/kLkPDKHa+/SjV1DOJfpqfCDOFBABWp0X89z1zsrShSyklgZ4j7BJvuDBGmv2ESLd0SzxHI+TQKnaw/59&X-Amz-Signature=8fd02fdc0fe183e61fe368398b551deefe5ed999faa0d68fdedcb73b2cf65c13&X-Amz-SignedHeaders=host&x-id=GetObject" alt="image"></p>
<p>大语言模型的工作流程主要包含以下几个关键步骤：</p>
<ol>
<li><p>数据预处理</p>
<ul>
<li>收集和清洗训练数据</li>
<li>文本标准化和分词</li>
<li>构建训练数据集</li>
</ul>
</li>
<li><p>模型训练</p>
<ul>
<li>预训练阶段：通过大规模文本学习语言知识</li>
<li>微调阶段：针对特定任务进行优化</li>
<li>参数调整和验证</li>
</ul>
</li>
<li><p>推理过程</p>
<ul>
<li>接收用户输入</li>
<li>文本编码和处理</li>
<li>生成响应结果</li>
</ul>
</li>
<li><p>输出优化</p>
<ul>
<li>结果过滤和优化</li>
<li>安全检查</li>
<li>格式化输出</li>
</ul>
</li>
</ol>
<p>这个工作流程是一个循环迭代的过程，通过持续的优化和改进来提升模型性能。每个步骤都需要严格的质量控制和监督，以确保模型输出的准确性和可靠性。</p>
<h1 id="大语言模型的Token预测机制"><a href="#大语言模型的Token预测机制" class="headerlink" title="大语言模型的Token预测机制"></a>大语言模型的Token预测机制</h1><p>大语言模型通过预测下一个可能出现的token来生成文本。这个过程可以分为以下几个关键步骤：</p>
<ol>
<li><p>输入处理</p>
<ul>
<li>将输入文本分割成token序列</li>
<li>对token进行编码转换为向量</li>
<li>建立上下文关系</li>
</ul>
</li>
<li><p>概率计算</p>
<ul>
<li>计算每个可能token的出现概率</li>
<li>使用注意力机制分析token间关系</li>
<li>考虑历史上下文信息</li>
</ul>
</li>
<li><p>token选择</p>
<ul>
<li>基于统计，通过大量数据的统计，选择最合适的token</li>
<li>应用采样策略（如温度参数调节）</li>
<li>控制生成的多样性和创造性</li>
</ul>
</li>
</ol>
<p>这种基于概率的token预测机制使得大语言模型能够生成连贯、符合语境的文本内容。通过不断预测下一个token，模型可以逐步构建出完整的响应。</p>
<h1 id="关键概念解释"><a href="#关键概念解释" class="headerlink" title="关键概念解释"></a>关键概念解释</h1><p>以下是大语言模型领域的一些重要概念：</p>
<p><strong>AIGC (AI Generated Content)</strong>：人工智能生成内容，指利用AI技术自动创作文本、图像、音频、视频等各类数字内容的技术和过程。它代表了内容创作的新范式，可以大规模、快速地生成高质量内容。</p>
<p><strong>AGI (Artificial General Intelligence)</strong>：通用人工智能，指具有与人类相似的通用智能，能够理解、学习和应用知识到各种不同任务的AI系统。这是人工智能发展的终极目标之一，目前尚未实现。</p>
<p><strong>Agent</strong>：智能代理，是一种能够自主执行任务、做出决策并与环境交互的AI系统。它可以理解用户意图，规划行动步骤，并通过调用各种工具和API来完成复杂任务。</p>
<p><strong>Prompt</strong>：提示词，是用户输入给AI模型的指令或上下文信息。好的prompt设计能够引导模型生成更准确、更符合需求的输出。这是与AI模型交互的关键接口。</p>
<p><strong>GPT (Generative Pre-trained Transformer)</strong>：生成式预训练转换器，是一种基于Transformer架构的大规模语言模型。它通过自监督学习在海量文本上预训练，能够理解和生成人类语言。</p>
<p><strong>Token</strong>：词元，是文本被分割成的最小单位，可能是单词、字符或子词。模型处理文本时会将输入转换为token序列，这是模型理解和生成文本的基础单位。</p>
<p><strong>矢量&#x2F;向量数据库</strong>：一种专门存储和检索高维向量数据的数据库系统。在AI应用中，它主要用于存储文本、图像等数据的向量表示，支持相似性搜索和语义检索。</p>
<p><strong>数据蒸馏</strong>：一种模型压缩技术，通过将大型模型（教师模型）的知识转移到更小的模型（学生模型）中，在保持性能的同时减少模型规模和计算需求。</p>
<p>这些概念反映了AI技术的不同方面，理解它们对于把握大语言模型的发展和应用至关重要。</p>
<h1 id="传统人机交互模式"><a href="#传统人机交互模式" class="headerlink" title="传统人机交互模式"></a>传统人机交互模式</h1><p>传统的人机交互模式主要基于以下特点：</p>
<ul>
<li><p>指令式交互</p>
<ul>
<li>用户需要学习特定的命令和操作方式</li>
<li>交互过程严格遵循预设的规则和流程</li>
<li>功能和响应方式高度结构化</li>
</ul>
</li>
<li><p>界面驱动</p>
<ul>
<li>通过图形用户界面（GUI）进行操作</li>
<li>依赖菜单、按钮等可视化元素</li>
<li>交互路径相对固定</li>
</ul>
</li>
<li><p>有限的适应性</p>
<ul>
<li>系统行为模式相对固定</li>
<li>缺乏对用户意图的深度理解</li>
<li>较难处理复杂或模糊的请求</li>
</ul>
</li>
</ul>
<h1 id="大模型时代的人机交互模式"><a href="#大模型时代的人机交互模式" class="headerlink" title="大模型时代的人机交互模式"></a>大模型时代的人机交互模式</h1><p>在LLM和AI Agent时代，人机交互发生了显著的变革：</p>
<ul>
<li><p>自然语言交互</p>
<ul>
<li>用户可以使用日常语言表达需求</li>
<li>系统能够理解上下文和隐含意图</li>
<li>交流更加自然流畅</li>
</ul>
</li>
<li><p>智能理解与推理</p>
<ul>
<li>能够处理模糊和不完整的指令</li>
<li>具备上下文理解和记忆能力</li>
<li>可以进行多轮对话和递进式交互</li>
</ul>
</li>
<li><p>主动式协助</p>
<ul>
<li>能够预测用户需求并提供建议</li>
<li>自动分解复杂任务并规划执行步骤</li>
<li>提供个性化的解决方案</li>
</ul>
</li>
<li><p>多模态交互</p>
<ul>
<li>支持文本、语音、图像等多种输入方式</li>
<li>能够理解和生成多模态内容</li>
<li>提供更丰富的交互体验</li>
</ul>
</li>
</ul>
<p>这种新型交互模式大大降低了用户的学习成本，提高了人机交互的效率和体验，使得技术工具更加平易近人、更具智能化。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/04/23/%E5%A4%AF%E5%AE%9E%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3LLm%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" data-id="cm9tdj3f6000680w8hon1bh5p" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-大模型 RAG 应用开发基础及入门" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/04/23/%E5%A4%A7%E6%A8%A1%E5%9E%8B%20RAG%20%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%85%A5%E9%97%A8/" class="article-date">
  <time class="dt-published" datetime="2025-04-23T03:02:04.028Z" itemprop="datePublished">2025-04-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="大语言模型中的幻觉问题-上"><a href="#大语言模型中的幻觉问题-上" class="headerlink" title="大语言模型中的幻觉问题(上)"></a>大语言模型中的幻觉问题(上)</h1><h1 id="什么是大语言模型的幻觉？"><a href="#什么是大语言模型的幻觉？" class="headerlink" title="什么是大语言模型的幻觉？"></a>什么是大语言模型的幻觉？</h1><p>大语言模型在处理自然语言时，有时候会出现”幻觉“现象。所谓幻觉，就是模型生成的内容与事实或上下文不一致的问题。这些问题会严重影响AI应用的可靠性和实用性。</p>
<h1 id="幻觉的两大类型"><a href="#幻觉的两大类型" class="headerlink" title="幻觉的两大类型"></a>幻觉的两大类型</h1><h1 id="事实性幻觉"><a href="#事实性幻觉" class="headerlink" title="事实性幻觉"></a>事实性幻觉</h1><p>指模型生成的内容与实际事实不匹配。比如在回答”第一个登上月球的人是谁?”这个问题时:</p>
<ul>
<li><p>错误回答: “Charles Lindbergh在1951年月球任务中第一个登上月球”</p>
</li>
<li><p>正确事实: Neil Armstrong才是第一个登上月球的人(1969年阿波罗11号任务)</p>
</li>
</ul>
<p>这种幻觉之所以危险，是因为模型生成的内容看起来很可信，但实际上完全错误。</p>
<h1 id="忠实性幻觉"><a href="#忠实性幻觉" class="headerlink" title="忠实性幻觉"></a>忠实性幻觉</h1><p>指模型生成的内容与提供的上下文不一致。这种幻觉可以分为三类：</p>
<ul>
<li><p>输出与原文不一致（编出原文中没有的信息）</p>
</li>
<li><p>上下文之间不一致（前后矛盾）</p>
</li>
<li><p>逻辑链不一致（推理过程存在漏洞）</p>
</li>
</ul>
<p>比如在总结新闻时，模型可能会添加原文中不存在的细节，或者前后描述矛盾。</p>
<h1 id="为什么会产生幻觉？"><a href="#为什么会产生幻觉？" class="headerlink" title="为什么会产生幻觉？"></a>为什么会产生幻觉？</h1><p>大语言模型产生幻觉的原因主要来自三个方面：</p>
<ol>
<li><p>数据源导致的幻觉</p>
<ul>
<li>训练数据中的质量问题</li>
<li>数据中存在的错误信息</li>
<li>数据覆盖范围有限</li>
</ul>
</li>
<li><p>训练过程导致的幻觉</p>
<ul>
<li>架构限制：无法准确理解长文本的上下文关联</li>
<li>累积错误：生成过程中的错误会逐步传递和放大</li>
</ul>
</li>
<li><p>推理相关的幻觉</p>
<ul>
<li>回答过于简略</li>
<li>生成过程中的不完整推理</li>
</ul>
</li>
</ol>
<h1 id="如何评估幻觉问题"><a href="#如何评估幻觉问题" class="headerlink" title="如何评估幻觉问题"></a>如何评估幻觉问题</h1><p>为了客观评估模型的幻觉问题，我们可以使用多种方法：</p>
<ol>
<li><p>事实一致性评估：将生成内容与权威来源进行比对</p>
</li>
<li><p>分类器评估：使用专门训练的模型来检测是否存在幻觉</p>
</li>
<li><p>问答测量：通过问答来验证生成内容的一致性</p>
</li>
<li><p>不确定度分析：评估模型对自身输出的确信程度</p>
</li>
<li><p>提示测量：让模型自我评估，通过特定提示策略来评估生成内容</p>
</li>
</ol>
<h1 id="大语言模型中的幻觉问题-下-："><a href="#大语言模型中的幻觉问题-下-：" class="headerlink" title="大语言模型中的幻觉问题(下)："></a>大语言模型中的幻觉问题(下)：</h1><h1 id="RAG解决方案"><a href="#RAG解决方案" class="headerlink" title="RAG解决方案"></a>RAG解决方案</h1><h1 id="RAG是什么？"><a href="#RAG是什么？" class="headerlink" title="RAG是什么？"></a>RAG是什么？</h1><p><strong>RAG</strong>（Retrieval-Augmented Generation）也叫<strong>检索增强生成</strong>，是指对大语言模型输出进行优化，使其能够参考并利用数据源之外的权威知识。简单来说，RAG就是从外部检索对应的知识内容，和用户的提问一起构成Prompt发给大模型，再让大模型生成内容。</p>
<p>它的核心思想是：</p>
<ol>
<li><p><strong>从外部知识库检索相关信息</strong></p>
</li>
<li><p><strong>将检索到的信息作为上下文提供给模型</strong></p>
</li>
<li><p><strong>让模型基于这些上下文生成回答</strong></p>
</li>
</ol>
<p>简单来说：RAG &#x3D; 外部知识检索 + Prompt构建 + LLM 生成</p>
<p><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/4d514fab-2492-4877-a269-a017b8992bb6/c4c615d5-19f1-4c9d-8caa-eee0f593fefc/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4664XC7ZUWP/20250214/us-west-2/s3/aws4_request&X-Amz-Date=20250214T120608Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAIaCXVzLXdlc3QtMiJHMEUCIQCAZa41QdRQfaC3rApbiaJfe9m/idMREjxX08IIOKCw8QIgXm0aF57xzgTPF4oLqgi8WHzfRYATdR/643ZcnF7zxTMq/wMIKxAAGgw2Mzc0MjMxODM4MDUiDFnfXMWqNQJ3zbsfnyrcA2TgLhmQ0PVbkU6kDJF/j1NFA54XTuY7OmT/7W/nRh6vhbqwAv0QZ0gURiJ52Fjm7bEtAZZn2nfyG3qHl1DaATsX9IZmXh0lgRaJ85AVhiSye47ggd8huVtSNpalbT5F7N5LMIIQ+1CTbh0M/blSvk26wsU+J+EemIXOuPnoaQSF7jqQS8jeaeq4xpoEagvrkp4uL4to8kDoo2q7aJczD/+DKsUERJVBiqQz8HbgvqLDpPzjAr4a6BIY0X7w5kIGHGCWa9RTuzqeJo5dHpsE8ajBbF9y9l3C2JdgWCB15K5J8gDTfRhsl07Iov2/5LGSl2Mk+o0TE9dBYoMKEdZHXtKQ5DLfLsChDaUwl3oK/LM0WA6dw48srGJt7Gw3VwPwIzWBitTPGldPiFCK8hRek9zl3MnA/01vw4rSwkfNrs8jIMFPBMMQuYspaoR7fMYFCXbehSJNOVQvkMc7DiCr/gpae/dE50tJUWQDOoWD9TJs65rJdkeJ7lRVPVZoiuZSteBuKHPoF0QnfVfg+Lfqu4/SNmY3EsDFKyH+whQkobEuUXZDbZLq6jR+5t/gep/YlRdODIuwPTsENOu42ZThpPo2AHlbxbMuIzsu4p7MhW1/DObUXUs0m6e8UrhdMJivvL0GOqUBa+eSt+TiL4K8X/LbZcoIdV5RGPlPB7asVTCmNhNtuWd7c7wAzPusYvpK8iTG3Jrqq5g+6GtFZwPVllB5Ds0BwmscGWRraZwEk6NDSdrlJS6njKlL6ZWKinx4pA38aqHJ9TMkiPojW6h+pxn6HynsYXR7s5XpQWDNOpXdA6sYF0yH5T1t0mMJ4TYn4VG2BJo2dbi4riGYE42nC52QV+w6PrASzlze&X-Amz-Signature=d84e1e8b64f24f87b8cae422bdebc0a0e5b4aa0ff0b8d3a609121aaf57a6271f&X-Amz-SignedHeaders=host&x-id=GetObject" alt="image"></p>
<h1 id="为什么需要RAG？"><a href="#为什么需要RAG？" class="headerlink" title="为什么需要RAG？"></a>为什么需要RAG？</h1><p>LLM虽然是一个强大的工具，但它本身拒绝了解任何时事，且它给出的答案总是非常流畅，内容却不一定靠谱。这存在几个主要的问题:</p>
<ol>
<li><p>LLM的训练数据量有限且无法更新到最新知识。</p>
</li>
<li><p>当用户需要专业或领域特定的数据时，LLM往往缺乏相应的知识</p>
</li>
<li><p>对于答案的问答内容很难从源创进行溯源</p>
</li>
<li><p>由于技术限制，不同的训练源使用相同的大语言技术，可能会产生不确信的响应</p>
</li>
</ol>
<p>而RAG为解决这些问题带来了以下优势：</p>
<ul>
<li><p><strong>经济高效</strong>：预训练和微调模型的成本很高，而RAG是一种经济高效的新方法</p>
</li>
<li><p><strong>信息时效</strong>：使用RAG可以为LLM提供最新的研究、统计数据或新闻</p>
</li>
<li><p><strong>增强用户信任度</strong>：RAG允许LLM通过来源归属来呈现具体的信息，输出可以包括对来源的引文或参考，这可以增加对对话的生成式人工智能解决方案的任何信心</p>
</li>
</ul>
<h1 id="RAG是如何工作的？"><a href="#RAG是如何工作的？" class="headerlink" title="RAG是如何工作的？"></a>RAG是如何工作的？</h1><p>RAG采用三种主要的检索方式：</p>
<ol>
<li><p><strong>一次性检索</strong>：</p>
<ul>
<li>从单次检索中获取相关知识</li>
<li>直接预置到大模型的提示词中</li>
<li>不会收集反馈信息</li>
</ul>
</li>
<li><p><strong>迭代检索</strong>：</p>
<ul>
<li>允许在对话过程中多次检索</li>
<li>每一轮都可能有新的检索</li>
<li>支持多轮对话优化</li>
</ul>
</li>
<li><p><strong>事后检索</strong>：</p>
<ul>
<li>先生成答案</li>
<li>然后检索验证</li>
<li>对答案进行修正</li>
</ul>
</li>
</ol>
<p><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/4d514fab-2492-4877-a269-a017b8992bb6/4686426d-2314-4f87-ba63-abeb1a60669f/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4664XC7ZUWP/20250214/us-west-2/s3/aws4_request&X-Amz-Date=20250214T120608Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAIaCXVzLXdlc3QtMiJHMEUCIQCAZa41QdRQfaC3rApbiaJfe9m/idMREjxX08IIOKCw8QIgXm0aF57xzgTPF4oLqgi8WHzfRYATdR/643ZcnF7zxTMq/wMIKxAAGgw2Mzc0MjMxODM4MDUiDFnfXMWqNQJ3zbsfnyrcA2TgLhmQ0PVbkU6kDJF/j1NFA54XTuY7OmT/7W/nRh6vhbqwAv0QZ0gURiJ52Fjm7bEtAZZn2nfyG3qHl1DaATsX9IZmXh0lgRaJ85AVhiSye47ggd8huVtSNpalbT5F7N5LMIIQ+1CTbh0M/blSvk26wsU+J+EemIXOuPnoaQSF7jqQS8jeaeq4xpoEagvrkp4uL4to8kDoo2q7aJczD/+DKsUERJVBiqQz8HbgvqLDpPzjAr4a6BIY0X7w5kIGHGCWa9RTuzqeJo5dHpsE8ajBbF9y9l3C2JdgWCB15K5J8gDTfRhsl07Iov2/5LGSl2Mk+o0TE9dBYoMKEdZHXtKQ5DLfLsChDaUwl3oK/LM0WA6dw48srGJt7Gw3VwPwIzWBitTPGldPiFCK8hRek9zl3MnA/01vw4rSwkfNrs8jIMFPBMMQuYspaoR7fMYFCXbehSJNOVQvkMc7DiCr/gpae/dE50tJUWQDOoWD9TJs65rJdkeJ7lRVPVZoiuZSteBuKHPoF0QnfVfg+Lfqu4/SNmY3EsDFKyH+whQkobEuUXZDbZLq6jR+5t/gep/YlRdODIuwPTsENOu42ZThpPo2AHlbxbMuIzsu4p7MhW1/DObUXUs0m6e8UrhdMJivvL0GOqUBa+eSt+TiL4K8X/LbZcoIdV5RGPlPB7asVTCmNhNtuWd7c7wAzPusYvpK8iTG3Jrqq5g+6GtFZwPVllB5Ds0BwmscGWRraZwEk6NDSdrlJS6njKlL6ZWKinx4pA38aqHJ9TMkiPojW6h+pxn6HynsYXR7s5XpQWDNOpXdA6sYF0yH5T1t0mMJ4TYn4VG2BJo2dbi4riGYE42nC52QV+w6PrASzlze&X-Amz-Signature=add131fb89938d1db2adc0a92d7ae5554f82e544b528eebc37a418c0498796cc&X-Amz-SignedHeaders=host&x-id=GetObject" alt="image"></p>
<h1 id="RAG实战示例"><a href="#RAG实战示例" class="headerlink" title="RAG实战示例"></a>RAG实战示例</h1><p>以一个简单的问答场景为例，展示RAG的实际应用流程:</p>
<ol>
<li><p>用户提问:”公司有销售什么产品？”</p>
</li>
<li><p>系统处理流程:</p>
<ul>
<li>使用检索器获取产品相关文档</li>
<li>将文档内容与问题组合成提示词</li>
<li>通过LLM生成回答</li>
<li>确保回答基于检索到的事实信息</li>
</ul>
</li>
<li><p>最终输出:包含准确的产品信息，并且所有信息都可以溯源。</p>
</li>
</ol>
<h1 id="AI应用开发利器：向量数据库详解"><a href="#AI应用开发利器：向量数据库详解" class="headerlink" title="AI应用开发利器：向量数据库详解"></a>AI应用开发利器：向量数据库详解</h1><h1 id="什么是向量数据库？"><a href="#什么是向量数据库？" class="headerlink" title="什么是向量数据库？"></a>什么是向量数据库？</h1><p>向量数据库（Vector Database）是一种专门用于存储和处理向量数据的数据库系统。它不同于传统的关系型数据库，因为它需要将所有数据映射为特定的向量格式，并采用相似性搜索作为主要的检索方式。</p>
<h1 id="一个生动的例子：识别猫咪"><a href="#一个生动的例子：识别猫咪" class="headerlink" title="一个生动的例子：识别猫咪"></a>一个生动的例子：识别猫咪</h1><p>让我们通过一个识别猫咪的例子来理解向量数据库。假设我们有一组不同品种的猫咪图片：</p>
<ul>
<li><p>波斯猫</p>
</li>
<li><p>英国短毛猫</p>
</li>
<li><p>暹罗猫</p>
</li>
<li><p>布偶猫</p>
</li>
<li><p>无毛猫</p>
</li>
</ul>
<p>每张猫咪图片都可以用一组数字向量来表示其特征，如:</p>
<figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">波斯猫: [<span class="number">0.4</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, ...]</span><br><span class="line">英国短毛猫: [<span class="number">0.7</span>, <span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, ...]</span><br><span class="line">暹罗猫: [<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, ...]</span><br></pre></td></tr></table></figure>


<p>这些数字代表了猫咪的各种特征，比如:</p>
<ul>
<li><p>毛发长度</p>
</li>
<li><p>体型大小</p>
</li>
<li><p>面部特征</p>
</li>
<li><p>耳朵形状等等</p>
</li>
</ul>
<h1 id="向量数据库的优势"><a href="#向量数据库的优势" class="headerlink" title="向量数据库的优势"></a>向量数据库的优势</h1><p>与传统的数据库相比，向量数据库有以下特点：</p>
<ol>
<li><p><strong>数据类型</strong>：</p>
<ul>
<li>传统数据库：数值、字符串、时间等结构化数据</li>
<li>向量数据库：向量数据(不存储原始数据，有的也支持)</li>
</ul>
</li>
<li><p><strong>数据规模</strong>：</p>
<ul>
<li>传统数据库：小，1亿条数据对关系型数据库来说规模很大</li>
<li>向量数据库：大，最少千亿数据是基线</li>
</ul>
</li>
<li><p><strong>数据组织方式</strong>：</p>
<ul>
<li>传统数据库：基于表格、按照行和列组织</li>
<li>向量数据库：基于向量、按向量维度组织</li>
</ul>
</li>
<li><p><strong>查找方式</strong>：</p>
<ul>
<li>传统数据库：精确查找&#x2F;范围查找</li>
<li>向量数据库：近似查找，查询结果是与输入向量最相似的向量</li>
</ul>
</li>
</ol>
<h1 id="相似性搜索算法"><a href="#相似性搜索算法" class="headerlink" title="相似性搜索算法"></a>相似性搜索算法</h1><p>在向量数据库中，支持通过多种方式来计算两个向量的相似度：</p>
<p><strong>余弦相似度</strong>：主要是用于衡量向量在方向上的相似性，特别适用于文本、图像和高维空间中的向量。它不受向量长度的影响，只考虑方向的相似程度，计算公式如下（计算两个向量间的夹角的余弦值，取值范围为[-1, 1]）：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">similarity(A,B) = (A·B)/(||A||·||B||)</span><br></pre></td></tr></table></figure>


<p><strong>欧式距离</strong>：主要是用于衡量向量之间的直线距离，得到的值可能很大，最小为0，通常用于低维空间或需要考虑向量各个维度之间差异的情况。欧式距离较小的向量被认为更相似，计算公式如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distance(A,B) = √∑(Ai-Bi)²</span><br></pre></td></tr></table></figure>


<p>例如下图：左侧就是<code>欧式距离</code>，右侧就是<code>余弦相似度</code>。</p>
<p><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/4d514fab-2492-4877-a269-a017b8992bb6/5ffcc1e2-d42e-43e1-8304-359cfa6e287b/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB4664XC7ZUWP/20250214/us-west-2/s3/aws4_request&X-Amz-Date=20250214T120608Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAIaCXVzLXdlc3QtMiJHMEUCIQCAZa41QdRQfaC3rApbiaJfe9m/idMREjxX08IIOKCw8QIgXm0aF57xzgTPF4oLqgi8WHzfRYATdR/643ZcnF7zxTMq/wMIKxAAGgw2Mzc0MjMxODM4MDUiDFnfXMWqNQJ3zbsfnyrcA2TgLhmQ0PVbkU6kDJF/j1NFA54XTuY7OmT/7W/nRh6vhbqwAv0QZ0gURiJ52Fjm7bEtAZZn2nfyG3qHl1DaATsX9IZmXh0lgRaJ85AVhiSye47ggd8huVtSNpalbT5F7N5LMIIQ+1CTbh0M/blSvk26wsU+J+EemIXOuPnoaQSF7jqQS8jeaeq4xpoEagvrkp4uL4to8kDoo2q7aJczD/+DKsUERJVBiqQz8HbgvqLDpPzjAr4a6BIY0X7w5kIGHGCWa9RTuzqeJo5dHpsE8ajBbF9y9l3C2JdgWCB15K5J8gDTfRhsl07Iov2/5LGSl2Mk+o0TE9dBYoMKEdZHXtKQ5DLfLsChDaUwl3oK/LM0WA6dw48srGJt7Gw3VwPwIzWBitTPGldPiFCK8hRek9zl3MnA/01vw4rSwkfNrs8jIMFPBMMQuYspaoR7fMYFCXbehSJNOVQvkMc7DiCr/gpae/dE50tJUWQDOoWD9TJs65rJdkeJ7lRVPVZoiuZSteBuKHPoF0QnfVfg+Lfqu4/SNmY3EsDFKyH+whQkobEuUXZDbZLq6jR+5t/gep/YlRdODIuwPTsENOu42ZThpPo2AHlbxbMuIzsu4p7MhW1/DObUXUs0m6e8UrhdMJivvL0GOqUBa+eSt+TiL4K8X/LbZcoIdV5RGPlPB7asVTCmNhNtuWd7c7wAzPusYvpK8iTG3Jrqq5g+6GtFZwPVllB5Ds0BwmscGWRraZwEk6NDSdrlJS6njKlL6ZWKinx4pA38aqHJ9TMkiPojW6h+pxn6HynsYXR7s5XpQWDNOpXdA6sYF0yH5T1t0mMJ4TYn4VG2BJo2dbi4riGYE42nC52QV+w6PrASzlze&X-Amz-Signature=f9f2309cf2ddf4433be34bfea9c288371a7af10835e77f90a67451cb53974f42&X-Amz-SignedHeaders=host&x-id=GetObject" alt="image"></p>
<h1 id="实际应用场景"><a href="#实际应用场景" class="headerlink" title="实际应用场景"></a>实际应用场景</h1><p>向量数据库的主要应用场景包括：</p>
<ol>
<li><p>人脸识别</p>
</li>
<li><p>图像搜索</p>
</li>
<li><p>音频识别</p>
</li>
<li><p>智能推荐系统</p>
</li>
</ol>
<p>这些场景的共同特点是：需要对非结构化数据（如图片、文本、音频）进行相似度搜索。</p>
<p>在RAG中，我们会将文档的知识按特定规则分成小块，转换成向量存储到向量数据库中。当人类提问时，我们将问题转换为向量，在数据库中找到最相似的文本块，这些文本块可以成为Prompt的补充内容。</p>
<h1 id="深入理解Embedding嵌入技术"><a href="#深入理解Embedding嵌入技术" class="headerlink" title="深入理解Embedding嵌入技术"></a>深入理解Embedding嵌入技术</h1><h1 id="Embedding-是什么？"><a href="#Embedding-是什么？" class="headerlink" title="Embedding 是什么？"></a>Embedding 是什么？</h1><p>Embedding(嵌入)是一种在机器学习中广泛使用的技术，它能将文本、图片、视频等非结构化数据映射到向量空间中。一个Embedding向量通常是一个包含N个浮点数的数组，这个向量不仅表示了数据的特征，更重要的是通过学习可以表达它们的内在语义。简而言之，Embedding就是一个模型生成方法，可以将非结构化的数据，例如文本&#x2F;图片&#x2F;视频等数据映射成有意义的向量数据。比如一段文本、一张图片、一段视频，警告Embedding模型处理后都会变成类似这样的向量：</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/04/23/%E5%A4%A7%E6%A8%A1%E5%9E%8B%20RAG%20%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%85%A5%E9%97%A8/" data-id="cm9tdj3f5000580w81zym4gt1" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-LangChain初入门" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/04/23/LangChain%E5%88%9D%E5%85%A5%E9%97%A8/" class="article-date">
  <time class="dt-published" datetime="2025-04-23T03:02:04.023Z" itemprop="datePublished">2025-04-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>不同LLM模型之间的输入输出结构存在差异，这些差异会导致开发者需要频繁修改代码，降低代码的可维护性。为了解决这个问题，Langchain应用框架应运而生。</p>
<h1 id="为什么选择LangChain"><a href="#为什么选择LangChain" class="headerlink" title="为什么选择LangChain"></a>为什么选择LangChain</h1><p>LangChain作为一个强大的框架，具有以下优势：</p>
<ul>
<li><p><strong>组件化和标准化</strong>：提供了标准化的接口来处理各种LLM，使开发更加灵活和可维护。</p>
</li>
<li><p><strong>丰富的工具集成</strong>：内置了大量工具和集成，可以轻松连接数据库、搜索引擎等外部服务。</p>
</li>
<li><p><strong>链式处理能力</strong>：可以将多个组件组合成链，实现复杂的处理流程。</p>
</li>
<li><p><strong>内存管理</strong>：提供了多种记忆组件，使应用能够保持上下文连贯性。</p>
</li>
</ul>
<h1 id="LangChain简介"><a href="#LangChain简介" class="headerlink" title="LangChain简介"></a>LangChain简介</h1><p>LangChain是一个用于开发由语言模型驱动的应用程序的框架。</p>
<h1 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h1><ul>
<li><p><strong>Models (模型)</strong>：提供与大语言模型的统一交互接口，支持各类LLM、聊天模型和文本嵌入模型的调用</p>
</li>
<li><p><strong>Prompts (提示)</strong>：专门用于管理和优化提示模板，提供标准化的提示工程工具</p>
</li>
<li><p><strong>Indexes (索引)</strong>：提供高效的文档加载、分割和向量存储系统，支持大规模文本处理和检索</p>
</li>
<li><p><strong>Memory (记忆)</strong>：用于在交互过程中管理和存储状态信息，确保对话的连贯性和上下文理解</p>
</li>
<li><p><strong>Chains (链)</strong>：能将多个组件组合成端到端应用的核心机制，实现复杂的处理流程</p>
</li>
<li><p><strong>Agents (代理)</strong>：赋予LLM使用工具的能力，支持自主推理和行动决策</p>
</li>
</ul>
<p><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/4d514fab-2492-4877-a269-a017b8992bb6/7c5c69d7-7584-4b18-93ac-72f0968bfd6b/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466366VPXX5/20250214/us-west-2/s3/aws4_request&X-Amz-Date=20250214T120552Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAIaCXVzLXdlc3QtMiJHMEUCICbFJSazKUJnRGyYSPCEjO5Pjm5GnjjA/vfAL5PzwlllAiEA2BFuvc3fZbAVmfD1fN2x0Tpc/DtIkB5hMAQ+BQBn1xEq/wMIKxAAGgw2Mzc0MjMxODM4MDUiDGL0ioewngxnxcVHhSrcAzJ4/3gWKEpiUvEzHIBHdGhywWmVbzZfirTkfSnmfc8dq4qGQrAzi+bJ2SHPE3vkOEk1WmoNg0cl9WOX4wlQ82ohFK6YOQvyixWVDR7Z1akxP4B4uBOyv+FJcmwIf085AR0zWfA8g8LhsYr6gxLjoobchKS05FLq3AoSiE1g/Spw2YJ2ythC0+XISqJNRHHUropmd95VyrR5BDIRP/s1sxHFm+8N+7zbPDF/KMQ2LpnCTPN4gYLgw1iA+WdRCYxjS9oBqCGVapmFAT9Ua0iHPB4UYHX5EHrIfAO3+Rsj6t5mSfcwPnOZwaPAw0IUz+KIg3eVTrpEXODzMRqQs7UxJ+Hy0MdTDfMWNR6DdDrFBPlSLxudt6iyTY3bTlh357XiA7aPQkVjlxEXR+yNR+DQpgjfE0wbCx7DMWzvvEa0XcB+OeDA7593KCVeKwt6RWYn7poBbeFz0B+/u829hzyXLqjVOhUduro9BvJw8SztRfDeFtZ1O/A2Z1+wtH0mlJsF5LdBhkZMBypw8hERe0ApBjCjlbJD9q1+yL29VqnHmu54Mo5FJwzFqnmAXx2Mgj170vRBRyXIgnhtObJ7D4v37H/pAQRZcFEoH2IX3lA2BOne888pIowu1XCT/Y2rMKKvvL0GOqUB2FgXuqUdhTS2SgKmvHtE1aZGpwpp5GEN/XdvSKkH5cTyjBvvbIpfak0vnzzM5bG83NA15op5xVu/ScgE2+jHp0cYu1ynxwCVUWmC55Lyl3hVSS1Dp4tDnzRkm8ip0dHV7bRz8ebhVM7E66rYexLn0QTMXVyQB4X15aAbHkFpwFL3UvZeZGGNbJhIw+3vZfV9gCUxrbgHyk/jT6ywe4v3v51ki7Z2&X-Amz-Signature=319fbfd7fd14fddc9017b74a93e4e5144f7020d58b2e1f37f3cd6953dad543f2&X-Amz-SignedHeaders=host&x-id=GetObject" alt="image"></p>
<p><strong>Prompts组件介绍</strong></p>
<p><strong>概念与作用</strong></p>
<p>在LLM应用开发中,我们通常不会直接将用户输入传递给大模型,而是会将用户输入添加到一个更大的文本片段中,这个文本片段被称为Prompt。Prompt为大模型提供了任务相关的上下文和指令,帮助模型更好地理解和执行任务。</p>
<p>LangChain中的Prompts组件提供了一系列工具来管理和优化这些提示模板。主要包含两大类:</p>
<ul>
<li><p>PromptTemplate: 将Prompt按照template进行格式化,处理变量和组合</p>
</li>
<li><p>Selectors: 根据不同条件选择不同的提示词</p>
</li>
</ul>
<p><strong>基本构成</strong></p>
<p>在LangChain中,Prompts组件包含多个子组件:</p>
<p>角色提示模板:</p>
<ul>
<li><p>SystemMessagePromptTemplate: 系统角色消息模板</p>
</li>
<li><p>HumanMessagePromptTemplate: 人类角色消息模板</p>
</li>
<li><p>AIMessagePromptTemplate: AI角色消息模板</p>
</li>
</ul>
<p>提示模板类型:</p>
<ul>
<li><p>PromptTemplate: 文本提示模板</p>
</li>
<li><p>ChatPromptTemplate: 聊天消息提示模板</p>
</li>
<li><p>MessagePlaceholder: 消息占位符</p>
</li>
</ul>
<p><strong>关键操作</strong></p>
<p>格式化LangChain支持两种格式化方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># f-string方式</span></span><br><span class="line">prompt = PromptTemplate.from_template(<span class="string">&quot;请将一个关于&#123;subject&#125;的笑话&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># jinja2方式</span></span><br><span class="line">prompt = PromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;请将一个关于&#123;&#123;subject&#125;&#125;的笑话&quot;</span>,</span><br><span class="line">    template_format=<span class="string">&quot;jinja2&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<p>提示模板拼接</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 字符串提示拼接</span></span><br><span class="line">prompt = (</span><br><span class="line">    PromptTemplate.from_template(<span class="string">&quot;请将一个关于&#123;subject&#125;的冷笑话&quot;</span>)</span><br><span class="line">    + <span class="string">&quot;，让我开心下&quot;</span></span><br><span class="line">    + <span class="string">&quot;\n使用&#123;language&#125;语言。&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 聊天提示拼接</span></span><br><span class="line">system_prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是OpenAI开发的聊天机器人，请根据用户的提问进行回复，我叫&#123;username&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line">human_prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;query&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line">prompt = system_prompt + human_prompt</span><br></pre></td></tr></table></figure>


<p><strong>模板复用</strong></p>
<p>对于复杂的提示模板,LangChain提供了PipelinePromptTemplate来实现模板的复用:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 描述提示模板</span></span><br><span class="line">instruction_template = <span class="string">&quot;你正在模拟&#123;person&#125;。&quot;</span></span><br><span class="line">instruction_prompt = PromptTemplate.from_template(instruction_template)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例提示模板</span></span><br><span class="line">example_template = <span class="string">&quot;&quot;&quot;下面是一个交互例子:</span></span><br><span class="line"><span class="string">Q: &#123;example_q&#125;</span></span><br><span class="line"><span class="string">A: &#123;example_a&#125;&quot;&quot;&quot;</span></span><br><span class="line">example_prompt = PromptTemplate.from_template(example_template)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始提示模板</span></span><br><span class="line">start_template = <span class="string">&quot;&quot;&quot;现在开始对话:</span></span><br><span class="line"><span class="string">Q: &#123;input&#125;</span></span><br><span class="line"><span class="string">A:&quot;&quot;&quot;</span></span><br><span class="line">start_prompt = PromptTemplate.from_template(start_template)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组合模板</span></span><br><span class="line">pipeline_prompt = PipelinePromptTemplate(</span><br><span class="line">    final_prompt=full_prompt,</span><br><span class="line">    pipeline_prompts=[</span><br><span class="line">        (<span class="string">&quot;instruction&quot;</span>, instruction_prompt),</span><br><span class="line">        (<span class="string">&quot;example&quot;</span>, example_prompt),</span><br><span class="line">        (<span class="string">&quot;start&quot;</span>, start_prompt),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<p><strong>最佳实践</strong></p>
<p>选择合适的格式化方式</p>
<ol>
<li><p>简单变量替换使用f-string</p>
</li>
<li><p>需要条件判断等复杂逻辑时使用jinja2</p>
</li>
</ol>
<p>提示模板设计</p>
<ol>
<li><p>保持模板的清晰和可维护性</p>
</li>
<li><p>合理使用系统消息和示例</p>
</li>
<li><p>避免过于复杂的嵌套结构</p>
</li>
</ol>
<p>错误处理</p>
<ol>
<li><p>验证必要的变量是否存在</p>
</li>
<li><p>处理格式化可能出现的异常</p>
</li>
</ol>
<p>性能优化</p>
<ol>
<li><p>重复使用的模板要缓存</p>
</li>
<li><p>避免不必要的模板拼接操作</p>
</li>
</ol>
<p><strong>Model组件详解</strong></p>
<p><strong>基本概念</strong></p>
<p>Models是LangChain的核心组件，提供了一个标准接口来封装不同类型的LLM进行交互，LangChain本身不提供LLM,而是提供了接口来集成各种模型。</p>
<p>LangChain支持两种类型的模型:</p>
<ul>
<li><p>LLM: 使用纯文本作为输入和输出的大语言模型</p>
</li>
<li><p>Chat Model: 使用聊天消息列表作为输入并返回聊天消息的聊天模型</p>
</li>
</ul>
<p><strong>组件架构</strong></p>
<p>LangChain中Models组件的基类结构如下:</p>
<p>BaseLanguageModel(基类)</p>
<ul>
<li><p>BaseLLM(大语言模型基类)</p>
<ul>
<li>SimpleLLM(简化大语言模型)</li>
<li>第三方LLM集成(OpenAI、百度文心等)</li>
</ul>
</li>
<li><p>BaseChatModel(聊天模型基类)</p>
<ul>
<li>SimpleChatModel(简化聊天模型)</li>
<li>第三方Chat Model集成</li>
</ul>
</li>
</ul>
<p>Message组件类型:</p>
<ul>
<li><p>SystemMessage: 系统消息</p>
</li>
<li><p>HumanMessage: 人类消息</p>
</li>
<li><p>AIMessage: AI消息</p>
</li>
<li><p>FunctionMessage: 函数调用消息</p>
</li>
<li><p>ToolMessage: 工具调用消息</p>
</li>
</ul>
<p><strong>核心办法</strong></p>
<p>Models组件提供了几个关键方法:</p>
<p>invoke&#x2F;invoke_sync: 调用模型生成内容</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基本调用</span></span><br><span class="line">llm = ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>)</span><br><span class="line">response = llm.invoke(<span class="string">&quot;你好!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 异步调用</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">generate</span>():</span><br><span class="line">    response = <span class="keyword">await</span> llm.ainvoke(<span class="string">&quot;你好!&quot;</span>)</span><br></pre></td></tr></table></figure>


<p>batch&#x2F;abatch: 批量调用处理多个输入</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">messages = [</span><br><span class="line">    <span class="string">&quot;请讲一个关于程序员的笑话&quot;</span>,</span><br><span class="line">    <span class="string">&quot;请讲一个关于Python的笑话&quot;</span></span><br><span class="line">]</span><br><span class="line">responses = llm.batch(messages)</span><br></pre></td></tr></table></figure>


<p>stream&#x2F;astream: 流式返回生成内容</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">response = llm.stream(<span class="string">&quot;请介绍下LLM和LLMOps&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> response:</span><br><span class="line">    <span class="built_in">print</span>(chunk.content, end=<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>


<p><strong>Message组件使用</strong></p>
<p>消息组件用于构建与聊天模型的交互:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> SystemMessage, HumanMessage, AIMessage</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建消息</span></span><br><span class="line">system_msg = SystemMessage(content=<span class="string">&quot;你是一个AI助手&quot;</span>)</span><br><span class="line">human_msg = HumanMessage(content=<span class="string">&quot;你好!&quot;</span>)</span><br><span class="line">ai_msg = AIMessage(content=<span class="string">&quot;你好!我是AI助手&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建消息列表</span></span><br><span class="line">messages = [system_msg, human_msg, ai_msg]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用消息与模型交互</span></span><br><span class="line">response = chat_model.invoke(messages)</span><br></pre></td></tr></table></figure>


<p><strong>实践示例</strong></p>
<p>基本对话示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建聊天模型</span></span><br><span class="line">chat = ChatOpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建提示模板</span></span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一位&#123;role&#125;&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;query&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用模型</span></span><br><span class="line">response = chat.invoke(</span><br><span class="line">    prompt.format_messages(</span><br><span class="line">        role=<span class="string">&quot;Python专家&quot;</span>,</span><br><span class="line">        query=<span class="string">&quot;什么是装饰器?&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<p>流式输出示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建提示模板</span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;&#123;subject&#125;的发展历史是什么?&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">llm = ChatOpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 流式生成</span></span><br><span class="line">response = llm.stream(</span><br><span class="line">    prompt.format_messages(subject=<span class="string">&quot;人工智能&quot;</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理输出</span></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> response:</span><br><span class="line">    <span class="built_in">print</span>(chunk.content, end=<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure>


<p><strong>最佳实践</strong></p>
<p>选择合适的模型类型</p>
<ol>
<li><p>简单文本生成任务使用LLM</p>
</li>
<li><p>对话类任务使用Chat Model</p>
</li>
</ol>
<p>正确处理异步操作</p>
<ol>
<li><p>在异步环境中使用ainvoke&#x2F;astream</p>
</li>
<li><p>批量处理时考虑使用batch</p>
</li>
</ol>
<p>异常处理</p>
<ol>
<li><p>处理模型调用可能的超时</p>
</li>
<li><p>捕获API错误并适当处理</p>
</li>
</ol>
<p>性能优化</p>
<ol>
<li><p>合理使用批处理</p>
</li>
<li><p>适时使用流式输出</p>
</li>
</ol>
<p><strong>OutputParser 解析器组件</strong></p>
<p>为什么需要输出解析器</p>
<p>在使用大模型时,我们经常会遇到输出解析的问题。比如:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">llm = ChatOpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例1: 返回的是自然语言</span></span><br><span class="line">llm.invoke(<span class="string">&quot;1+1等于几?&quot;</span>)  <span class="comment"># 输出: 1 + 1 等于 2。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例2: 包含多余信息</span></span><br><span class="line">llm.invoke(<span class="string">&quot;告诉我3个动物的名字。&quot;</span>)  <span class="comment"># 输出: 好的，这里有三种动物的名字：\n1. 狮子\n2. 大熊猫\n3. 斑马</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例3: 格式不统一</span></span><br><span class="line">llm.invoke(<span class="string">&quot;给我一个json数据,键为a和b&quot;</span>)  <span class="comment"># 输出: &#123;\n &quot;a&quot;: 10,\n &quot;b&quot;: 20\n&#125;</span></span><br></pre></td></tr></table></figure>


<p>OutputParser就是为了解决这些问题而设计的。它通过:</p>
<ol>
<li><p>预设提示 - 告诉LLM需要的输出格式</p>
</li>
<li><p>解析功能 - 将输出转换成指定格式</p>
</li>
</ol>
<p><strong>Parser类型详解</strong></p>
<p>Langchain 提供了多种Parser：</p>
<ol>
<li><p>基础Parser：</p>
<ul>
<li>StrOutputParser: 最简单的Parser,原样返回文本</li>
<li>BaseOutputParser: 所有Parser的基类</li>
<li>BaseLLMOutputParser: 专门用于LLM输出的基类</li>
</ul>
</li>
<li><p>格式化Parser：</p>
<ul>
<li>JsonOutputParser: 解析JSON格式输出</li>
<li>XMLOutputParser: 解析XML格式输出</li>
<li>PydanticOutputParser: 使用Pydantic模型解析输出</li>
</ul>
</li>
<li><p>列表类Parser：</p>
<ul>
<li>CommaSeparatedListOutputParser: 解析逗号分隔的列表</li>
<li>NumberedListOutputParser: 解析数字编号的列表</li>
</ul>
</li>
</ol>
<p><strong>实践示例</strong></p>
<ol>
<li>StrOutputParser使用：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建链</span></span><br><span class="line">chain = (</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;&#123;query&#125;&quot;</span>)</span><br><span class="line">    | ChatOpenAI()</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用</span></span><br><span class="line">response = chain.invoke(&#123;<span class="string">&quot;query&quot;</span>: <span class="string">&quot;你好!&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>JsonOutputParser使用：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> JsonOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.pydantic_v1 <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输出结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Joke</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    joke: <span class="built_in">str</span> = Field(description=<span class="string">&quot;回答用户的冷笑话&quot;</span>)</span><br><span class="line">    punchline: <span class="built_in">str</span> = Field(description=<span class="string">&quot;冷笑话的笑点&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Parser</span></span><br><span class="line">parser = JsonOutputParser(pydantic_object=Joke)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建提示模板</span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;回答用户的问题。\n&#123;format_instructions&#125;\n&#123;query&#125;\n&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加格式说明</span></span><br><span class="line">prompt = prompt.partial(format_instructions=parser.get_format_instructions())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建链</span></span><br><span class="line">chain = prompt | ChatOpenAI() | parser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line">response = chain.invoke(&#123;<span class="string">&quot;query&quot;</span>: <span class="string">&quot;请讲一个关于程序员的冷笑话&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>


<p><strong>错误处理</strong></p>
<ol>
<li>解析失败的处理：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> OutputParserException</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = parser.parse(llm_output)</span><br><span class="line"><span class="keyword">except</span> OutputParserException <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># 处理解析错误</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;解析错误: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 可以选择重试或使用默认值</span></span><br></pre></td></tr></table></figure>


<ol start="2">
<li>使用重试机制：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以配置回调来处理重试</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.callbacks <span class="keyword">import</span> BaseCallbackHandler</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RetryHandler</span>(<span class="title class_ inherited__">BaseCallbackHandler</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_retry</span>(<span class="params">self, retry_state</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;重试次数: <span class="subst">&#123;retry_state.attempt_number&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>


<p><strong>最佳实践</strong></p>
<ol>
<li><p>选择合适的Parser</p>
<ul>
<li>简单文本使用StrOutputParser</li>
<li>结构化数据使用JsonOutputParser或PydanticOutputParser</li>
<li>列表数据使用专门的列表Parser</li>
</ul>
</li>
<li><p>提示设计</p>
<ul>
<li>在提示中明确指定输出格式</li>
<li>使用Parser提供的format_instructions</li>
</ul>
</li>
<li><p>异常处理</p>
<ul>
<li>总是处理可能的解析错误</li>
<li>考虑添加重试机制</li>
<li>提供合理的默认值</li>
</ul>
</li>
<li><p>性能优化</p>
<ul>
<li>避免过于复杂的解析逻辑</li>
<li>合理使用缓存</li>
</ul>
</li>
</ol>
<p><strong>LCEL表达式与Runnable协议</strong></p>
<p><strong>为什么需要LCEL</strong></p>
<p>传统的链式调用方式存在嵌套问题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">content = parser.invoke(</span><br><span class="line">    llm.invoke(</span><br><span class="line">        prompt.invoke(</span><br><span class="line">            &#123;<span class="string">&quot;query&quot;</span>: req.query.data&#125;</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<p>LCEL 提供了更优雅的方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chain = prompt | llm | parser</span><br><span class="line">content = chain.invoke(&#123;<span class="string">&quot;query&quot;</span>: req.query.data&#125;)</span><br></pre></td></tr></table></figure>


<p><strong>Runnable协议核心方法</strong></p>
<ul>
<li><p>invoke&#x2F;ainvoke: 调用组件</p>
</li>
<li><p>batch&#x2F;abatch: 批量处理</p>
</li>
<li><p>stream&#x2F;astream: 流式输出</p>
</li>
<li><p>transform: 转换输入输出</p>
</li>
</ul>
<p><strong>两个核心类</strong></p>
<ol>
<li>RunnableParallel - 并行执行多个Runnable</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableParallel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 并行执行多个链</span></span><br><span class="line">chain = RunnableParallel(</span><br><span class="line">    joke=joke_chain,</span><br><span class="line">    poem=poem_chain</span><br><span class="line">)</span><br><span class="line">resp = chain.invoke(&#123;<span class="string">&quot;subject&quot;</span>: <span class="string">&quot;程序员&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>RunnablePassthrough - 传递数据</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建检索链</span></span><br><span class="line">chain = (</span><br><span class="line">    RunnablePassthrough.assign(</span><br><span class="line">        context=<span class="keyword">lambda</span> query: retrieval(query)</span><br><span class="line">    )</span><br><span class="line">    | prompt </span><br><span class="line">    | llm </span><br><span class="line">    | parser</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<p><strong>实践示例</strong></p>
<ol>
<li>基础链构建：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建组件</span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;&#123;input&#125;&quot;</span>)</span><br><span class="line">llm = ChatOpenAI()</span><br><span class="line">parser = StrOutputParser()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建链</span></span><br><span class="line">chain = prompt | llm | parser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">response = chain.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;Hello!&quot;</span>&#125;)</span><br></pre></td></tr></table></figure>


<ol start="2">
<li>带检索的链：</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">retrieval</span>(<span class="params">query: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;相关文档内容...&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建链</span></span><br><span class="line">chain = (</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;context&quot;</span>: retrieval,</span><br><span class="line">        <span class="string">&quot;question&quot;</span>: RunnablePassthrough()</span><br><span class="line">    &#125;</span><br><span class="line">    | prompt</span><br><span class="line">    | llm</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">response = chain.invoke(<span class="string">&quot;问题&quot;</span>)</span><br></pre></td></tr></table></figure>


<p><strong>最佳实践</strong></p>
<ol>
<li><p>链的设计</p>
<ul>
<li>使用管道操作符(|)构建简单链</li>
<li>复杂逻辑使用RunnableParallel</li>
<li>数据传递用RunnablePassthrough</li>
</ul>
</li>
<li><p>错误处理</p>
<ul>
<li>合理使用try&#x2F;except</li>
<li>实现错误回调处理</li>
</ul>
</li>
<li><p>性能优化</p>
<ul>
<li>合适场景使用并行执行</li>
<li>批处理代替单个处理</li>
</ul>
</li>
<li><p>代码可维护性</p>
<ul>
<li>链结构保持清晰</li>
<li>适当拆分复杂链</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/04/23/LangChain%E5%88%9D%E5%85%A5%E9%97%A8/" data-id="cm9tdj3f4000380w87vgu64ad" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-LangChain RAG 应用开发组件深度解析" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/04/23/LangChain%20RAG%20%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E7%BB%84%E4%BB%B6%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/" class="article-date">
  <time class="dt-published" datetime="2025-04-23T03:02:04.021Z" itemprop="datePublished">2025-04-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>在当今的AI应用开发中，检索增强生成(Retrieval-Augmented Generation, RAG)已经成为一种重要的技术范式。它通过将大语言模型与外部知识库结合，极大地提升了AI系统的知识获取能力和输出质量。而LangChain作为一个强大的框架，为RAG应用的开发提供了丰富的组件支持。本文将深入剖析LangChain中RAG应用开发的核心组件，帮助你更好地理解和使用这些工具。</p>
<h1 id="核心组件概览"><a href="#核心组件概览" class="headerlink" title="核心组件概览"></a>核心组件概览</h1><p>在开始深入学习之前,我们先来了解LangChain中RAG应用开发涉及的主要组件:</p>
<ol>
<li><p>Document组件与文档加载器 - 负责文档的加载和基础处理</p>
</li>
<li><p>文档转换器与分割器 - 处理文档转换和分块</p>
</li>
<li><p>VectorStore组件 - 实现向量存储和检索</p>
</li>
<li><p>Blob相关组件 - 处理二进制大对象数据</p>
</li>
</ol>
<h1 id="Document-组件与文档加载器详解"><a href="#Document-组件与文档加载器详解" class="headerlink" title="Document 组件与文档加载器详解"></a>Document 组件与文档加载器详解</h1><h1 id="Document-组件基础"><a href="#Document-组件基础" class="headerlink" title="Document 组件基础"></a>Document 组件基础</h1><p>Document 是 LangChain 的核心组件之一，它定义了一个通用的文档结构，包含两个基本要素：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Document = page_content(页面内容) + metadata(元数据)</span><br></pre></td></tr></table></figure>


<p>这种结构允许我们统一处理各种类型的文档，同时保留文档的元信息。</p>
<h1 id="文档加载器类型"><a href="#文档加载器类型" class="headerlink" title="文档加载器类型"></a>文档加载器类型</h1><p>LangChain 提供了多种文档加载器：</p>
<ol>
<li><p>通用文本加载器</p>
</li>
<li><p>CSV文件加载器</p>
</li>
<li><p>HTML网页加载器</p>
</li>
<li><p>PDF文档加载器</p>
</li>
<li><p>Markdown文档加载器</p>
</li>
</ol>
<p><img src="https://prod-files-secure.s3.us-west-2.amazonaws.com/4d514fab-2492-4877-a269-a017b8992bb6/cb837df0-87b9-4c19-901c-a0fcdad22f4e/image.png?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=ASIAZI2LB466SYW3IZR5/20250214/us-west-2/s3/aws4_request&X-Amz-Date=20250214T120551Z&X-Amz-Expires=3600&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEAIaCXVzLXdlc3QtMiJIMEYCIQDUGJ9lVmDKClKDaOJ895Jz4gq0uL4Gzc0zfl1+64/0rQIhAMbHqoFiJxBAaPNAzZRFNa0zAF5sOtIPtDo1tBFDgDksKv8DCCsQABoMNjM3NDIzMTgzODA1IgxlcHkcbVGFv4NDdO0q3AOLCbqv7b6eCQK4Nd3NfFbLVSbYqxsNyHqMEtu7wrbIMNidTTB9g007CX/CEHJ3ufYsiUC32OLi/acdwBqCPNC06Cu0ytlWmuA7Od9xEn9KC/OquaUqs8HpkWTp6jbkzRC8+1ITewYJDEOfHDJ2cG5txbzNnsLhr1n5sGPOnDDCY31dE5G1XPOkcyhM4Uogbe+Wmh6imJfkQQmxpVRhwX1qhCqvS3XRNyBkhi2u/dimHWRL/8Gzkq1tu4wUMFOrK3/3Au/sl2hnwS6F+mjdXo5TJIHOMaoFc6xQNR5P1xzIQLANFyR3PhDzz3LcLop7bM4QsclUpAgFiWFykifKeNAGfy5RMZqFLOg8Zac8AbVWHW/OX/j5MPWwyef8Qi2+oCrkwyd8pn+Th6tBvLfnOQpXaN2NpQ4mbdUorlUQg4l1hFo7ylLdCb5JsemkzcdTm8ZtIStHmcLwvPFKRBB9YwjojGoMkg6uIt+lFrvc6+jCX779vlH+wfTY5si7fl1u+v6/k95ZKP8xOpjbF82r8+b1Tbm/LexSFt2WVfhQudgs9TvyO1prxEqbjv3vWU9ykWRH5SNLQbiAMyIl0cnbebOO4VN1TV41GZz4cDhf8CGH/02rF69q83+kbnro1TD+rry9BjqkAc5Wt48iKu0OVlZBPAEokg1crLv86kGBmYfpoMDMFJX7nxCnD1bkfVaoHxxBnSgOiC3atQVYf7xLlJvMZ4wifxVhWyUGIpsQt8S0nlTjRD8b3F6SRkafBOcaygaIlNoMbl1DdjMbYKQJ7rPcYFzhYIERFp6JKFvU04hgnb50bEjsJiPk0NcnIZbqteWcgAs43lx6aQz6WMjOiEhT4KcVVGEWp+MX&X-Amz-Signature=33c110b762ed812b4f43e4f19213627c791460cb98893ecd87300343f9b70c11&X-Amz-SignedHeaders=host&x-id=GetObject" alt="image"></p>
<p>每种加载器都专门处理特定类型的文档，但它们都会将文档转换成统一的Document格式。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载文本文件示例</span></span><br><span class="line">loader = TextLoader(<span class="string">&quot;./data.txt&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">documents = loader.load()</span><br></pre></td></tr></table></figure>


<h1 id="异步加载支持"><a href="#异步加载支持" class="headerlink" title="异步加载支持"></a>异步加载支持</h1><p>对于大型文档，LangChain提供了异步加载方式：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">load_documents</span>():</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiofiles.<span class="built_in">open</span>(file_path, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 异步处理文档</span></span><br><span class="line">        <span class="keyword">yield</span> Document(</span><br><span class="line">            page_content=line,</span><br><span class="line">            metadata=&#123;<span class="string">&quot;source&quot;</span>: file_path&#125;</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>


<h1 id="文档转换器与分割器"><a href="#文档转换器与分割器" class="headerlink" title="文档转换器与分割器"></a>文档转换器与分割器</h1><h1 id="DocumentTransformer-组件"><a href="#DocumentTransformer-组件" class="headerlink" title="DocumentTransformer 组件"></a>DocumentTransformer 组件</h1><p>文档转换器用于处理以下常见问题：</p>
<ol>
<li><p>文档太大导致的性能问题</p>
</li>
<li><p>原始文档格式不符合要求</p>
</li>
<li><p>文档内容需要标准化处理</p>
</li>
</ol>
<h1 id="文档转换器的工作原理"><a href="#文档转换器的工作原理" class="headerlink" title="文档转换器的工作原理"></a>文档转换器的工作原理</h1><p>DocumentTransformer组件的主要职责是对文档进行各种转换操作，包括：</p>
<ol>
<li><p>文档切割</p>
</li>
<li><p>文档层级提取</p>
</li>
<li><p>文档翻译</p>
</li>
<li><p>HTML标签处理</p>
</li>
<li><p>重排等多个功能</p>
</li>
</ol>
<p>在LangChain中，所有文档转换器都继承自BaseDocumentTransformer基类，它提供了两个核心方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BaseDocumentTransformer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transform_documents</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="comment"># 转换文档列表</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">atransform_documents</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 异步转换处理</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>


<h1 id="文档分割器详解"><a href="#文档分割器详解" class="headerlink" title="文档分割器详解"></a>文档分割器详解</h1><h1 id="字符分割器（CharacterTextSplitter）"><a href="#字符分割器（CharacterTextSplitter）" class="headerlink" title="字符分割器（CharacterTextSplitter）"></a>字符分割器（CharacterTextSplitter）</h1><p>CharacterTextSplitter 是基础的分割器，它有以下重要参数：</p>
<ol>
<li><p><code>separator</code>: 分割符,默认为’\n\n’</p>
</li>
<li><p><code>chunk_size</code>: 每块文本的最大大小,默认4000</p>
</li>
<li><p><code>chunk_overlap</code>: 块与块之间的重叠大小,默认200</p>
</li>
<li><p><code>length_function</code>: 计算文本长度的函数,默认len</p>
</li>
<li><p><code>keep_separator</code>: 是否在分割的块中保留分隔符</p>
</li>
</ol>
<p>使用示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"></span><br><span class="line">text_splitter = CharacterTextSplitter(</span><br><span class="line">    separator=<span class="string">&quot;\n\n&quot;</span>,</span><br><span class="line">    chunk_size=<span class="number">500</span>,</span><br><span class="line">    chunk_overlap=<span class="number">50</span>,</span><br><span class="line">    add_start_index=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用分割器处理文档</span></span><br><span class="line">splits = text_splitter.split_documents(documents)</span><br></pre></td></tr></table></figure>


<h1 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a>实践建议</h1><p>在实际应用中，有以下几点建议：</p>
<ol>
<li><p>选择合适的chunk_size</p>
<ul>
<li>太大会影响处理效率</li>
<li>太小可能破坏语义完整性</li>
<li>建议根据实际需求在400-1000之间调整</li>
</ul>
</li>
<li><p>合理设置overlap</p>
<ul>
<li>设置适当的重叠可以保持上下文连贯</li>
<li>通常设置为chunk_size的10%-20%</li>
</ul>
</li>
<li><p>注意分隔符的选择</p>
<ul>
<li>根据文档类型选择合适的分隔符</li>
<li>可以使用多级分隔符策略</li>
</ul>
</li>
</ol>
<h1 id="VectorStore组件与检索器"><a href="#VectorStore组件与检索器" class="headerlink" title="VectorStore组件与检索器"></a>VectorStore组件与检索器</h1><h1 id="VectorStore基础概念"><a href="#VectorStore基础概念" class="headerlink" title="VectorStore基础概念"></a>VectorStore基础概念</h1><p>VectorStore组件负责：</p>
<ol>
<li><p>存储文档的向量表示</p>
</li>
<li><p>提供相似性检索功能</p>
</li>
<li><p>支持不同的向量检索策略</p>
</li>
</ol>
<h1 id="检索器的使用"><a href="#检索器的使用" class="headerlink" title="检索器的使用"></a>检索器的使用</h1><p>LangChain 提供了多种检索策略：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> VectorStore</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基础相似性检索</span></span><br><span class="line">results = vectorstore.similarity_search(query)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 带相似度分数的检索</span></span><br><span class="line">results = vectorstore.similarity_search_with_score(query)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MMR检索策略</span></span><br><span class="line">results = vectorstore.max_marginal_relevance_search(query)</span><br></pre></td></tr></table></figure>


<h1 id="VectorStore实现细节"><a href="#VectorStore实现细节" class="headerlink" title="VectorStore实现细节"></a>VectorStore实现细节</h1><h1 id="支持的向量数据库"><a href="#支持的向量数据库" class="headerlink" title="支持的向量数据库"></a>支持的向量数据库</h1><ul>
<li><p>Chroma</p>
</li>
<li><p>FAISS</p>
</li>
<li><p>Pinecone</p>
</li>
<li><p>Milvus</p>
</li>
</ul>
<h1 id="检索策略详解"><a href="#检索策略详解" class="headerlink" title="检索策略详解"></a>检索策略详解</h1><ol>
<li><p>相似度检索(Similarity Search)</p>
<ul>
<li>基于余弦相似度</li>
<li>支持Top-K检索</li>
</ul>
</li>
<li><p>MMR检索(Maximum Marginal Relevance)</p>
<ul>
<li>平衡相关性和多样性</li>
<li>可配置lambda参数调整权重</li>
</ul>
</li>
<li><p>混合检索策略</p>
<ul>
<li>关键词+语义检索</li>
<li>支持自定义评分函数</li>
</ul>
</li>
</ol>
<h1 id="Blob与BlobParser组件"><a href="#Blob与BlobParser组件" class="headerlink" title="Blob与BlobParser组件"></a>Blob与BlobParser组件</h1><h1 id="Blob方案介绍"><a href="#Blob方案介绍" class="headerlink" title="Blob方案介绍"></a>Blob方案介绍</h1><p>Blob是LangChain处理二进制数据的解决方案，它具有以下特点：</p>
<ol>
<li><p>支持存储字节流数据</p>
</li>
<li><p>提供统一的数据访问接口</p>
</li>
<li><p>灵活的元数据管理</p>
</li>
</ol>
<p>基本使用示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.document_loaders <span class="keyword">import</span> Blob</span><br><span class="line"><span class="keyword">from</span> langchain_core.document_loaders.base <span class="keyword">import</span> BaseBlobParser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Blob对象</span></span><br><span class="line">blob = Blob.from_path(<span class="string">&quot;./data.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用解析器</span></span><br><span class="line">parser = CustomParser()</span><br><span class="line">documents = <span class="built_in">list</span>(parser.lazy_parse(blob))</span><br></pre></td></tr></table></figure>


<h1 id="Blob数据存储类详解"><a href="#Blob数据存储类详解" class="headerlink" title="Blob数据存储类详解"></a>Blob数据存储类详解</h1><p>LangChain中的Blob数据存储提供了丰富的属性和方法，让我们详细了解一下：</p>
<p><strong>核心属性</strong></p>
<ol>
<li><p><strong>data</strong>: 原始数据，支持存储字节，字符串数据</p>
</li>
<li><p><strong>mimetype</strong>: 文件的mimetype类型</p>
</li>
<li><p><strong>encoding</strong>: 文件的编码，默认utf-8</p>
</li>
<li><p><strong>path</strong>: 文件的原始路径</p>
</li>
<li><p><strong>metadata</strong>: 存储的元数据，通常包含source字段</p>
</li>
</ol>
<p><strong>常用方法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 字符串转换</span></span><br><span class="line">as_string(): <span class="comment"># 将数据转换为字符串</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 字节转换</span></span><br><span class="line">as_bytes(): <span class="comment"># 将数据转换为字节数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 字节流操作</span></span><br><span class="line">as_bytes_io(): <span class="comment"># 将数据转换为字节流</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从路径加载</span></span><br><span class="line">from_path(): <span class="comment"># 从文件路径加载Blob数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从原始数据加载</span></span><br><span class="line">from_data(): <span class="comment"># 从原始数据加载Blob数据</span></span><br></pre></td></tr></table></figure>


<h1 id="BlobLoader实现"><a href="#BlobLoader实现" class="headerlink" title="BlobLoader实现"></a>BlobLoader实现</h1><p>BlobLoader是一个抽象接口，用于实现二进制数据的加载。以下是一个自定义BlobLoader的示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.document_loaders <span class="keyword">import</span> Blob</span><br><span class="line"><span class="keyword">from</span> langchain_core.document_loaders.base <span class="keyword">import</span> BaseBlobParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomBlobLoader</span>(<span class="title class_ inherited__">ABC</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义Blob加载器实现&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">yield_blobs</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">    </span>) -&gt; Iterable[Blob]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;加载并返回Blob数据流&quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.file_path = file_path</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lazy_load</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;延迟加载实现&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> blob <span class="keyword">in</span> <span class="variable language_">self</span>.yield_blobs():</span><br><span class="line">            <span class="keyword">yield</span> Document(</span><br><span class="line">                page_content=blob.as_string(),</span><br><span class="line">                metadata=&#123;<span class="string">&quot;source&quot;</span>: blob.source&#125;</span><br><span class="line">            )</span><br></pre></td></tr></table></figure>


<h1 id="通用加载器使用最佳实践"><a href="#通用加载器使用最佳实践" class="headerlink" title="通用加载器使用最佳实践"></a>通用加载器使用最佳实践</h1><p>GenericLoader是LangChain提供的一个通用加载器，它结合了BlobLoader和BaseBlobParser的功能：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders.generic <span class="keyword">import</span> GenericLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建通用加载器</span></span><br><span class="line">loader = GenericLoader.from_filesystem(</span><br><span class="line">    <span class="string">&quot;./&quot;</span>,  <span class="comment"># 文件系统路径</span></span><br><span class="line">    glob=<span class="string">&quot;*.txt&quot;</span>,  <span class="comment"># 文件匹配模式</span></span><br><span class="line">    show_progress=<span class="literal">True</span>  <span class="comment"># 显示进度</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用加载器</span></span><br><span class="line"><span class="keyword">for</span> idx, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader.lazy_load()):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;当前加载第<span class="subst">&#123;idx + <span class="number">1</span>&#125;</span>个文件, 文件信息:<span class="subst">&#123;doc.metadata&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>


<h1 id="性能优化建议"><a href="#性能优化建议" class="headerlink" title="性能优化建议"></a>性能优化建议</h1>
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/04/23/LangChain%20RAG%20%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E7%BB%84%E4%BB%B6%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/" data-id="cm9tdj3f3000280w87zqac0l5" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-LangChain RAG 应用开发优化策略详解" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/04/23/LangChain%20RAG%20%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%E8%AF%A6%E8%A7%A3/" class="article-date">
  <time class="dt-published" datetime="2025-04-23T03:02:04.019Z" itemprop="datePublished">2025-04-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="引言：理解RAG及其重要性"><a href="#引言：理解RAG及其重要性" class="headerlink" title="引言：理解RAG及其重要性"></a>引言：理解RAG及其重要性</h1><p>在大语言模型（LLM）应用开发中，检索增强生成（Retrival-Augmented Generation, RAG）已经成为提升模型输出质量的关键技术。本文将深入探讨在LangChain框架中如何优化RAG应用，帮助开发者构建更智能、更准确的AI应用。</p>
<h1 id="RAG的基本概念"><a href="#RAG的基本概念" class="headerlink" title="RAG的基本概念"></a>RAG的基本概念</h1><blockquote>
<p>📌 什么是RAG?<br>RAG是一种将外部知识检索与语言模型生成相结合的技术架构。它通过检索相关信息来增强LLM的知识储备，从而产生更准确、更可靠的输出。</p>
</blockquote>
<h1 id="为什么需要优化RAG？"><a href="#为什么需要优化RAG？" class="headerlink" title="为什么需要优化RAG？"></a>为什么需要优化RAG？</h1><p>在实际应用中，基础的RAG实现往往会遇到以下挑战：</p>
<ol>
<li><p>检索准确性不足</p>
</li>
<li><p>复杂问题处理能力有限</p>
</li>
<li><p>知识关联不够紧密</p>
</li>
<li><p>响应质量不够稳定</p>
</li>
</ol>
<p>这些问题促使我们需要采用多种优化策略来提升RAG的性能。</p>
<h1 id="第一部分：多查询检索优化策略"><a href="#第一部分：多查询检索优化策略" class="headerlink" title="第一部分：多查询检索优化策略"></a>第一部分：多查询检索优化策略</h1><h1 id="理解多查询检索的必要性"><a href="#理解多查询检索的必要性" class="headerlink" title="理解多查询检索的必要性"></a>理解多查询检索的必要性</h1><p>在RAG应用中，单一查询往往无法完整捕捉用户问题的所有方面。例如，当用户问”Python如何实现多线程并发控制？“时，我们可能需要同时检索：</p>
<ul>
<li><p>Python线程基础知识</p>
</li>
<li><p>并发控制机制</p>
</li>
<li><p>线程安全实现方法</p>
</li>
</ul>
<h1 id="多查询检索的工作原理"><a href="#多查询检索的工作原理" class="headerlink" title="多查询检索的工作原理"></a>多查询检索的工作原理</h1><blockquote>
<p>🔍 核心思路：利用LLM的理解能力，将一个复杂查询拆分或重写为多个相关查询，然后通过融合算法整合检索结果。</p>
</blockquote>
<p><strong>工作流程</strong>：</p>
<ol>
<li><p><strong>查询重写</strong>：LLM将原始查询转换为多个相关查询</p>
</li>
<li><p><strong>并行检索</strong>：对每个查询进行独立检索</p>
</li>
<li><p><strong>结果融合</strong>：使用RRF（Reciprocal Rank Fusion）算法融合检索结果</p>
</li>
<li><p><strong>内容生成</strong>：将融合后的结果输入LLM生成最终答案</p>
</li>
</ol>
<h1 id="代码实现示例"><a href="#代码实现示例" class="headerlink" title="代码实现示例"></a>代码实现示例</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> MultiQueryRetriever</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> LLMChain</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 创建多查询检索器</span></span><br><span class="line">retriever = MultiQueryRetriever(</span><br><span class="line">    retriever=base_retriever,</span><br><span class="line">    llm=ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>),</span><br><span class="line">    prompt_template=<span class="string">&quot;&quot;&quot;基于用户的问题，生成3个不同的相关查询：</span></span><br><span class="line"><span class="string">    原始问题: &#123;question&#125;</span></span><br><span class="line"><span class="string">    生成的查询应该探索问题的不同方面。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 使用RRF算法融合结果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rrf_fusion</span>(<span class="params">results, k=<span class="number">60</span></span>):</span><br><span class="line">    fused_scores = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> rank, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(results):</span><br><span class="line">        doc_str = doc.page_content</span><br><span class="line">        <span class="keyword">if</span> doc_str <span class="keyword">not</span> <span class="keyword">in</span> fused_scores:</span><br><span class="line">            fused_scores[doc_str] = <span class="number">1.0</span> / (k + rank + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            fused_scores[doc_str] += <span class="number">1.0</span> / (k + rank + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 排序并返回结果</span></span><br><span class="line">    sorted_results = <span class="built_in">sorted</span>(fused_scores.items(), </span><br><span class="line">                          key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], </span><br><span class="line">                          reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted_results</span><br></pre></td></tr></table></figure>


<p>RRF 算法原理如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">RRF (Reciprocal Rank Fusion) 算法的核心公式：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">RRFscore(d ∈ D) = ∑ 1/(k + r(d))</span></span><br><span class="line"><span class="string">其中：</span></span><br><span class="line"><span class="string">- d 是文档</span></span><br><span class="line"><span class="string">- D 是所有文档集合</span></span><br><span class="line"><span class="string">- k 是一个常数(通常取60)</span></span><br><span class="line"><span class="string">- r(d)是文档d在排序中的位置</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">这个公式的特点：</span></span><br><span class="line"><span class="string">1. 对排名靠前的文档给予更高的权重</span></span><br><span class="line"><span class="string">2. k参数可以调节排名的影响程度</span></span><br><span class="line"><span class="string">3. 适合融合不同来源的排序结果</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>


<h1 id="优化效果分析"><a href="#优化效果分析" class="headerlink" title="优化效果分析"></a>优化效果分析</h1><p>多查询检索策略带来的主要优势：</p>
<ol>
<li><p><strong>提升召回率</strong></p>
<ul>
<li>通过多角度查询提高相关文档的覆盖率</li>
<li>减少因单一查询表达不当导致的漏检</li>
</ul>
</li>
<li><p><strong>提高准确性</strong></p>
<ul>
<li>RRF融合算法可以突出高质量的共同结果</li>
<li>降低单个查询的噪声影响</li>
</ul>
</li>
<li><p><strong>增强鲁棒性</strong></p>
<ul>
<li>对查询表达的变化更不敏感</li>
<li>能更好地处理复杂或模糊的问题</li>
</ul>
</li>
</ol>
<h1 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a>实践建议</h1><p>在实际应用中，需要注意以下几点：</p>
<ul>
<li><p><strong>查询数量选择</strong>：通常生成3-5个查询即可，过多查询可能引入噪声</p>
</li>
<li><p><strong>相似度阈值设置</strong>：建议在RRF融合时设置合适的相似度阈值，过滤低相关性结果</p>
</li>
<li><p><strong>资源消耗考虑</strong>：多查询会增加API调用和计算资源，需要在效果和成本间权衡</p>
</li>
</ul>
<blockquote>
<p>💡 实践小贴士：可以通过监控检索结果的diversity和relevance指标，来调整多查询策略的参数。</p>
</blockquote>
<h1 id="第二部分：问题分解策略优化"><a href="#第二部分：问题分解策略优化" class="headerlink" title="第二部分：问题分解策略优化"></a>第二部分：问题分解策略优化</h1><h1 id="复杂问题的分解处理"><a href="#复杂问题的分解处理" class="headerlink" title="复杂问题的分解处理"></a>复杂问题的分解处理</h1><p>在实际应用中，我们经常遇到复杂的多层次问题。例如：”请分析特斯拉近五年的财务状况，并评估其在电动汽车市场的竞争优势。”这类问题需要：</p>
<ul>
<li><p>处理大量相关信息</p>
</li>
<li><p>分析多个维度</p>
</li>
<li><p>综合多方面结论</p>
</li>
</ul>
<h1 id="并行分解模式"><a href="#并行分解模式" class="headerlink" title="并行分解模式"></a><strong>并行分解模式</strong></h1><blockquote>
<p>🔄 并行模式：将问题同时分解为多个独立子问题，分别获取答案后合并。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 并行分解示例</span></span><br><span class="line">decomposition_chain = &#123;</span><br><span class="line">    <span class="string">&quot;question&quot;</span>: RunnablePassthrough(),</span><br><span class="line">    | decomposition_prompt    <span class="comment"># 分解问题</span></span><br><span class="line">    | ChatOpenAI(temperature=<span class="number">0</span>)</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 并行处理子问题</span></span><br><span class="line">sub_questions = decomposition_chain.invoke(question)</span><br><span class="line">answers = <span class="keyword">await</span> asyncio.gather(*[</span><br><span class="line">    process_subquestion(q) <span class="keyword">for</span> q <span class="keyword">in</span> sub_questions</span><br><span class="line">])</span><br></pre></td></tr></table></figure>


<h1 id="串行分解模式"><a href="#串行分解模式" class="headerlink" title="串行分解模式"></a><strong>串行分解模式</strong></h1><blockquote>
<p>⛓️ 串行模式：按照逻辑顺序依次处理子问题，后面的问题依赖前面的答案。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 串行分解示例</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StepBackRetriever</span>(<span class="title class_ inherited__">BaseRetriever</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_relevant_documents</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, query: <span class="built_in">str</span>, *, run_manager: CallbackManagerForRetrieverRun</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">List</span>[Document]:</span><br><span class="line">        <span class="comment"># 1. 生成中间查询</span></span><br><span class="line">        intermediate_query = <span class="variable language_">self</span>.llm.predict(</span><br><span class="line">            <span class="string">f&quot;为了回答&#x27;<span class="subst">&#123;query&#125;</span>&#x27;，我们需要先了解什么？&quot;</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 检索中间知识</span></span><br><span class="line">        intermediate_docs = <span class="variable language_">self</span>.retriever.get_relevant_documents(</span><br><span class="line">            intermediate_query</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 基于中间知识检索最终答案</span></span><br><span class="line">        final_docs = <span class="variable language_">self</span>.retriever.get_relevant_documents(query)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> intermediate_docs + final_docs</span><br></pre></td></tr></table></figure>


<h1 id="Step-Back-策略实现"><a href="#Step-Back-策略实现" class="headerlink" title="Step-Back 策略实现"></a>Step-Back 策略实现</h1><p>Step-Back策略是一种特殊的串行分解方法，它通过“后退一步”来获取更基础的知识背景。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">示例：用户问题&quot;量子计算机如何影响现代密码学？&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Step-Back分解：</span></span><br><span class="line"><span class="string">1. 基础知识查询：</span></span><br><span class="line"><span class="string">   - 什么是量子计算机的基本原理？</span></span><br><span class="line"><span class="string">   - 现代密码学的核心技术有哪些？</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2. 关联分析：</span></span><br><span class="line"><span class="string">   - 量子计算对RSA等算法的影响</span></span><br><span class="line"><span class="string">   - 后量子密码学的发展</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">3. 最终综合：</span></span><br><span class="line"><span class="string">   基于以上知识形成完整答案</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>


<p><strong>工作流程</strong>：</p>
<ol>
<li><p>分析原始问题</p>
</li>
<li><p>生成更基础的前置问题</p>
</li>
<li><p>获取基础知识</p>
</li>
<li><p>结合基础知识回答原问题</p>
</li>
</ol>
<h1 id="Step-Back-代码实现"><a href="#Step-Back-代码实现" class="headerlink" title="Step-Back 代码实现"></a>Step-Back 代码实现</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">system_prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">你是一位专业的助手，需要：</span></span><br><span class="line"><span class="string">1. 理解用户的具体问题</span></span><br><span class="line"><span class="string">2. 思考需要哪些基础知识</span></span><br><span class="line"><span class="string">3. 生成相关的基础问题</span></span><br><span class="line"><span class="string">4. 基于基础知识回答原问题</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">few_shot_prompt = FewShotChatMessagePromptTemplate(</span><br><span class="line">    example_prompt=example_prompt,</span><br><span class="line">    examples=examples,</span><br><span class="line">    suffix=<span class="string">&quot;现在，请帮我回答：&#123;question&#125;&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<h1 id="优化效果对比"><a href="#优化效果对比" class="headerlink" title="优化效果对比"></a>优化效果对比</h1><table>
<thead>
<tr>
<th>分解策略</th>
<th>适用场景</th>
<th>优势</th>
<th>劣势</th>
</tr>
</thead>
<tbody><tr>
<td>并行分解</td>
<td>独立子问题</td>
<td>处理速度快，资源利用高</td>
<td>结果整合可能不够连贯</td>
</tr>
<tr>
<td>串行分解</td>
<td>逻辑依赖性强</td>
<td>答案更连贯，逻辑性强</td>
<td>处理时间较长</td>
</tr>
<tr>
<td>Step-Back</td>
<td>需要深入理解</td>
<td>回答更全面，准确度高</td>
<td>资源消耗较大</td>
</tr>
</tbody></table>
<h1 id="实践建议-1"><a href="#实践建议-1" class="headerlink" title="实践建议"></a>实践建议</h1><ol>
<li><p>选择策略时考虑因素: </p>
<ul>
<li>问题的复杂度</li>
<li>子问题间的依赖关系</li>
<li>响应时间要求</li>
<li>资源限制</li>
</ul>
</li>
<li><p>优化建议：</p>
<ul>
<li>对于并行模式，注意结果融合的质量</li>
<li>串行模式要控制分解的层级深度</li>
<li>Step-Back策略要平衡基础知识的范围</li>
</ul>
</li>
</ol>
<blockquote>
<p>🌟 最佳实践：可以根据问题类型动态选择分解策略，甚至组合使用多种策略。</p>
</blockquote>
<h1 id="第三部分：混合检索策略实现"><a href="#第三部分：混合检索策略实现" class="headerlink" title="第三部分：混合检索策略实现"></a>第三部分：混合检索策略实现</h1><h1 id="理解混合检索的价值"><a href="#理解混合检索的价值" class="headerlink" title="理解混合检索的价值"></a>理解混合检索的价值</h1><p>在实际应用中，单一的检索方法往往难以应对所有场景。例如：</p>
<ul>
<li><p>语义检索擅长理解上下文，但可能错过关键词</p>
</li>
<li><p>关键词检索准确度高，但缺乏语义理解</p>
</li>
<li><p>密集检索和稀疏检索各有优势</p>
</li>
</ul>
<p>因此，将多种检索方法结合起来，可以取长补短，提升整体检索效果。</p>
<h1 id="混合检索器的架构设计"><a href="#混合检索器的架构设计" class="headerlink" title="混合检索器的架构设计"></a>混合检索器的架构设计</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> EnsembleRetriever</span><br><span class="line"><span class="keyword">from</span> langchain_community.retrievers <span class="keyword">import</span> BM25Retriever</span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 创建不同类型的检索器</span></span><br><span class="line"><span class="comment"># BM25检索器（基于关键词）</span></span><br><span class="line">bm25_retriever = BM25Retriever.from_documents(</span><br><span class="line">    documents, k=<span class="number">4</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FAISS检索器（基于向量）</span></span><br><span class="line">faiss_retriever = FAISS.from_documents(</span><br><span class="line">    documents,</span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)</span><br><span class="line">).as_retriever(search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">4</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建集成检索器</span></span><br><span class="line">ensemble_retriever = EnsembleRetriever(</span><br><span class="line">    retrievers=[bm25_retriever, faiss_retriever],</span><br><span class="line">    weights=[<span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>


<h1 id="主要检索方法的特点"><a href="#主要检索方法的特点" class="headerlink" title="主要检索方法的特点"></a>主要检索方法的特点</h1><p>下面是几种常用检索方法的对比：</p>
<table>
<thead>
<tr>
<th>检索方法</th>
<th>优势</th>
<th>适用场景</th>
<th>注意事项</th>
</tr>
</thead>
<tbody><tr>
<td>BM25</td>
<td>精确匹配，速度快</td>
<td>关键词搜索</td>
<td>不理解语义变化</td>
</tr>
<tr>
<td>向量检索</td>
<td>理解语义相似</td>
<td>概念搜索</td>
<td>计算资源消耗大</td>
</tr>
<tr>
<td>混合检索</td>
<td>综合优势</td>
<td>复杂查询</td>
<td>需要调整权重</td>
</tr>
</tbody></table>
<h1 id="实现细节和优化"><a href="#实现细节和优化" class="headerlink" title="实现细节和优化"></a>实现细节和优化</h1><p><strong>检索器配置</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置检索参数</span></span><br><span class="line">faiss_retriever = faiss_db.as_retriever(</span><br><span class="line">    search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">4</span>&#125;</span><br><span class="line">).configurable_fields(</span><br><span class="line">    search_kwargs=ConfigurableField(</span><br><span class="line">        <span class="built_in">id</span>=<span class="string">&quot;search_kwargs_faiss&quot;</span>,</span><br><span class="line">        name=<span class="string">&quot;检索参数&quot;</span>,</span><br><span class="line">        description=<span class="string">&quot;设置检索的参数&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置运行时配置</span></span><br><span class="line">config = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;search_kwargs_faiss&quot;</span>: &#123;<span class="string">&quot;k&quot;</span>: <span class="number">4</span>&#125;&#125;&#125;</span><br><span class="line">docs = ensemble_retriever.invoke(<span class="string">&quot;查询&quot;</span>, config=config)</span><br></pre></td></tr></table></figure>


<p><strong>权重调整策略</strong></p>
<ol>
<li><p><strong>初始设置</strong>：开始时可以给各检索器相同权重</p>
</li>
<li><p><strong>动态调整</strong>：根据查询类型动态调整权重</p>
</li>
<li><p><strong>性能监控</strong>：跟踪各检索器的表现，定期优化权重</p>
</li>
<li><p><strong>场景适配</strong>：针对不同领域调整最优权重组合</p>
</li>
</ol>
<h1 id="应用效果优化"><a href="#应用效果优化" class="headerlink" title="应用效果优化"></a>应用效果优化</h1><p>为了获得最佳检索效果，建议：</p>
<ol>
<li><p>检索器选择</p>
<ul>
<li>根据数据特点选择合适的检索器组合</li>
<li>考虑计算资源和响应时间的平衡</li>
<li>评估检索器的互补性</li>
</ul>
</li>
<li><p>参数优化</p>
<ul>
<li>使用验证集调整检索参数</li>
<li>监控检索质量指标</li>
<li>定期更新检索模型</li>
</ul>
</li>
<li><p>结果融合</p>
<ul>
<li>采用多样化的融合策略</li>
<li>考虑结果的去重和排序</li>
<li>平衡相关性和多样性</li>
</ul>
</li>
</ol>
<h1 id="性能监控与改进"><a href="#性能监控与改进" class="headerlink" title="性能监控与改进"></a>性能监控与改进</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 性能监控示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_retrieval</span>(<span class="params">retriever, test_queries, ground_truth</span>):</span><br><span class="line">    metrics = &#123;</span><br><span class="line">        <span class="string">&#x27;precision&#x27;</span>: [],</span><br><span class="line">        <span class="string">&#x27;recall&#x27;</span>: [],</span><br><span class="line">        <span class="string">&#x27;latency&#x27;</span>: []</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> query, truth <span class="keyword">in</span> <span class="built_in">zip</span>(test_queries, ground_truth):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        results = retriever.get_relevant_documents(query)</span><br><span class="line">        latency = time.time() - start_time</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算评估指标</span></span><br><span class="line">        metrics[<span class="string">&#x27;latency&#x27;</span>].append(latency)</span><br><span class="line">        <span class="comment"># ... 计算precision和recall</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> metrics</span><br></pre></td></tr></table></figure>


<h1 id="总结：RAG优化策略的实践指南"><a href="#总结：RAG优化策略的实践指南" class="headerlink" title="总结：RAG优化策略的实践指南"></a>总结：RAG优化策略的实践指南</h1><h1 id="优化策略的综合比较"><a href="#优化策略的综合比较" class="headerlink" title="优化策略的综合比较"></a>优化策略的综合比较</h1><p>以下是我们讨论过的主要优化策略的特点对比：</p>
<table>
<thead>
<tr>
<th>优化策略</th>
<th>主要优势</th>
<th>实现复杂度</th>
<th>资源消耗</th>
<th>适用场景</th>
</tr>
</thead>
<tbody><tr>
<td>多查询检索</td>
<td>提高召回率</td>
<td>中等</td>
<td>中等</td>
<td>复杂查询、模糊问题</td>
</tr>
<tr>
<td>问题分解</td>
<td>提升理解深度</td>
<td>较高</td>
<td>较高</td>
<td>多维度分析问题</td>
</tr>
<tr>
<td>Step-Back</td>
<td>增强理解准确性</td>
<td>高</td>
<td>高</td>
<td>需要深入理解的问题</td>
</tr>
<tr>
<td>混合检索</td>
<td>综合性能提升</td>
<td>中等</td>
<td>较高</td>
<td>通用场景</td>
</tr>
</tbody></table>
<h1 id="优化路径建议"><a href="#优化路径建议" class="headerlink" title="优化路径建议"></a>优化路径建议</h1><ol>
<li><strong>基础阶段</strong><ul>
<li>实现基本的RAG流程</li>
<li>优化向量检索参数</li>
<li>改进提示词设计</li>
</ul>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/04/23/LangChain%20RAG%20%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%E8%AF%A6%E8%A7%A3/" data-id="cm9tdj3f1000180w8gg0158uo" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-AI" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/04/23/AI/" class="article-date">
  <time class="dt-published" datetime="2025-04-23T03:02:04.017Z" itemprop="datePublished">2025-04-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/04/23/AI/" data-id="cm9tdj3ev000080w8gy09e4u2" data-title="" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-我的第一篇博客" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/04/23/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" class="article-date">
  <time class="dt-published" datetime="2025-04-23T03:00:43.000Z" itemprop="datePublished">2025-04-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/04/23/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/">我的第一篇博客</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        
      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/04/23/%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/" data-id="cm9tdj3fb000780w89z09e237" data-title="我的第一篇博客" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/04/23/hello-world/" class="article-date">
  <time class="dt-published" datetime="2025-04-23T02:59:29.275Z" itemprop="datePublished">2025-04-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/04/23/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2025/04/23/hello-world/" data-id="cm9tdj3f4000480w8fjad8wcj" data-title="Hello World" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/04/">April 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/04/23/%E5%A4%AF%E5%AE%9E%E5%9F%BA%E7%A1%80%E4%BA%86%E8%A7%A3LLm%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/04/23/%E5%A4%A7%E6%A8%A1%E5%9E%8B%20RAG%20%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E5%9F%BA%E7%A1%80%E5%8F%8A%E5%85%A5%E9%97%A8/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/04/23/LangChain%E5%88%9D%E5%85%A5%E9%97%A8/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/04/23/LangChain%20RAG%20%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E7%BB%84%E4%BB%B6%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/">(no title)</a>
          </li>
        
          <li>
            <a href="/2025/04/23/LangChain%20RAG%20%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%E8%AF%A6%E8%A7%A3/">(no title)</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>