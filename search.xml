<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>大模型 RAG 应用开发基础及入门</title>
      <link href="/2025/02/12/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/"/>
      <url>/2025/02/12/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/</url>
      
        <content type="html"><![CDATA[<h1 id="大语言模型中的幻觉问题-上"><a href="#大语言模型中的幻觉问题-上" class="headerlink" title="大语言模型中的幻觉问题(上)"></a>大语言模型中的幻觉问题(上)</h1><h2 id="什么是大语言模型的幻觉？"><a href="#什么是大语言模型的幻觉？" class="headerlink" title="什么是大语言模型的幻觉？"></a>什么是大语言模型的幻觉？</h2><p>大语言模型在处理自然语言时，有时候会出现”幻觉“现象。所谓幻觉，就是模型生成的内容与事实或上下文不一致的问题。这些问题会严重影响AI应用的可靠性和实用性。</p><h2 id="幻觉的两大类型"><a href="#幻觉的两大类型" class="headerlink" title="幻觉的两大类型"></a>幻觉的两大类型</h2><h3 id="事实性幻觉"><a href="#事实性幻觉" class="headerlink" title="事实性幻觉"></a>事实性幻觉</h3><p>指模型生成的内容与实际事实不匹配。比如在回答”第一个登上月球的人是谁?”这个问题时:</p><ul><li>错误回答: “Charles Lindbergh在1951年月球任务中第一个登上月球”</li><li>正确事实: Neil Armstrong才是第一个登上月球的人(1969年阿波罗11号任务)</li></ul><p>这种幻觉之所以危险，是因为模型生成的内容看起来很可信，但实际上完全错误。</p><h3 id="忠实性幻觉"><a href="#忠实性幻觉" class="headerlink" title="忠实性幻觉"></a>忠实性幻觉</h3><p>指模型生成的内容与提供的上下文不一致。这种幻觉可以分为三类：</p><ul><li>输出与原文不一致（编出原文中没有的信息）</li><li>上下文之间不一致（前后矛盾）</li><li>逻辑链不一致（推理过程存在漏洞）</li></ul><p>比如在总结新闻时，模型可能会添加原文中不存在的细节，或者前后描述矛盾。</p><h2 id="为什么会产生幻觉？"><a href="#为什么会产生幻觉？" class="headerlink" title="为什么会产生幻觉？"></a>为什么会产生幻觉？</h2><p>大语言模型产生幻觉的原因主要来自三个方面：</p><ol><li>数据源导致的幻觉<ul><li>训练数据中的质量问题</li><li>数据中存在的错误信息</li><li>数据覆盖范围有限</li></ul></li><li>训练过程导致的幻觉<ul><li>架构限制：无法准确理解长文本的上下文关联</li><li>累积错误：生成过程中的错误会逐步传递和放大</li></ul></li><li>推理相关的幻觉<ul><li>回答过于简略</li><li>生成过程中的不完整推理</li></ul></li></ol><h2 id="如何评估幻觉问题"><a href="#如何评估幻觉问题" class="headerlink" title="如何评估幻觉问题"></a>如何评估幻觉问题</h2><p>为了客观评估模型的幻觉问题，我们可以使用多种方法：</p><ol><li>事实一致性评估：将生成内容与权威来源进行比对</li><li>分类器评估：使用专门训练的模型来检测是否存在幻觉</li><li>问答测量：通过问答来验证生成内容的一致性</li><li>不确定度分析：评估模型对自身输出的确信程度</li><li>提示测量：让模型自我评估，通过特定提示策略来评估生成内容</li></ol><h1 id="大语言模型中的幻觉问题-下-："><a href="#大语言模型中的幻觉问题-下-：" class="headerlink" title="大语言模型中的幻觉问题(下)："></a>大语言模型中的幻觉问题(下)：</h1><h2 id="RAG解决方案"><a href="#RAG解决方案" class="headerlink" title="RAG解决方案"></a>RAG解决方案</h2><h3 id="RAG是什么？"><a href="#RAG是什么？" class="headerlink" title="RAG是什么？"></a>RAG是什么？</h3><p><strong>RAG</strong>（Retrieval-Augmented Generation）也叫<strong>检索增强生成</strong>，是指对大语言模型输出进行优化，使其能够参考并利用数据源之外的权威知识。简单来说，RAG就是从外部检索对应的知识内容，和用户的提问一起构成Prompt发给大模型，再让大模型生成内容。</p><p>它的核心思想是：</p><ol><li><strong>从外部知识库检索相关信息</strong></li><li><strong>将检索到的信息作为上下文提供给模型</strong></li><li><strong>让模型基于这些上下文生成回答</strong></li></ol><p>简单来说：RAG &#x3D; 外部知识检索 + Prompt构建 + LLM 生成</p><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image1.png" alt="image"></p><h3 id="为什么需要RAG？"><a href="#为什么需要RAG？" class="headerlink" title="为什么需要RAG？"></a>为什么需要RAG？</h3><p>LLM虽然是一个强大的工具，但它本身拒绝了解任何时事，且它给出的答案总是非常流畅，内容却不一定靠谱。这存在几个主要的问题:</p><ol><li>LLM的训练数据量有限且无法更新到最新知识。</li><li>当用户需要专业或领域特定的数据时，LLM往往缺乏相应的知识</li><li>对于答案的问答内容很难从源创进行溯源</li><li>由于技术限制，不同的训练源使用相同的大语言技术，可能会产生不确信的响应</li></ol><p>而RAG为解决这些问题带来了以下优势：</p><ul><li><strong>经济高效</strong>：预训练和微调模型的成本很高，而RAG是一种经济高效的新方法</li><li><strong>信息时效</strong>：使用RAG可以为LLM提供最新的研究、统计数据或新闻</li><li><strong>增强用户信任度</strong>：RAG允许LLM通过来源归属来呈现具体的信息，输出可以包括对来源的引文或参考，这可以增加对对话的生成式人工智能解决方案的任何信心</li></ul><h3 id="RAG是如何工作的？"><a href="#RAG是如何工作的？" class="headerlink" title="RAG是如何工作的？"></a>RAG是如何工作的？</h3><p>RAG采用三种主要的检索方式：</p><ol><li><strong>一次性检索</strong>：<ul><li>从单次检索中获取相关知识</li><li>直接预置到大模型的提示词中</li><li>不会收集反馈信息</li></ul></li><li><strong>迭代检索</strong>：<ul><li>允许在对话过程中多次检索</li><li>每一轮都可能有新的检索</li><li>支持多轮对话优化</li></ul></li><li><strong>事后检索</strong>：<ul><li>先生成答案</li><li>然后检索验证</li><li>对答案进行修正</li></ul></li></ol><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image2.png" alt="image"></p><h3 id="RAG实战示例"><a href="#RAG实战示例" class="headerlink" title="RAG实战示例"></a>RAG实战示例</h3><p>以一个简单的问答场景为例，展示RAG的实际应用流程:</p><ol><li>用户提问:”公司有销售什么产品？”</li><li>系统处理流程:<ul><li>使用检索器获取产品相关文档</li><li>将文档内容与问题组合成提示词</li><li>通过LLM生成回答</li><li>确保回答基于检索到的事实信息</li></ul></li><li>最终输出:包含准确的产品信息，并且所有信息都可以溯源。</li></ol><h2 id="AI应用开发利器：向量数据库详解"><a href="#AI应用开发利器：向量数据库详解" class="headerlink" title="AI应用开发利器：向量数据库详解"></a>AI应用开发利器：向量数据库详解</h2><h3 id="什么是向量数据库？"><a href="#什么是向量数据库？" class="headerlink" title="什么是向量数据库？"></a>什么是向量数据库？</h3><p>向量数据库（Vector Database）是一种专门用于存储和处理向量数据的数据库系统。它不同于传统的关系型数据库，因为它需要将所有数据映射为特定的向量格式，并采用相似性搜索作为主要的检索方式。</p><h3 id="一个生动的例子：识别猫咪"><a href="#一个生动的例子：识别猫咪" class="headerlink" title="一个生动的例子：识别猫咪"></a>一个生动的例子：识别猫咪</h3><p>让我们通过一个识别猫咪的例子来理解向量数据库。假设我们有一组不同品种的猫咪图片：</p><ul><li>波斯猫</li><li>英国短毛猫</li><li>暹罗猫</li><li>布偶猫</li><li>无毛猫</li></ul><p>每张猫咪图片都可以用一组数字向量来表示其特征，如:</p><pre class="line-numbers language-language-json"><code class="language-language-json">波斯猫: [0.4, 0.3, 0.4, 0.5, 0.3, 0.4, 0.5, ...]英国短毛猫: [0.7, 0.2, 0.5, 0.5, 0.5, 0.5, 0.5, ...]暹罗猫: [0.5, 0.3, 0.4, 0.5, 0.3, 0.4, 0.5, ...]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>这些数字代表了猫咪的各种特征，比如:</p><ul><li>毛发长度</li><li>体型大小</li><li>面部特征</li><li>耳朵形状等等</li></ul><h3 id="向量数据库的优势"><a href="#向量数据库的优势" class="headerlink" title="向量数据库的优势"></a>向量数据库的优势</h3><p>与传统的数据库相比，向量数据库有以下特点：</p><ol><li><strong>数据类型</strong>：<ul><li>传统数据库：数值、字符串、时间等结构化数据</li><li>向量数据库：向量数据(不存储原始数据，有的也支持)</li></ul></li><li><strong>数据规模</strong>：<ul><li>传统数据库：小，1亿条数据对关系型数据库来说规模很大</li><li>向量数据库：大，最少千亿数据是基线</li></ul></li><li><strong>数据组织方式</strong>：<ul><li>传统数据库：基于表格、按照行和列组织</li><li>向量数据库：基于向量、按向量维度组织</li></ul></li><li><strong>查找方式</strong>：<ul><li>传统数据库：精确查找&#x2F;范围查找</li><li>向量数据库：近似查找，查询结果是与输入向量最相似的向量</li></ul></li></ol><h3 id="相似性搜索算法"><a href="#相似性搜索算法" class="headerlink" title="相似性搜索算法"></a>相似性搜索算法</h3><p>在向量数据库中，支持通过多种方式来计算两个向量的相似度：</p><p><strong>余弦相似度</strong>：主要是用于衡量向量在方向上的相似性，特别适用于文本、图像和高维空间中的向量。它不受向量长度的影响，只考虑方向的相似程度，计算公式如下（计算两个向量间的夹角的余弦值，取值范围为[-1, 1]）：</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">similarity(A,B) = (A·B)/(||A||·||B||)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><strong>欧式距离</strong>：主要是用于衡量向量之间的直线距离，得到的值可能很大，最小为0，通常用于低维空间或需要考虑向量各个维度之间差异的情况。欧式距离较小的向量被认为更相似，计算公式如下：</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">distance(A,B) = √∑(Ai-Bi)²<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>例如下图：左侧就是<code>欧式距离</code>，右侧就是<code>余弦相似度</code>。</p><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image3.png" alt="image"></p><h3 id="实际应用场景"><a href="#实际应用场景" class="headerlink" title="实际应用场景"></a>实际应用场景</h3><p>向量数据库的主要应用场景包括：</p><ol><li>人脸识别</li><li>图像搜索</li><li>音频识别</li><li>智能推荐系统</li></ol><p>这些场景的共同特点是：需要对非结构化数据（如图片、文本、音频）进行相似度搜索。</p><p>在RAG中，我们会将文档的知识按特定规则分成小块，转换成向量存储到向量数据库中。当人类提问时，我们将问题转换为向量，在数据库中找到最相似的文本块，这些文本块可以成为Prompt的补充内容。</p><h2 id="深入理解Embedding嵌入技术"><a href="#深入理解Embedding嵌入技术" class="headerlink" title="深入理解Embedding嵌入技术"></a>深入理解Embedding嵌入技术</h2><h3 id="Embedding-是什么？"><a href="#Embedding-是什么？" class="headerlink" title="Embedding 是什么？"></a>Embedding 是什么？</h3><p>Embedding(嵌入)是一种在机器学习中广泛使用的技术，它能将文本、图片、视频等非结构化数据映射到向量空间中。一个Embedding向量通常是一个包含N个浮点数的数组，这个向量不仅表示了数据的特征，更重要的是通过学习可以表达它们的内在语义。简而言之，Embedding就是一个模型生成方法，可以将非结构化的数据，例如文本&#x2F;图片&#x2F;视频等数据映射成有意义的向量数据。比如一段文本、一张图片、一段视频，警告Embedding模型处理后都会变成类似这样的向量：</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">[0.5, 0.8, 0.7, 0.5, 0.8, 0.7, 0.5, 0.8, 0.7, 0.5]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image4.png" alt="image"></p><h3 id="主流的Embedding模型"><a href="#主流的Embedding模型" class="headerlink" title="主流的Embedding模型"></a>主流的Embedding模型</h3><p>目前主要有这几类Embedding模型：</p><ol><li><strong>Word2Vec（词嵌入模型）</strong><ul><li>通过学习词语转化为连续的向量表示</li><li>基于两种主要算法：<code>CBOW</code> 和 <code>Skip-gram</code></li><li>能够捕捉词语之间的语义关系</li></ul></li><li><strong>1GloVe</strong><ul><li>类似Word2Vec但采用不同的训练方式</li><li>同时考虑全局共现信息</li><li>能较好地保存词语间的语义关系</li><li>适用于多种自然语言处理任务</li></ul></li><li><strong>FastText</strong><ul><li>考虑了单词的子词信息</li><li>能处理训练集中未出现的生词</li><li>支持多语言处理</li></ul></li><li><strong>大模型Embeddings</strong><ul><li>如OpenAI的text-embedding-ada-002</li><li>输入维度8191个tokens</li><li>输出维度1536维向量</li></ul></li></ol><h3 id="Embedding的神奇之处"><a href="#Embedding的神奇之处" class="headerlink" title="Embedding的神奇之处"></a>Embedding的神奇之处</h3><p>Embedding最有趣的特性是它能够捕捉语义关系。让我们看一个著名的例子</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">King - Man + Woman ≈ Queen(国王 - 男人 + 女人 ≈ 女王)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>这个公式展示了Embedding不仅仅是把词语转换成数字，它还能：</p><ol><li>保留词语之间的关系</li><li>支持向量运算</li><li>产生有意义的结果</li></ol><p>我们可以通过可视化的方式看到这些词语在向量空间中的分布：</p><ul><li>woman和girl的向量位置接近</li><li>man和boy的向量位置接近</li><li>king和queen虽然性别不同，但都位于表示”统治者”的维度上</li></ul><h3 id="Embedding的重要价值"><a href="#Embedding的重要价值" class="headerlink" title="Embedding的重要价值"></a>Embedding的重要价值</h3><ol><li><strong>降维</strong>：将高维数据映射到低维空间，大大降低了计算复杂度</li><li><strong>捕捉语义信息</strong>：不仅能记录表面的词频信息，还能捕捉深层的语义关联</li><li><strong>泛化性</strong>：Embedding学习到的是通用的语言表达方式，可以应用到新的场景</li><li><strong>泛化能力</strong>：对于未见过的数据，也能基于已学习的语义特征给出合理的向量表示</li><li><strong>可视化支持</strong>：虽然Embedding本身很复杂，但我们可以使用t-SNE等工具将其可视化，帮助理解数据的内在结构。</li></ol><h3 id="在RAG中的应用"><a href="#在RAG中的应用" class="headerlink" title="在RAG中的应用"></a>在RAG中的应用</h3><p>在RAG系统中，Embedding主要用于两个场景：</p><ol><li><strong>文档向量化</strong>：将知识库中的文档转换为向量</li><li><strong>查询向量化</strong>：将用户的问题转换为向量</li></ol><p>通过比较这些向量的相似度，我们可以找到与用户问题最相关的文档片段，从而提供更准确的答案。</p><h2 id="RAG应用实战-OpenAI-Embedding与LangChain的结合"><a href="#RAG应用实战-OpenAI-Embedding与LangChain的结合" class="headerlink" title="RAG应用实战:OpenAI Embedding与LangChain的结合"></a>RAG应用实战:OpenAI Embedding与LangChain的结合</h2><h3 id="OpenAI-Embedding接口简介"><a href="#OpenAI-Embedding接口简介" class="headerlink" title="OpenAI Embedding接口简介"></a>OpenAI Embedding接口简介</h3><p>OpenAI提供了多个Embedding模型选择，以下是几个主要模型的对比:</p><table><thead><tr><th>模型</th><th>Token数(每个文档800个)</th><th>性能评估</th><th>最大输入</th><th>向量维度</th></tr></thead><tbody><tr><td>text-embedding-3-small</td><td>62,500</td><td>62.3%</td><td>8191</td><td>1536</td></tr><tr><td>text-embedding-3-large</td><td>9,615</td><td>64.6%</td><td>8191</td><td>3072</td></tr><tr><td>text-embedding-ada-002</td><td>12,500</td><td>61.0%</td><td>8191</td><td>1536</td></tr></tbody></table><h3 id="LangChain中的Embedding组件使用"><a href="#LangChain中的Embedding组件使用" class="headerlink" title="LangChain中的Embedding组件使用"></a>LangChain中的Embedding组件使用</h3><p>在LangChain中，Embedding类提供了统一的接口来使用各种嵌入模型:</p><pre class="line-numbers language-language-bash"><code class="language-language-bash">class Embeddings(ABC):    """Interface for embedding models."""        @abstractmethod    def embed_documents(self, texts: List[str]) -> List[List[float]]:        """Embed search docs."""            @abstractmethod    def embed_query(self, text: str) -> List[float]:        """Embed query text."""<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用示例:</p><pre class="line-numbers language-language-python"><code class="language-language-python">import dotenvimport numpy as npfrom langchain_openai import OpenAIEmbeddingsfrom numpy.linalg import norm# 初始化Embedding模型embeddings = OpenAIEmbeddings()# 进行文本嵌入query_vector = embeddings.embed_query("你好, 我是小潘")documents_vector = embeddings.embed_documents([    "你好, 我是小潘",    "这个自然语言处理的人叫小潘",    "来知若惘, 既心若旷"])# 计算相似度def cosine_similarity(vector1, vector2):    dot_product = np.dot(vector1, vector2)    norm1 = norm(vector1)    norm2 = norm(vector2)    return dot_product / (norm1 * norm2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="CacheBackEmbedding的使用"><a href="#CacheBackEmbedding的使用" class="headerlink" title="CacheBackEmbedding的使用"></a>CacheBackEmbedding的使用</h3><p>为了提高性能，LangChain提供了缓存功能：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain.embeddings import CacheBackedEmbeddingsfrom langchain.storage import LocalFileStoreembeddings = OpenAIEmbeddings(model="text-embedding-3-small")embeddings_with_cache = CacheBackedEmbeddings.from_bytes_store(    embeddings,    LocalFileStore("./cache/"),    namespace=embeddings.model,    query_embedding_cache=True)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用缓存时需要注意：</p><ol><li>underlying_embedder: 使用的基础嵌入模型</li><li>document_embedding_cache: 用于缓存文档的存储结构</li><li>batch_size: 可选参数，默认None</li><li>namespace: 用于文档缓存的命名空间</li><li>query_embedding_cache: 是否缓存查询向量</li></ol><h3 id="运行流程分析"><a href="#运行流程分析" class="headerlink" title="运行流程分析"></a>运行流程分析</h3><p>一个完整的RAG应用运行流程如下：</p><ol><li><strong>文档预处理</strong><ul><li>分割文档</li><li>生成向量</li><li>存入向量数据库</li></ul></li><li><strong>查询处理</strong><ul><li>将用户问题转为向量</li><li>在向量数据库中检索</li><li>组合上下文生成回答</li></ul></li><li><strong>缓存优化</strong><ul><li>缓存常见文档的向量</li><li>缓存常见查询的向量</li><li>提供响应速度</li></ul></li></ol><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li>向量维度的选择<ul><li>需要平衡精度和效率</li><li>维度越高，表达能力越强，但计算成本也越高</li></ul></li><li>缓存策略<ul><li>合理设置缓存大小</li><li>选择适当的缓存淘汰策略</li><li>定期更新缓存</li></ul></li><li>性能优化<ul><li>使用批处理提高效率</li><li>合理使用多线程</li><li>监控资源使用情况</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LangChain RAG 应用开发优化策略详解</title>
      <link href="/2025/02/12/langchain-rag-ying-yong-kai-fa-you-hua-ce-lue-xiang-jie/"/>
      <url>/2025/02/12/langchain-rag-ying-yong-kai-fa-you-hua-ce-lue-xiang-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="引言：理解RAG及其重要性"><a href="#引言：理解RAG及其重要性" class="headerlink" title="引言：理解RAG及其重要性"></a>引言：理解RAG及其重要性</h2><p>在大语言模型（LLM）应用开发中，检索增强生成（Retrival-Augmented Generation, RAG）已经成为提升模型输出质量的关键技术。本文将深入探讨在LangChain框架中如何优化RAG应用，帮助开发者构建更智能、更准确的AI应用。</p><h2 id="RAG的基本概念"><a href="#RAG的基本概念" class="headerlink" title="RAG的基本概念"></a>RAG的基本概念</h2><blockquote><p>📌 什么是RAG?<br>RAG是一种将外部知识检索与语言模型生成相结合的技术架构。它通过检索相关信息来增强LLM的知识储备，从而产生更准确、更可靠的输出。</p></blockquote><h3 id="为什么需要优化RAG？"><a href="#为什么需要优化RAG？" class="headerlink" title="为什么需要优化RAG？"></a>为什么需要优化RAG？</h3><p>在实际应用中，基础的RAG实现往往会遇到以下挑战：</p><ol><li>检索准确性不足</li><li>复杂问题处理能力有限</li><li>知识关联不够紧密</li><li>响应质量不够稳定</li></ol><p>这些问题促使我们需要采用多种优化策略来提升RAG的性能。</p><h3 id="第一部分：多查询检索优化策略"><a href="#第一部分：多查询检索优化策略" class="headerlink" title="第一部分：多查询检索优化策略"></a>第一部分：多查询检索优化策略</h3><h4 id="理解多查询检索的必要性"><a href="#理解多查询检索的必要性" class="headerlink" title="理解多查询检索的必要性"></a>理解多查询检索的必要性</h4><p>在RAG应用中，单一查询往往无法完整捕捉用户问题的所有方面。例如，当用户问”Python如何实现多线程并发控制？“时，我们可能需要同时检索：</p><ul><li>Python线程基础知识</li><li>并发控制机制</li><li>线程安全实现方法</li></ul><h4 id="多查询检索的工作原理"><a href="#多查询检索的工作原理" class="headerlink" title="多查询检索的工作原理"></a>多查询检索的工作原理</h4><blockquote><p>🔍 核心思路：利用LLM的理解能力，将一个复杂查询拆分或重写为多个相关查询，然后通过融合算法整合检索结果。</p></blockquote><p><strong>工作流程</strong>：</p><ol><li><strong>查询重写</strong>：LLM将原始查询转换为多个相关查询</li><li><strong>并行检索</strong>：对每个查询进行独立检索</li><li><strong>结果融合</strong>：使用RRF（Reciprocal Rank Fusion）算法融合检索结果</li><li><strong>内容生成</strong>：将融合后的结果输入LLM生成最终答案</li></ol><h3 id="代码实现示例"><a href="#代码实现示例" class="headerlink" title="代码实现示例"></a>代码实现示例</h3><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain.retrievers import MultiQueryRetrieverfrom langchain.chains import LLMChainfrom langchain.prompts import PromptTemplate# 1. 创建多查询检索器retriever = MultiQueryRetriever(    retriever=base_retriever,    llm=ChatOpenAI(model="gpt-3.5-turbo-16k", temperature=0),    prompt_template="""基于用户的问题，生成3个不同的相关查询：    原始问题: &#123;question&#125;    生成的查询应该探索问题的不同方面。    """)# 2. 使用RRF算法融合结果def rrf_fusion(results, k=60):    fused_scores = &#123;&#125;    for rank, doc in enumerate(results):        doc_str = doc.page_content        if doc_str not in fused_scores:            fused_scores[doc_str] = 1.0 / (k + rank + 1)        else:            fused_scores[doc_str] += 1.0 / (k + rank + 1)        # 排序并返回结果    sorted_results = sorted(fused_scores.items(),                           key=lambda x: x[1],                           reverse=True)    return sorted_results<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>RRF 算法原理如下</p><pre class="line-numbers language-language-python"><code class="language-language-python">"""RRF (Reciprocal Rank Fusion) 算法的核心公式：RRFscore(d ∈ D) = ∑ 1/(k + r(d))其中：- d 是文档- D 是所有文档集合- k 是一个常数(通常取60)- r(d)是文档d在排序中的位置这个公式的特点：1. 对排名靠前的文档给予更高的权重2. k参数可以调节排名的影响程度3. 适合融合不同来源的排序结果"""<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="优化效果分析"><a href="#优化效果分析" class="headerlink" title="优化效果分析"></a>优化效果分析</h4><p>多查询检索策略带来的主要优势：</p><ol><li><strong>提升召回率</strong><ul><li>通过多角度查询提高相关文档的覆盖率</li><li>减少因单一查询表达不当导致的漏检</li></ul></li><li><strong>提高准确性</strong><ul><li>RRF融合算法可以突出高质量的共同结果</li><li>降低单个查询的噪声影响</li></ul></li><li><strong>增强鲁棒性</strong><ul><li>对查询表达的变化更不敏感</li><li>能更好地处理复杂或模糊的问题</li></ul></li></ol><h4 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a>实践建议</h4><p>在实际应用中，需要注意以下几点：</p><ul><li><strong>查询数量选择</strong>：通常生成3-5个查询即可，过多查询可能引入噪声</li><li><strong>相似度阈值设置</strong>：建议在RRF融合时设置合适的相似度阈值，过滤低相关性结果</li><li><strong>资源消耗考虑</strong>：多查询会增加API调用和计算资源，需要在效果和成本间权衡</li></ul><blockquote><p>💡 实践小贴士：可以通过监控检索结果的diversity和relevance指标，来调整多查询策略的参数。</p></blockquote><h3 id="第二部分：问题分解策略优化"><a href="#第二部分：问题分解策略优化" class="headerlink" title="第二部分：问题分解策略优化"></a>第二部分：问题分解策略优化</h3><h4 id="复杂问题的分解处理"><a href="#复杂问题的分解处理" class="headerlink" title="复杂问题的分解处理"></a>复杂问题的分解处理</h4><p>在实际应用中，我们经常遇到复杂的多层次问题。例如：”请分析特斯拉近五年的财务状况，并评估其在电动汽车市场的竞争优势。”这类问题需要：</p><ul><li>处理大量相关信息</li><li>分析多个维度</li><li>综合多方面结论</li></ul><p><strong>并行分解模式</strong></p><blockquote><p>🔄 并行模式：将问题同时分解为多个独立子问题，分别获取答案后合并。</p></blockquote><pre class="line-numbers language-language-python"><code class="language-language-python"># 并行分解示例decomposition_chain = &#123;    "question": RunnablePassthrough(),    | decomposition_prompt    # 分解问题    | ChatOpenAI(temperature=0)    | StrOutputParser()&#125;# 并行处理子问题sub_questions = decomposition_chain.invoke(question)answers = await asyncio.gather(*[    process_subquestion(q) for q in sub_questions])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>串行分解模式</strong></p><blockquote><p>⛓️ 串行模式：按照逻辑顺序依次处理子问题，后面的问题依赖前面的答案。</p></blockquote><pre class="line-numbers language-language-python"><code class="language-language-python"># 串行分解示例class StepBackRetriever(BaseRetriever):    def _get_relevant_documents(        self, query: str, *, run_manager: CallbackManagerForRetrieverRun    ) -> List[Document]:        # 1. 生成中间查询        intermediate_query = self.llm.predict(            f"为了回答'&#123;query&#125;'，我们需要先了解什么？"        )                # 2. 检索中间知识        intermediate_docs = self.retriever.get_relevant_documents(            intermediate_query        )                # 3. 基于中间知识检索最终答案        final_docs = self.retriever.get_relevant_documents(query)                return intermediate_docs + final_docs<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Step-Back-策略实现"><a href="#Step-Back-策略实现" class="headerlink" title="Step-Back 策略实现"></a>Step-Back 策略实现</h4><p>Step-Back策略是一种特殊的串行分解方法，它通过“后退一步”来获取更基础的知识背景。</p><pre class="line-numbers language-language-python"><code class="language-language-python">"""示例：用户问题"量子计算机如何影响现代密码学？"Step-Back分解：1. 基础知识查询：   - 什么是量子计算机的基本原理？   - 现代密码学的核心技术有哪些？2. 关联分析：   - 量子计算对RSA等算法的影响   - 后量子密码学的发展3. 最终综合：   基于以上知识形成完整答案"""<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>工作流程</strong>：</p><ol><li>分析原始问题</li><li>生成更基础的前置问题</li><li>获取基础知识</li><li>结合基础知识回答原问题</li></ol><h4 id="Step-Back-代码实现"><a href="#Step-Back-代码实现" class="headerlink" title="Step-Back 代码实现"></a>Step-Back 代码实现</h4><pre class="line-numbers language-language-python"><code class="language-language-python"># system_prompt = """你是一位专业的助手，需要：1. 理解用户的具体问题2. 思考需要哪些基础知识3. 生成相关的基础问题4. 基于基础知识回答原问题"""few_shot_prompt = FewShotChatMessagePromptTemplate(    example_prompt=example_prompt,    examples=examples,    suffix="现在，请帮我回答：&#123;question&#125;")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="优化效果对比"><a href="#优化效果对比" class="headerlink" title="优化效果对比"></a>优化效果对比</h4><table><thead><tr><th>分解策略</th><th>适用场景</th><th>优势</th><th>劣势</th></tr></thead><tbody><tr><td>并行分解</td><td>独立子问题</td><td>处理速度快，资源利用高</td><td>结果整合可能不够连贯</td></tr><tr><td>串行分解</td><td>逻辑依赖性强</td><td>答案更连贯，逻辑性强</td><td>处理时间较长</td></tr><tr><td>Step-Back</td><td>需要深入理解</td><td>回答更全面，准确度高</td><td>资源消耗较大</td></tr></tbody></table><h4 id="实践建议-1"><a href="#实践建议-1" class="headerlink" title="实践建议"></a>实践建议</h4><ol><li>选择策略时考虑因素: <ul><li>问题的复杂度</li><li>子问题间的依赖关系</li><li>响应时间要求</li><li>资源限制</li></ul></li><li>优化建议：<ul><li>对于并行模式，注意结果融合的质量</li><li>串行模式要控制分解的层级深度</li><li>Step-Back策略要平衡基础知识的范围</li></ul></li></ol><blockquote><p>🌟 最佳实践：可以根据问题类型动态选择分解策略，甚至组合使用多种策略。</p></blockquote><h3 id="第三部分：混合检索策略实现"><a href="#第三部分：混合检索策略实现" class="headerlink" title="第三部分：混合检索策略实现"></a>第三部分：混合检索策略实现</h3><h4 id="理解混合检索的价值"><a href="#理解混合检索的价值" class="headerlink" title="理解混合检索的价值"></a>理解混合检索的价值</h4><p>在实际应用中，单一的检索方法往往难以应对所有场景。例如：</p><ul><li>语义检索擅长理解上下文，但可能错过关键词</li><li>关键词检索准确度高，但缺乏语义理解</li><li>密集检索和稀疏检索各有优势</li></ul><p>因此，将多种检索方法结合起来，可以取长补短，提升整体检索效果。</p><h4 id="混合检索器的架构设计"><a href="#混合检索器的架构设计" class="headerlink" title="混合检索器的架构设计"></a>混合检索器的架构设计</h4><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain.retrievers import EnsembleRetrieverfrom langchain_community.retrievers import BM25Retrieverfrom langchain_community.vectorstores import FAISS# 1. 创建不同类型的检索器# BM25检索器（基于关键词）bm25_retriever = BM25Retriever.from_documents(    documents, k=4)# FAISS检索器（基于向量）faiss_retriever = FAISS.from_documents(    documents,    embedding=OpenAIEmbeddings(model="text-embedding-3-small")).as_retriever(search_kwargs=&#123;"k": 4&#125;)# 2. 创建集成检索器ensemble_retriever = EnsembleRetriever(    retrievers=[bm25_retriever, faiss_retriever],    weights=[0.5, 0.5])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="主要检索方法的特点"><a href="#主要检索方法的特点" class="headerlink" title="主要检索方法的特点"></a>主要检索方法的特点</h4><p>下面是几种常用检索方法的对比：</p><table><thead><tr><th>检索方法</th><th>优势</th><th>适用场景</th><th>注意事项</th></tr></thead><tbody><tr><td>BM25</td><td>精确匹配，速度快</td><td>关键词搜索</td><td>不理解语义变化</td></tr><tr><td>向量检索</td><td>理解语义相似</td><td>概念搜索</td><td>计算资源消耗大</td></tr><tr><td>混合检索</td><td>综合优势</td><td>复杂查询</td><td>需要调整权重</td></tr></tbody></table><h4 id="实现细节和优化"><a href="#实现细节和优化" class="headerlink" title="实现细节和优化"></a>实现细节和优化</h4><p><strong>检索器配置</strong></p><pre class="line-numbers language-language-python"><code class="language-language-python"># 配置检索参数faiss_retriever = faiss_db.as_retriever(    search_kwargs=&#123;"k": 4&#125;).configurable_fields(    search_kwargs=ConfigurableField(        id="search_kwargs_faiss",        name="检索参数",        description="设置检索的参数"    ))# 设置运行时配置config = &#123;"configurable": &#123;"search_kwargs_faiss": &#123;"k": 4&#125;&#125;&#125;docs = ensemble_retriever.invoke("查询", config=config)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>权重调整策略</strong></p><ol><li><strong>初始设置</strong>：开始时可以给各检索器相同权重</li><li><strong>动态调整</strong>：根据查询类型动态调整权重</li><li><strong>性能监控</strong>：跟踪各检索器的表现，定期优化权重</li><li><strong>场景适配</strong>：针对不同领域调整最优权重组合</li></ol><h4 id="应用效果优化"><a href="#应用效果优化" class="headerlink" title="应用效果优化"></a>应用效果优化</h4><p>为了获得最佳检索效果，建议：</p><ol><li>检索器选择<ul><li>根据数据特点选择合适的检索器组合</li><li>考虑计算资源和响应时间的平衡</li><li>评估检索器的互补性</li></ul></li><li>参数优化<ul><li>使用验证集调整检索参数</li><li>监控检索质量指标</li><li>定期更新检索模型</li></ul></li><li>结果融合<ul><li>采用多样化的融合策略</li><li>考虑结果的去重和排序</li><li>平衡相关性和多样性</li></ul></li></ol><h4 id="性能监控与改进"><a href="#性能监控与改进" class="headerlink" title="性能监控与改进"></a>性能监控与改进</h4><pre class="line-numbers language-language-python"><code class="language-language-python"># 性能监控示例def evaluate_retrieval(retriever, test_queries, ground_truth):    metrics = &#123;        'precision': [],        'recall': [],        'latency': []    &#125;        for query, truth in zip(test_queries, ground_truth):        start_time = time.time()        results = retriever.get_relevant_documents(query)        latency = time.time() - start_time                # 计算评估指标        metrics['latency'].append(latency)        # ... 计算precision和recall            return metrics<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="总结：RAG优化策略的实践指南"><a href="#总结：RAG优化策略的实践指南" class="headerlink" title="总结：RAG优化策略的实践指南"></a>总结：RAG优化策略的实践指南</h3><h4 id="优化策略的综合比较"><a href="#优化策略的综合比较" class="headerlink" title="优化策略的综合比较"></a>优化策略的综合比较</h4><p>以下是我们讨论过的主要优化策略的特点对比：</p><table><thead><tr><th>优化策略</th><th>主要优势</th><th>实现复杂度</th><th>资源消耗</th><th>适用场景</th></tr></thead><tbody><tr><td>多查询检索</td><td>提高召回率</td><td>中等</td><td>中等</td><td>复杂查询、模糊问题</td></tr><tr><td>问题分解</td><td>提升理解深度</td><td>较高</td><td>较高</td><td>多维度分析问题</td></tr><tr><td>Step-Back</td><td>增强理解准确性</td><td>高</td><td>高</td><td>需要深入理解的问题</td></tr><tr><td>混合检索</td><td>综合性能提升</td><td>中等</td><td>较高</td><td>通用场景</td></tr></tbody></table><h4 id="优化路径建议"><a href="#优化路径建议" class="headerlink" title="优化路径建议"></a>优化路径建议</h4><ol><li><strong>基础阶段</strong><ul><li>实现基本的RAG流程</li><li>优化向量检索参数</li><li>改进提示词设计</li></ul></li><li><strong>进阶阶段</strong><ul><li>引入多查询策略</li><li>实现基本的问题分解</li><li>尝试混合检索方法</li></ul></li><li><strong>高级阶段</strong><ul><li>实现完整的Step-Back策略</li><li>优化多检索器集成</li><li>构建自适应检索系统</li></ul></li></ol><p><strong>场景选择指南</strong></p><p>根据不同的应用场景，推荐以下优化组合：</p><ol><li><strong>知识问答系统</strong><ul><li>多查询检索 + 混合检索</li><li>重点优化检索准确性</li></ul></li><li><strong>文档分析系统</strong><ul><li>问题分解 + Step-Back</li><li>强化深度理解能力</li></ul></li><li><strong>通用对话系统</strong><ul><li>混合检索 + 多查询</li><li>平衡效率和准确性</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Langchain </tag>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LangChain初入门</title>
      <link href="/2025/02/09/langchain-chu-ru-men/"/>
      <url>/2025/02/09/langchain-chu-ru-men/</url>
      
        <content type="html"><![CDATA[<h2 id="为什么选择LangChain"><a href="#为什么选择LangChain" class="headerlink" title="为什么选择LangChain"></a>为什么选择LangChain</h2><p>LangChain作为一个强大的框架，具有以下优势：</p><ul><li><p><strong>组件化和标准化</strong>：提供了标准化的接口来处理各种LLM，使开发更加灵活和可维护。</p></li><li><p><strong>丰富的工具集成</strong>：内置了大量工具和集成，可以轻松连接数据库、搜索引擎等外部服务。</p></li><li><p><strong>链式处理能力</strong>：可以将多个组件组合成链，实现复杂的处理流程。</p></li><li><p><strong>内存管理</strong>：提供了多种记忆组件，使应用能够保持上下文连贯性。</p></li></ul><h2 id="LangChain简介"><a href="#LangChain简介" class="headerlink" title="LangChain简介"></a>LangChain简介</h2><p>LangChain是一个用于开发由语言模型驱动的应用程序的框架。</p><h3 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h3><ul><li><p><strong>Models (模型)</strong>：提供与大语言模型的统一交互接口，支持各类LLM、聊天模型和文本嵌入模型的调用</p></li><li><p><strong>Prompts (提示)</strong>：专门用于管理和优化提示模板，提供标准化的提示工程工具</p></li><li><p><strong>Indexes (索引)</strong>：提供高效的文档加载、分割和向量存储系统，支持大规模文本处理和检索</p></li><li><p><strong>Memory (记忆)</strong>：用于在交互过程中管理和存储状态信息，确保对话的连贯性和上下文理解</p></li><li><p><strong>Chains (链)</strong>：能将多个组件组合成端到端应用的核心机制，实现复杂的处理流程</p></li><li><p><strong>Agents (代理)</strong>：赋予LLM使用工具的能力，支持自主推理和行动决策</p></li></ul><p><img src="/medias/featureimages/blog/langchain-chu-ru-men/image1.png" alt="image"></p><h4 id="Prompts组件"><a href="#Prompts组件" class="headerlink" title="Prompts组件"></a>Prompts组件</h4><p><strong>概念与作用</strong></p><p>在LLM应用开发中,我们通常不会直接将用户输入传递给大模型,而是会将用户输入添加到一个更大的文本片段中,这个文本片段被称为Prompt。Prompt为大模型提供了任务相关的上下文和指令,帮助模型更好地理解和执行任务。</p><p>LangChain中的Prompts组件提供了一系列工具来管理和优化这些提示模板。主要包含两大类:</p><ul><li><p>PromptTemplate: 将Prompt按照template进行格式化,处理变量和组合</p></li><li><p>Selectors: 根据不同条件选择不同的提示词</p></li></ul><p><strong>基本构成</strong></p><p>在LangChain中,Prompts组件包含多个子组件:</p><p>角色提示模板:</p><ul><li><p>SystemMessagePromptTemplate: 系统角色消息模板</p></li><li><p>HumanMessagePromptTemplate: 人类角色消息模板</p></li><li><p>AIMessagePromptTemplate: AI角色消息模板</p></li></ul><p>提示模板类型:</p><ul><li><p>PromptTemplate: 文本提示模板</p></li><li><p>ChatPromptTemplate: 聊天消息提示模板</p></li><li><p>MessagePlaceholder: 消息占位符</p></li></ul><p><strong>关键操作</strong></p><p>格式化LangChain支持两种格式化方式</p><pre class="line-numbers language-language-python"><code class="language-language-python"># f-string方式prompt = PromptTemplate.from_template("请将一个关于&#123;subject&#125;的笑话")# jinja2方式prompt = PromptTemplate.from_template(    "请将一个关于&#123;&#123;subject&#125;&#125;的笑话",    template_format="jinja2")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>提示模板拼接</p><pre class="line-numbers language-language-python"><code class="language-language-python"># 字符串提示拼接prompt = (    PromptTemplate.from_template("请将一个关于&#123;subject&#125;的冷笑话")    + "，让我开心下"    + "\n使用&#123;language&#125;语言。")# 聊天提示拼接system_prompt = ChatPromptTemplate.from_messages([    ("system", "你是OpenAI开发的聊天机器人，请根据用户的提问进行回复，我叫&#123;username&#125;")])human_prompt = ChatPromptTemplate.from_messages([    ("human", "&#123;query&#125;")])prompt = system_prompt + human_prompt<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>模板复用</strong></p><p>对于复杂的提示模板,LangChain提供了PipelinePromptTemplate来实现模板的复用:</p><pre class="line-numbers language-language-python"><code class="language-language-python"># 描述提示模板instruction_template = "你正在模拟&#123;person&#125;。"instruction_prompt = PromptTemplate.from_template(instruction_template)# 示例提示模板example_template = """下面是一个交互例子:Q: &#123;example_q&#125;A: &#123;example_a&#125;"""example_prompt = PromptTemplate.from_template(example_template)# 开始提示模板start_template = """现在开始对话:Q: &#123;input&#125;A:"""start_prompt = PromptTemplate.from_template(start_template)# 组合模板pipeline_prompt = PipelinePromptTemplate(    final_prompt=full_prompt,    pipeline_prompts=[        ("instruction", instruction_prompt),        ("example", example_prompt),        ("start", start_prompt),    ])<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>最佳实践</strong></p><p>选择合适的格式化方式</p><ol><li><p>简单变量替换使用f-string</p></li><li><p>需要条件判断等复杂逻辑时使用jinja2</p></li></ol><p>提示模板设计</p><ol><li><p>保持模板的清晰和可维护性</p></li><li><p>合理使用系统消息和示例</p></li><li><p>避免过于复杂的嵌套结构</p></li></ol><p>错误处理</p><ol><li><p>验证必要的变量是否存在</p></li><li><p>处理格式化可能出现的异常</p></li></ol><p>性能优化</p><ol><li><p>重复使用的模板要缓存</p></li><li><p>避免不必要的模板拼接操作</p></li></ol><h4 id="Model组件"><a href="#Model组件" class="headerlink" title="Model组件"></a>Model组件</h4><p><strong>基本概念</strong></p><p>Models是LangChain的核心组件，提供了一个标准接口来封装不同类型的LLM进行交互，LangChain本身不提供LLM,而是提供了接口来集成各种模型。</p><p>LangChain支持两种类型的模型:</p><ul><li><p>LLM: 使用纯文本作为输入和输出的大语言模型</p></li><li><p>Chat Model: 使用聊天消息列表作为输入并返回聊天消息的聊天模型</p></li></ul><p><strong>组件架构</strong></p><p>LangChain中Models组件的基类结构如下:</p><p>BaseLanguageModel(基类)</p><ul><li><p>BaseLLM(大语言模型基类)</p><ul><li>SimpleLLM(简化大语言模型)</li><li>第三方LLM集成(OpenAI、百度文心等)</li></ul></li><li><p>BaseChatModel(聊天模型基类)</p><ul><li>SimpleChatModel(简化聊天模型)</li><li>第三方Chat Model集成</li></ul></li></ul><h4 id="Message组件"><a href="#Message组件" class="headerlink" title="Message组件"></a>Message组件</h4><ul><li><p>SystemMessage: 系统消息</p></li><li><p>HumanMessage: 人类消息</p></li><li><p>AIMessage: AI消息</p></li><li><p>FunctionMessage: 函数调用消息</p></li><li><p>ToolMessage: 工具调用消息</p></li></ul><p><strong>核心办法</strong></p><p>Models组件提供了几个关键方法:</p><p>invoke&#x2F;invoke_sync: 调用模型生成内容</p><pre class="line-numbers language-language-python"><code class="language-language-python"># 基本调用llm = ChatOpenAI(model="gpt-3.5-turbo-16k")response = llm.invoke("你好!")# 异步调用async def generate():    response = await llm.ainvoke("你好!")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>batch&#x2F;abatch: 批量调用处理多个输入</p><pre class="line-numbers language-language-python"><code class="language-language-python">messages = [    "请讲一个关于程序员的笑话",    "请讲一个关于Python的笑话"]responses = llm.batch(messages)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>stream&#x2F;astream: 流式返回生成内容</p><pre class="line-numbers language-language-python"><code class="language-language-python">response = llm.stream("请介绍下LLM和LLMOps")for chunk in response:    print(chunk.content, end="")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>Message组件使用</strong></p><p>消息组件用于构建与聊天模型的交互:</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.messages import SystemMessage, HumanMessage, AIMessage# 创建消息system_msg = SystemMessage(content="你是一个AI助手")human_msg = HumanMessage(content="你好!")ai_msg = AIMessage(content="你好!我是AI助手")# 构建消息列表messages = [system_msg, human_msg, ai_msg]# 使用消息与模型交互response = chat_model.invoke(messages)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>实践示例</strong></p><p>基本对话示例：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_openai import ChatOpenAIfrom langchain_core.prompts import ChatPromptTemplate# 创建聊天模型chat = ChatOpenAI()# 创建提示模板prompt = ChatPromptTemplate.from_messages([    ("system", "你是一位&#123;role&#125;"),    ("human", "&#123;query&#125;")])# 调用模型response = chat.invoke(    prompt.format_messages(        role="Python专家",        query="什么是装饰器?"    ))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>流式输出示例：</p><pre class="line-numbers language-language-python"><code class="language-language-python"># 创建提示模板prompt = ChatPromptTemplate.from_template("&#123;subject&#125;的发展历史是什么?")# 创建模型llm = ChatOpenAI()# 流式生成response = llm.stream(    prompt.format_messages(subject="人工智能"))# 处理输出for chunk in response:    print(chunk.content, end="")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>最佳实践</strong></p><p>选择合适的模型类型</p><ol><li><p>简单文本生成任务使用LLM</p></li><li><p>对话类任务使用Chat Model</p></li></ol><p>正确处理异步操作</p><ol><li><p>在异步环境中使用ainvoke&#x2F;astream</p></li><li><p>批量处理时考虑使用batch</p></li></ol><p>异常处理</p><ol><li><p>处理模型调用可能的超时</p></li><li><p>捕获API错误并适当处理</p></li></ol><p>性能优化</p><ol><li><p>合理使用批处理</p></li><li><p>适时使用流式输出</p></li></ol><p><strong>OutputParser 解析器组件</strong></p><p>为什么需要输出解析器</p><p>在使用大模型时,我们经常会遇到输出解析的问题。比如:</p><pre class="line-numbers language-language-python"><code class="language-language-python">llm = ChatOpenAI()# 示例1: 返回的是自然语言llm.invoke("1+1等于几?")  # 输出: 1 + 1 等于 2。# 示例2: 包含多余信息llm.invoke("告诉我3个动物的名字。")  # 输出: 好的，这里有三种动物的名字：\n1. 狮子\n2. 大熊猫\n3. 斑马# 示例3: 格式不统一llm.invoke("给我一个json数据,键为a和b")  # 输出: &#123;\n "a": 10,\n "b": 20\n&#125;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>OutputParser就是为了解决这些问题而设计的。它通过:</p><ol><li><p>预设提示 - 告诉LLM需要的输出格式</p></li><li><p>解析功能 - 将输出转换成指定格式</p></li></ol><p><strong>Parser类型详解</strong></p><p>Langchain 提供了多种Parser：</p><ol><li><p>基础Parser：</p><ul><li>StrOutputParser: 最简单的Parser,原样返回文本</li><li>BaseOutputParser: 所有Parser的基类</li><li>BaseLLMOutputParser: 专门用于LLM输出的基类</li></ul></li><li><p>格式化Parser：</p><ul><li>JsonOutputParser: 解析JSON格式输出</li><li>XMLOutputParser: 解析XML格式输出</li><li>PydanticOutputParser: 使用Pydantic模型解析输出</li></ul></li><li><p>列表类Parser：</p><ul><li>CommaSeparatedListOutputParser: 解析逗号分隔的列表</li><li>NumberedListOutputParser: 解析数字编号的列表</li></ul></li></ol><p><strong>实践示例</strong></p><ol><li>StrOutputParser使用：</li></ol><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.output_parsers import StrOutputParserfrom langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAI# 创建链chain = (    ChatPromptTemplate.from_template("&#123;query&#125;")    | ChatOpenAI()    | StrOutputParser())# 调用response = chain.invoke(&#123;"query": "你好!"&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>JsonOutputParser使用：</li></ol><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.output_parsers import JsonOutputParserfrom langchain_core.pydantic_v1 import BaseModel, Field# 定义输出结构class Joke(BaseModel):    joke: str = Field(description="回答用户的冷笑话")    punchline: str = Field(description="冷笑话的笑点")# 创建Parserparser = JsonOutputParser(pydantic_object=Joke)# 创建提示模板prompt = ChatPromptTemplate.from_template(    "回答用户的问题。\n&#123;format_instructions&#125;\n&#123;query&#125;\n")# 添加格式说明prompt = prompt.partial(format_instructions=parser.get_format_instructions())# 创建链chain = prompt | ChatOpenAI() | parser# 使用response = chain.invoke(&#123;"query": "请讲一个关于程序员的冷笑话"&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>错误处理</strong></p><ol><li>解析失败的处理：</li></ol><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.output_parsers import OutputParserExceptiontry:    result = parser.parse(llm_output)except OutputParserException as e:    # 处理解析错误    print(f"解析错误: &#123;e&#125;")    # 可以选择重试或使用默认值<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>使用重试机制：</li></ol><pre class="line-numbers language-language-python"><code class="language-language-python"># 可以配置回调来处理重试from langchain_core.callbacks import BaseCallbackHandlerclass RetryHandler(BaseCallbackHandler):    def on_retry(self, retry_state):        print(f"重试次数: &#123;retry_state.attempt_number&#125;")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>最佳实践</strong></p><ol><li><p>选择合适的Parser</p><ul><li>简单文本使用StrOutputParser</li><li>结构化数据使用JsonOutputParser或PydanticOutputParser</li><li>列表数据使用专门的列表Parser</li></ul></li><li><p>提示设计</p><ul><li>在提示中明确指定输出格式</li><li>使用Parser提供的format_instructions</li></ul></li><li><p>异常处理</p><ul><li>总是处理可能的解析错误</li><li>考虑添加重试机制</li><li>提供合理的默认值</li></ul></li><li><p>性能优化</p><ul><li>避免过于复杂的解析逻辑</li><li>合理使用缓存</li></ul></li></ol><h4 id="LCEL表达式与Runnable协议"><a href="#LCEL表达式与Runnable协议" class="headerlink" title="LCEL表达式与Runnable协议"></a>LCEL表达式与Runnable协议</h4><p><strong>为什么需要LCEL</strong></p><p>传统的链式调用方式存在嵌套问题：</p><pre class="line-numbers language-language-python"><code class="language-language-python">content = parser.invoke(    llm.invoke(        prompt.invoke(            &#123;"query": req.query.data&#125;        )    ))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>LCEL 提供了更优雅的方式：</p><pre class="line-numbers language-language-python"><code class="language-language-python">chain = prompt | llm | parsercontent = chain.invoke(&#123;"query": req.query.data&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p><strong>Runnable协议核心方法</strong></p><ul><li><p>invoke&#x2F;ainvoke: 调用组件</p></li><li><p>batch&#x2F;abatch: 批量处理</p></li><li><p>stream&#x2F;astream: 流式输出</p></li><li><p>transform: 转换输入输出</p></li></ul><p><strong>两个核心类</strong></p><ol><li>RunnableParallel - 并行执行多个Runnable</li></ol><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.runnables import RunnableParallel# 并行执行多个链chain = RunnableParallel(    joke=joke_chain,    poem=poem_chain)resp = chain.invoke(&#123;"subject": "程序员"&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>RunnablePassthrough - 传递数据</li></ol><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.runnables import RunnablePassthrough# 构建检索链chain = (    RunnablePassthrough.assign(        context=lambda query: retrieval(query)    )    | prompt     | llm     | parser)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>实践示例</strong></p><ol><li>基础链构建：</li></ol><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.prompts import ChatPromptTemplatefrom langchain_openai import ChatOpenAIfrom langchain_core.output_parsers import StrOutputParser# 创建组件prompt = ChatPromptTemplate.from_template("&#123;input&#125;")llm = ChatOpenAI()parser = StrOutputParser()# 构建链chain = prompt | llm | parser# 执行response = chain.invoke(&#123;"input": "Hello!"&#125;)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ol start="2"><li>带检索的链：</li></ol><pre class="line-numbers language-language-python"><code class="language-language-python">def retrieval(query: str) -> str:    return "相关文档内容..."# 构建链chain = (    &#123;        "context": retrieval,        "question": RunnablePassthrough()    &#125;    | prompt    | llm    | StrOutputParser())# 执行response = chain.invoke("问题")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>最佳实践</strong></p><ol><li><p>链的设计</p><ul><li>使用管道操作符(|)构建简单链</li><li>复杂逻辑使用RunnableParallel</li><li>数据传递用RunnablePassthrough</li></ul></li><li><p>错误处理</p><ul><li>合理使用try&#x2F;except</li><li>实现错误回调处理</li></ul></li><li><p>性能优化</p><ul><li>合适场景使用并行执行</li><li>批处理代替单个处理</li></ul></li><li><p>代码可维护性</p><ul><li>链结构保持清晰</li><li>适当拆分复杂链</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Langchain </tag>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LangChain RAG 应用开发组件深度解析</title>
      <link href="/2025/02/09/langchain-rag-ying-yong-kai-fa-zu-jian-shen-du-jie-xi/"/>
      <url>/2025/02/09/langchain-rag-ying-yong-kai-fa-zu-jian-shen-du-jie-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在当今的AI应用开发中，检索增强生成(Retrieval-Augmented Generation, RAG)已经成为一种重要的技术范式。它通过将大语言模型与外部知识库结合，极大地提升了AI系统的知识获取能力和输出质量。而LangChain作为一个强大的框架，为RAG应用的开发提供了丰富的组件支持。本文将深入剖析LangChain中RAG应用开发的核心组件，帮助你更好地理解和使用这些工具。</p><h2 id="核心组件概览"><a href="#核心组件概览" class="headerlink" title="核心组件概览"></a>核心组件概览</h2><p>在开始深入学习之前,我们先来了解LangChain中RAG应用开发涉及的主要组件:</p><ol><li>Document组件与文档加载器 - 负责文档的加载和基础处理</li><li>文档转换器与分割器 - 处理文档转换和分块</li><li>VectorStore组件 - 实现向量存储和检索</li><li>Blob相关组件 - 处理二进制大对象数据</li></ol><h3 id="Document-组件与文档加载器详解"><a href="#Document-组件与文档加载器详解" class="headerlink" title="Document 组件与文档加载器详解"></a>Document 组件与文档加载器详解</h3><h4 id="Document-组件基础"><a href="#Document-组件基础" class="headerlink" title="Document 组件基础"></a>Document 组件基础</h4><p>Document 是 LangChain 的核心组件之一，它定义了一个通用的文档结构，包含两个基本要素：</p><pre class="line-numbers language-language-python"><code class="language-language-python">Document = page_content(页面内容) + metadata(元数据)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这种结构允许我们统一处理各种类型的文档，同时保留文档的元信息。</p><h4 id="文档加载器类型"><a href="#文档加载器类型" class="headerlink" title="文档加载器类型"></a>文档加载器类型</h4><p>LangChain 提供了多种文档加载器：</p><ol><li>通用文本加载器</li><li>CSV文件加载器</li><li>HTML网页加载器</li><li>PDF文档加载器</li><li>Markdown文档加载器</li></ol><p><img src="/medias/featureimages/blog/langchain-rag-ying-yong-kai-fa-zu-jian-shen-du-jie-xi/image1.png" alt="image"></p><p>每种加载器都专门处理特定类型的文档，但它们都会将文档转换成统一的Document格式。</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_community.document_loaders import TextLoader# 加载文本文件示例loader = TextLoader("./data.txt", encoding="utf-8")documents = loader.load()<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="异步加载支持"><a href="#异步加载支持" class="headerlink" title="异步加载支持"></a>异步加载支持</h4><p>对于大型文档，LangChain提供了异步加载方式：</p><pre class="line-numbers language-language-python"><code class="language-language-python">async def load_documents():    async with aiofiles.open(file_path, encoding="utf-8") as f:        # 异步处理文档        yield Document(            page_content=line,            metadata=&#123;"source": file_path&#125;        )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="文档转换器与分割器"><a href="#文档转换器与分割器" class="headerlink" title="文档转换器与分割器"></a>文档转换器与分割器</h3><h4 id="DocumentTransformer-组件"><a href="#DocumentTransformer-组件" class="headerlink" title="DocumentTransformer 组件"></a>DocumentTransformer 组件</h4><p>文档转换器用于处理以下常见问题：</p><ol><li>文档太大导致的性能问题</li><li>原始文档格式不符合要求</li><li>文档内容需要标准化处理</li></ol><h4 id="文档转换器的工作原理"><a href="#文档转换器的工作原理" class="headerlink" title="文档转换器的工作原理"></a>文档转换器的工作原理</h4><p>DocumentTransformer组件的主要职责是对文档进行各种转换操作，包括：</p><ol><li>文档切割</li><li>文档层级提取</li><li>文档翻译</li><li>HTML标签处理</li><li>重排等多个功能</li></ol><p>在LangChain中，所有文档转换器都继承自BaseDocumentTransformer基类，它提供了两个核心方法：</p><pre class="line-numbers language-language-python"><code class="language-language-python">class BaseDocumentTransformer:    def transform_documents(self):         # 转换文档列表        pass            async def atransform_documents(self):        # 异步转换处理        pass<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="文档分割器详解"><a href="#文档分割器详解" class="headerlink" title="文档分割器详解"></a>文档分割器详解</h3><h4 id="字符分割器（CharacterTextSplitter）"><a href="#字符分割器（CharacterTextSplitter）" class="headerlink" title="字符分割器（CharacterTextSplitter）"></a>字符分割器（CharacterTextSplitter）</h4><p>CharacterTextSplitter 是基础的分割器，它有以下重要参数：</p><ol><li><code>separator</code>: 分割符,默认为’\n\n’</li><li><code>chunk_size</code>: 每块文本的最大大小,默认4000</li><li><code>chunk_overlap</code>: 块与块之间的重叠大小,默认200</li><li><code>length_function</code>: 计算文本长度的函数,默认len</li><li><code>keep_separator</code>: 是否在分割的块中保留分隔符</li></ol><p>使用示例：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_text_splitters import CharacterTextSplittertext_splitter = CharacterTextSplitter(    separator="\n\n",    chunk_size=500,    chunk_overlap=50,    add_start_index=True)# 使用分割器处理文档splits = text_splitter.split_documents(documents)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a>实践建议</h4><p>在实际应用中，有以下几点建议：</p><ol><li>选择合适的chunk_size<ul><li>太大会影响处理效率</li><li>太小可能破坏语义完整性</li><li>建议根据实际需求在400-1000之间调整</li></ul></li><li>合理设置overlap<ul><li>设置适当的重叠可以保持上下文连贯</li><li>通常设置为chunk_size的10%-20%</li></ul></li><li>注意分隔符的选择<ul><li>根据文档类型选择合适的分隔符</li><li>可以使用多级分隔符策略</li></ul></li></ol><h3 id="VectorStore组件与检索器"><a href="#VectorStore组件与检索器" class="headerlink" title="VectorStore组件与检索器"></a>VectorStore组件与检索器</h3><h4 id="VectorStore基础概念"><a href="#VectorStore基础概念" class="headerlink" title="VectorStore基础概念"></a>VectorStore基础概念</h4><p>VectorStore组件负责：</p><ol><li>存储文档的向量表示</li><li>提供相似性检索功能</li><li>支持不同的向量检索策略</li></ol><h4 id="检索器的使用"><a href="#检索器的使用" class="headerlink" title="检索器的使用"></a>检索器的使用</h4><p>LangChain 提供了多种检索策略：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain import VectorStore# 基础相似性检索results = vectorstore.similarity_search(query)# 带相似度分数的检索results = vectorstore.similarity_search_with_score(query)# MMR检索策略results = vectorstore.max_marginal_relevance_search(query)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="VectorStore实现细节"><a href="#VectorStore实现细节" class="headerlink" title="VectorStore实现细节"></a>VectorStore实现细节</h3><h4 id="支持的向量数据库"><a href="#支持的向量数据库" class="headerlink" title="支持的向量数据库"></a>支持的向量数据库</h4><ul><li>Chroma</li><li>FAISS</li><li>Pinecone</li><li>Milvus</li></ul><h4 id="检索策略详解"><a href="#检索策略详解" class="headerlink" title="检索策略详解"></a>检索策略详解</h4><ol><li>相似度检索(Similarity Search)<ul><li>基于余弦相似度</li><li>支持Top-K检索</li></ul></li><li>MMR检索(Maximum Marginal Relevance)<ul><li>平衡相关性和多样性</li><li>可配置lambda参数调整权重</li></ul></li><li>混合检索策略<ul><li>关键词+语义检索</li><li>支持自定义评分函数</li></ul></li></ol><h3 id="Blob与BlobParser组件"><a href="#Blob与BlobParser组件" class="headerlink" title="Blob与BlobParser组件"></a>Blob与BlobParser组件</h3><h4 id="Blob方案介绍"><a href="#Blob方案介绍" class="headerlink" title="Blob方案介绍"></a>Blob方案介绍</h4><p>Blob是LangChain处理二进制数据的解决方案，它具有以下特点：</p><ol><li>支持存储字节流数据</li><li>提供统一的数据访问接口</li><li>灵活的元数据管理</li></ol><p>基本使用示例：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.document_loaders import Blobfrom langchain_core.document_loaders.base import BaseBlobParser# 创建Blob对象blob = Blob.from_path("./data.txt")# 使用解析器parser = CustomParser()documents = list(parser.lazy_parse(blob))<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="Blob数据存储类详解"><a href="#Blob数据存储类详解" class="headerlink" title="Blob数据存储类详解"></a>Blob数据存储类详解</h4><p>LangChain中的Blob数据存储提供了丰富的属性和方法，让我们详细了解一下：</p><p><strong>核心属性</strong></p><ol><li><strong>data</strong>: 原始数据，支持存储字节，字符串数据</li><li><strong>mimetype</strong>: 文件的mimetype类型</li><li><strong>encoding</strong>: 文件的编码，默认utf-8</li><li><strong>path</strong>: 文件的原始路径</li><li><strong>metadata</strong>: 存储的元数据，通常包含source字段</li></ol><p><strong>常用方法</strong></p><pre class="line-numbers language-language-python"><code class="language-language-python"># 字符串转换as_string(): # 将数据转换为字符串# 字节转换as_bytes(): # 将数据转换为字节数据# 字节流操作as_bytes_io(): # 将数据转换为字节流# 从路径加载from_path(): # 从文件路径加载Blob数据# 从原始数据加载from_data(): # 从原始数据加载Blob数据<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="BlobLoader实现"><a href="#BlobLoader实现" class="headerlink" title="BlobLoader实现"></a>BlobLoader实现</h4><p>BlobLoader是一个抽象接口，用于实现二进制数据的加载。以下是一个自定义BlobLoader的示例：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_core.document_loaders import Blobfrom langchain_core.document_loaders.base import BaseBlobParserclass CustomBlobLoader(ABC):    """自定义Blob加载器实现"""        @abstractmethod    def yield_blobs(        self,    ) -> Iterable[Blob]:        """加载并返回Blob数据流"""            def __init__(self, file_path: str):        self.file_path = file_path            def lazy_load(self):        """延迟加载实现"""        for blob in self.yield_blobs():            yield Document(                page_content=blob.as_string(),                metadata=&#123;"source": blob.source&#125;            )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="通用加载器使用最佳实践"><a href="#通用加载器使用最佳实践" class="headerlink" title="通用加载器使用最佳实践"></a>通用加载器使用最佳实践</h4><p>GenericLoader是LangChain提供的一个通用加载器，它结合了BlobLoader和BaseBlobParser的功能：</p><pre class="line-numbers language-language-python"><code class="language-language-python">from langchain_community.document_loaders.generic import GenericLoader# 创建通用加载器loader = GenericLoader.from_filesystem(    "./",  # 文件系统路径    glob="*.txt",  # 文件匹配模式    show_progress=True  # 显示进度)# 使用加载器for idx, doc in enumerate(loader.lazy_load()):    print(f"当前加载第&#123;idx + 1&#125;个文件, 文件信息:&#123;doc.metadata&#125;")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="性能优化建议"><a href="#性能优化建议" class="headerlink" title="性能优化建议"></a>性能优化建议</h4><ol><li>使用延迟加载<ul><li>对于大文件优先使用lazy_load()</li><li>避免一次性加载全部内容</li></ul></li><li>合理配置缓存<ul><li>利用缓存减少重复加载</li><li>及时清理不需要的缓存</li></ul></li><li>错误处理<ul><li>实现适当的错误处理机制</li><li>记录加载过程中的异常</li></ul></li><li>进度监控<ul><li>对大规模数据处理添加进度显示</li><li>实现断点续传机制</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Langchain </tag>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
