<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>大模型 RAG 应用开发基础及入门</title>
      <link href="/2025/02/12/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/"/>
      <url>/2025/02/12/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/</url>
      
        <content type="html"><![CDATA[<h1 id="大语言模型中的幻觉问题-上"><a href="#大语言模型中的幻觉问题-上" class="headerlink" title="大语言模型中的幻觉问题(上)"></a>大语言模型中的幻觉问题(上)</h1><h2 id="什么是大语言模型的幻觉？"><a href="#什么是大语言模型的幻觉？" class="headerlink" title="什么是大语言模型的幻觉？"></a>什么是大语言模型的幻觉？</h2><p>大语言模型在处理自然语言时，有时候会出现”幻觉“现象。所谓幻觉，就是模型生成的内容与事实或上下文不一致的问题。这些问题会严重影响AI应用的可靠性和实用性。</p><h2 id="幻觉的两大类型"><a href="#幻觉的两大类型" class="headerlink" title="幻觉的两大类型"></a>幻觉的两大类型</h2><h3 id="事实性幻觉"><a href="#事实性幻觉" class="headerlink" title="事实性幻觉"></a>事实性幻觉</h3><p>指模型生成的内容与实际事实不匹配。比如在回答”第一个登上月球的人是谁?”这个问题时:</p><ul><li>错误回答: “Charles Lindbergh在1951年月球任务中第一个登上月球”</li><li>正确事实: Neil Armstrong才是第一个登上月球的人(1969年阿波罗11号任务)</li></ul><p>这种幻觉之所以危险，是因为模型生成的内容看起来很可信，但实际上完全错误。</p><h3 id="忠实性幻觉"><a href="#忠实性幻觉" class="headerlink" title="忠实性幻觉"></a>忠实性幻觉</h3><p>指模型生成的内容与提供的上下文不一致。这种幻觉可以分为三类：</p><ul><li>输出与原文不一致（编出原文中没有的信息）</li><li>上下文之间不一致（前后矛盾）</li><li>逻辑链不一致（推理过程存在漏洞）</li></ul><p>比如在总结新闻时，模型可能会添加原文中不存在的细节，或者前后描述矛盾。</p><h2 id="为什么会产生幻觉？"><a href="#为什么会产生幻觉？" class="headerlink" title="为什么会产生幻觉？"></a>为什么会产生幻觉？</h2><p>大语言模型产生幻觉的原因主要来自三个方面：</p><ol><li>数据源导致的幻觉<ul><li>训练数据中的质量问题</li><li>数据中存在的错误信息</li><li>数据覆盖范围有限</li></ul></li><li>训练过程导致的幻觉<ul><li>架构限制：无法准确理解长文本的上下文关联</li><li>累积错误：生成过程中的错误会逐步传递和放大</li></ul></li><li>推理相关的幻觉<ul><li>回答过于简略</li><li>生成过程中的不完整推理</li></ul></li></ol><h2 id="如何评估幻觉问题"><a href="#如何评估幻觉问题" class="headerlink" title="如何评估幻觉问题"></a>如何评估幻觉问题</h2><p>为了客观评估模型的幻觉问题，我们可以使用多种方法：</p><ol><li>事实一致性评估：将生成内容与权威来源进行比对</li><li>分类器评估：使用专门训练的模型来检测是否存在幻觉</li><li>问答测量：通过问答来验证生成内容的一致性</li><li>不确定度分析：评估模型对自身输出的确信程度</li><li>提示测量：让模型自我评估，通过特定提示策略来评估生成内容</li></ol><h1 id="大语言模型中的幻觉问题-下-："><a href="#大语言模型中的幻觉问题-下-：" class="headerlink" title="大语言模型中的幻觉问题(下)："></a>大语言模型中的幻觉问题(下)：</h1><h2 id="RAG解决方案"><a href="#RAG解决方案" class="headerlink" title="RAG解决方案"></a>RAG解决方案</h2><h3 id="RAG是什么？"><a href="#RAG是什么？" class="headerlink" title="RAG是什么？"></a>RAG是什么？</h3><p><strong>RAG</strong>（Retrieval-Augmented Generation）也叫<strong>检索增强生成</strong>，是指对大语言模型输出进行优化，使其能够参考并利用数据源之外的权威知识。简单来说，RAG就是从外部检索对应的知识内容，和用户的提问一起构成Prompt发给大模型，再让大模型生成内容。</p><p>它的核心思想是：</p><ol><li><strong>从外部知识库检索相关信息</strong></li><li><strong>将检索到的信息作为上下文提供给模型</strong></li><li><strong>让模型基于这些上下文生成回答</strong></li></ol><p>简单来说：RAG &#x3D; 外部知识检索 + Prompt构建 + LLM 生成</p><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image1.png" alt="image"></p><h3 id="为什么需要RAG？"><a href="#为什么需要RAG？" class="headerlink" title="为什么需要RAG？"></a>为什么需要RAG？</h3><p>LLM虽然是一个强大的工具，但它本身拒绝了解任何时事，且它给出的答案总是非常流畅，内容却不一定靠谱。这存在几个主要的问题:</p><ol><li>LLM的训练数据量有限且无法更新到最新知识。</li><li>当用户需要专业或领域特定的数据时，LLM往往缺乏相应的知识</li><li>对于答案的问答内容很难从源创进行溯源</li><li>由于技术限制，不同的训练源使用相同的大语言技术，可能会产生不确信的响应</li></ol><p>而RAG为解决这些问题带来了以下优势：</p><ul><li><strong>经济高效</strong>：预训练和微调模型的成本很高，而RAG是一种经济高效的新方法</li><li><strong>信息时效</strong>：使用RAG可以为LLM提供最新的研究、统计数据或新闻</li><li><strong>增强用户信任度</strong>：RAG允许LLM通过来源归属来呈现具体的信息，输出可以包括对来源的引文或参考，这可以增加对对话的生成式人工智能解决方案的任何信心</li></ul><h3 id="RAG是如何工作的？"><a href="#RAG是如何工作的？" class="headerlink" title="RAG是如何工作的？"></a>RAG是如何工作的？</h3><p>RAG采用三种主要的检索方式：</p><ol><li><strong>一次性检索</strong>：<ul><li>从单次检索中获取相关知识</li><li>直接预置到大模型的提示词中</li><li>不会收集反馈信息</li></ul></li><li><strong>迭代检索</strong>：<ul><li>允许在对话过程中多次检索</li><li>每一轮都可能有新的检索</li><li>支持多轮对话优化</li></ul></li><li><strong>事后检索</strong>：<ul><li>先生成答案</li><li>然后检索验证</li><li>对答案进行修正</li></ul></li></ol><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image2.png" alt="image"></p><h3 id="RAG实战示例"><a href="#RAG实战示例" class="headerlink" title="RAG实战示例"></a>RAG实战示例</h3><p>以一个简单的问答场景为例，展示RAG的实际应用流程:</p><ol><li>用户提问:”公司有销售什么产品？”</li><li>系统处理流程:<ul><li>使用检索器获取产品相关文档</li><li>将文档内容与问题组合成提示词</li><li>通过LLM生成回答</li><li>确保回答基于检索到的事实信息</li></ul></li><li>最终输出:包含准确的产品信息，并且所有信息都可以溯源。</li></ol><h2 id="AI应用开发利器：向量数据库详解"><a href="#AI应用开发利器：向量数据库详解" class="headerlink" title="AI应用开发利器：向量数据库详解"></a>AI应用开发利器：向量数据库详解</h2><h3 id="什么是向量数据库？"><a href="#什么是向量数据库？" class="headerlink" title="什么是向量数据库？"></a>什么是向量数据库？</h3><p>向量数据库（Vector Database）是一种专门用于存储和处理向量数据的数据库系统。它不同于传统的关系型数据库，因为它需要将所有数据映射为特定的向量格式，并采用相似性搜索作为主要的检索方式。</p><h3 id="一个生动的例子：识别猫咪"><a href="#一个生动的例子：识别猫咪" class="headerlink" title="一个生动的例子：识别猫咪"></a>一个生动的例子：识别猫咪</h3><p>让我们通过一个识别猫咪的例子来理解向量数据库。假设我们有一组不同品种的猫咪图片：</p><ul><li>波斯猫</li><li>英国短毛猫</li><li>暹罗猫</li><li>布偶猫</li><li>无毛猫</li></ul><p>每张猫咪图片都可以用一组数字向量来表示其特征，如:</p><figure class="highlight jsx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">波斯猫: [<span class="number">0.4</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, ...]</span><br><span class="line">英国短毛猫: [<span class="number">0.7</span>, <span class="number">0.2</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, ...]</span><br><span class="line">暹罗猫: [<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, <span class="number">0.5</span>, ...]</span><br></pre></td></tr></table></figure><p>这些数字代表了猫咪的各种特征，比如:</p><ul><li>毛发长度</li><li>体型大小</li><li>面部特征</li><li>耳朵形状等等</li></ul><h3 id="向量数据库的优势"><a href="#向量数据库的优势" class="headerlink" title="向量数据库的优势"></a>向量数据库的优势</h3><p>与传统的数据库相比，向量数据库有以下特点：</p><ol><li><strong>数据类型</strong>：<ul><li>传统数据库：数值、字符串、时间等结构化数据</li><li>向量数据库：向量数据(不存储原始数据，有的也支持)</li></ul></li><li><strong>数据规模</strong>：<ul><li>传统数据库：小，1亿条数据对关系型数据库来说规模很大</li><li>向量数据库：大，最少千亿数据是基线</li></ul></li><li><strong>数据组织方式</strong>：<ul><li>传统数据库：基于表格、按照行和列组织</li><li>向量数据库：基于向量、按向量维度组织</li></ul></li><li><strong>查找方式</strong>：<ul><li>传统数据库：精确查找&#x2F;范围查找</li><li>向量数据库：近似查找，查询结果是与输入向量最相似的向量</li></ul></li></ol><h3 id="相似性搜索算法"><a href="#相似性搜索算法" class="headerlink" title="相似性搜索算法"></a>相似性搜索算法</h3><p>在向量数据库中，支持通过多种方式来计算两个向量的相似度：</p><p><strong>余弦相似度</strong>：主要是用于衡量向量在方向上的相似性，特别适用于文本、图像和高维空间中的向量。它不受向量长度的影响，只考虑方向的相似程度，计算公式如下（计算两个向量间的夹角的余弦值，取值范围为[-1, 1]）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">similarity(A,B) = (A·B)/(||A||·||B||)</span><br></pre></td></tr></table></figure><p><strong>欧式距离</strong>：主要是用于衡量向量之间的直线距离，得到的值可能很大，最小为0，通常用于低维空间或需要考虑向量各个维度之间差异的情况。欧式距离较小的向量被认为更相似，计算公式如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">distance(A,B) = √∑(Ai-Bi)²</span><br></pre></td></tr></table></figure><p>例如下图：左侧就是<code>欧式距离</code>，右侧就是<code>余弦相似度</code>。</p><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image3.png" alt="image"></p><h3 id="实际应用场景"><a href="#实际应用场景" class="headerlink" title="实际应用场景"></a>实际应用场景</h3><p>向量数据库的主要应用场景包括：</p><ol><li>人脸识别</li><li>图像搜索</li><li>音频识别</li><li>智能推荐系统</li></ol><p>这些场景的共同特点是：需要对非结构化数据（如图片、文本、音频）进行相似度搜索。</p><p>在RAG中，我们会将文档的知识按特定规则分成小块，转换成向量存储到向量数据库中。当人类提问时，我们将问题转换为向量，在数据库中找到最相似的文本块，这些文本块可以成为Prompt的补充内容。</p><h2 id="深入理解Embedding嵌入技术"><a href="#深入理解Embedding嵌入技术" class="headerlink" title="深入理解Embedding嵌入技术"></a>深入理解Embedding嵌入技术</h2><h3 id="Embedding-是什么？"><a href="#Embedding-是什么？" class="headerlink" title="Embedding 是什么？"></a>Embedding 是什么？</h3><p>Embedding(嵌入)是一种在机器学习中广泛使用的技术，它能将文本、图片、视频等非结构化数据映射到向量空间中。一个Embedding向量通常是一个包含N个浮点数的数组，这个向量不仅表示了数据的特征，更重要的是通过学习可以表达它们的内在语义。简而言之，Embedding就是一个模型生成方法，可以将非结构化的数据，例如文本&#x2F;图片&#x2F;视频等数据映射成有意义的向量数据。比如一段文本、一张图片、一段视频，警告Embedding模型处理后都会变成类似这样的向量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0.5, 0.8, 0.7, 0.5, 0.8, 0.7, 0.5, 0.8, 0.7, 0.5]</span><br></pre></td></tr></table></figure><p><img src="/medias/featureimages/blog/da-mo-xing-rag-ying-yong-kai-fa-ji-chu-ji-ru-men/image4.png" alt="image"></p><h3 id="主流的Embedding模型"><a href="#主流的Embedding模型" class="headerlink" title="主流的Embedding模型"></a>主流的Embedding模型</h3><p>目前主要有这几类Embedding模型：</p><ol><li><strong>Word2Vec（词嵌入模型）</strong><ul><li>通过学习词语转化为连续的向量表示</li><li>基于两种主要算法：<code>CBOW</code> 和 <code>Skip-gram</code></li><li>能够捕捉词语之间的语义关系</li></ul></li><li><strong>1GloVe</strong><ul><li>类似Word2Vec但采用不同的训练方式</li><li>同时考虑全局共现信息</li><li>能较好地保存词语间的语义关系</li><li>适用于多种自然语言处理任务</li></ul></li><li><strong>FastText</strong><ul><li>考虑了单词的子词信息</li><li>能处理训练集中未出现的生词</li><li>支持多语言处理</li></ul></li><li><strong>大模型Embeddings</strong><ul><li>如OpenAI的text-embedding-ada-002</li><li>输入维度8191个tokens</li><li>输出维度1536维向量</li></ul></li></ol><h3 id="Embedding的神奇之处"><a href="#Embedding的神奇之处" class="headerlink" title="Embedding的神奇之处"></a>Embedding的神奇之处</h3><p>Embedding最有趣的特性是它能够捕捉语义关系。让我们看一个著名的例子</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">King - Man + Woman ≈ Queen</span><br><span class="line">(国王 - 男人 + 女人 ≈ 女王)</span><br></pre></td></tr></table></figure><p>这个公式展示了Embedding不仅仅是把词语转换成数字，它还能：</p><ol><li>保留词语之间的关系</li><li>支持向量运算</li><li>产生有意义的结果</li></ol><p>我们可以通过可视化的方式看到这些词语在向量空间中的分布：</p><ul><li>woman和girl的向量位置接近</li><li>man和boy的向量位置接近</li><li>king和queen虽然性别不同，但都位于表示”统治者”的维度上</li></ul><h3 id="Embedding的重要价值"><a href="#Embedding的重要价值" class="headerlink" title="Embedding的重要价值"></a>Embedding的重要价值</h3><ol><li><strong>降维</strong>：将高维数据映射到低维空间，大大降低了计算复杂度</li><li><strong>捕捉语义信息</strong>：不仅能记录表面的词频信息，还能捕捉深层的语义关联</li><li><strong>泛化性</strong>：Embedding学习到的是通用的语言表达方式，可以应用到新的场景</li><li><strong>泛化能力</strong>：对于未见过的数据，也能基于已学习的语义特征给出合理的向量表示</li><li><strong>可视化支持</strong>：虽然Embedding本身很复杂，但我们可以使用t-SNE等工具将其可视化，帮助理解数据的内在结构。</li></ol><h3 id="在RAG中的应用"><a href="#在RAG中的应用" class="headerlink" title="在RAG中的应用"></a>在RAG中的应用</h3><p>在RAG系统中，Embedding主要用于两个场景：</p><ol><li><strong>文档向量化</strong>：将知识库中的文档转换为向量</li><li><strong>查询向量化</strong>：将用户的问题转换为向量</li></ol><p>通过比较这些向量的相似度，我们可以找到与用户问题最相关的文档片段，从而提供更准确的答案。</p><h2 id="RAG应用实战-OpenAI-Embedding与LangChain的结合"><a href="#RAG应用实战-OpenAI-Embedding与LangChain的结合" class="headerlink" title="RAG应用实战:OpenAI Embedding与LangChain的结合"></a>RAG应用实战:OpenAI Embedding与LangChain的结合</h2><h3 id="OpenAI-Embedding接口简介"><a href="#OpenAI-Embedding接口简介" class="headerlink" title="OpenAI Embedding接口简介"></a>OpenAI Embedding接口简介</h3><p>OpenAI提供了多个Embedding模型选择，以下是几个主要模型的对比:</p><table><thead><tr><th>模型</th><th>Token数(每个文档800个)</th><th>性能评估</th><th>最大输入</th><th>向量维度</th></tr></thead><tbody><tr><td>text-embedding-3-small</td><td>62,500</td><td>62.3%</td><td>8191</td><td>1536</td></tr><tr><td>text-embedding-3-large</td><td>9,615</td><td>64.6%</td><td>8191</td><td>3072</td></tr><tr><td>text-embedding-ada-002</td><td>12,500</td><td>61.0%</td><td>8191</td><td>1536</td></tr></tbody></table><h3 id="LangChain中的Embedding组件使用"><a href="#LangChain中的Embedding组件使用" class="headerlink" title="LangChain中的Embedding组件使用"></a>LangChain中的Embedding组件使用</h3><p>在LangChain中，Embedding类提供了统一的接口来使用各种嵌入模型:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">class Embeddings(ABC):</span><br><span class="line">    <span class="string">&quot;&quot;</span><span class="string">&quot;Interface for embedding models.&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line">    @abstractmethod</span><br><span class="line">    def embed_documents(self, texts: List[str]) -&gt; List[List[<span class="built_in">float</span>]]:</span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="string">&quot;Embed search docs.&quot;</span><span class="string">&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">    @abstractmethod</span><br><span class="line">    def embed_query(self, text: str) -&gt; List[<span class="built_in">float</span>]:</span><br><span class="line">        <span class="string">&quot;&quot;</span><span class="string">&quot;Embed query text.&quot;</span><span class="string">&quot;&quot;</span></span><br></pre></td></tr></table></figure><p>使用示例:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> dotenv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> OpenAIEmbeddings</span><br><span class="line"><span class="keyword">from</span> numpy.linalg <span class="keyword">import</span> norm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化Embedding模型</span></span><br><span class="line">embeddings = OpenAIEmbeddings()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进行文本嵌入</span></span><br><span class="line">query_vector = embeddings.embed_query(<span class="string">&quot;你好, 我是小潘&quot;</span>)</span><br><span class="line">documents_vector = embeddings.embed_documents([</span><br><span class="line">    <span class="string">&quot;你好, 我是小潘&quot;</span>,</span><br><span class="line">    <span class="string">&quot;这个自然语言处理的人叫小潘&quot;</span>,</span><br><span class="line">    <span class="string">&quot;来知若惘, 既心若旷&quot;</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相似度</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cosine_similarity</span>(<span class="params">vector1, vector2</span>):</span><br><span class="line">    dot_product = np.dot(vector1, vector2)</span><br><span class="line">    norm1 = norm(vector1)</span><br><span class="line">    norm2 = norm(vector2)</span><br><span class="line">    <span class="keyword">return</span> dot_product / (norm1 * norm2)</span><br></pre></td></tr></table></figure><h3 id="CacheBackEmbedding的使用"><a href="#CacheBackEmbedding的使用" class="headerlink" title="CacheBackEmbedding的使用"></a>CacheBackEmbedding的使用</h3><p>为了提高性能，LangChain提供了缓存功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.embeddings <span class="keyword">import</span> CacheBackedEmbeddings</span><br><span class="line"><span class="keyword">from</span> langchain.storage <span class="keyword">import</span> LocalFileStore</span><br><span class="line"></span><br><span class="line">embeddings = OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)</span><br><span class="line">embeddings_with_cache = CacheBackedEmbeddings.from_bytes_store(</span><br><span class="line">    embeddings,</span><br><span class="line">    LocalFileStore(<span class="string">&quot;./cache/&quot;</span>),</span><br><span class="line">    namespace=embeddings.model,</span><br><span class="line">    query_embedding_cache=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>使用缓存时需要注意：</p><ol><li>underlying_embedder: 使用的基础嵌入模型</li><li>document_embedding_cache: 用于缓存文档的存储结构</li><li>batch_size: 可选参数，默认None</li><li>namespace: 用于文档缓存的命名空间</li><li>query_embedding_cache: 是否缓存查询向量</li></ol><h3 id="运行流程分析"><a href="#运行流程分析" class="headerlink" title="运行流程分析"></a>运行流程分析</h3><p>一个完整的RAG应用运行流程如下：</p><ol><li><strong>文档预处理</strong><ul><li>分割文档</li><li>生成向量</li><li>存入向量数据库</li></ul></li><li><strong>查询处理</strong><ul><li>将用户问题转为向量</li><li>在向量数据库中检索</li><li>组合上下文生成回答</li></ul></li><li><strong>缓存优化</strong><ul><li>缓存常见文档的向量</li><li>缓存常见查询的向量</li><li>提供响应速度</li></ul></li></ol><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><ol><li>向量维度的选择<ul><li>需要平衡精度和效率</li><li>维度越高，表达能力越强，但计算成本也越高</li></ul></li><li>缓存策略<ul><li>合理设置缓存大小</li><li>选择适当的缓存淘汰策略</li><li>定期更新缓存</li></ul></li><li>性能优化<ul><li>使用批处理提高效率</li><li>合理使用多线程</li><li>监控资源使用情况</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LangChain RAG 应用开发优化策略详解</title>
      <link href="/2025/02/12/langchain-rag-ying-yong-kai-fa-you-hua-ce-lue-xiang-jie/"/>
      <url>/2025/02/12/langchain-rag-ying-yong-kai-fa-you-hua-ce-lue-xiang-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="引言：理解RAG及其重要性"><a href="#引言：理解RAG及其重要性" class="headerlink" title="引言：理解RAG及其重要性"></a>引言：理解RAG及其重要性</h2><p>在大语言模型（LLM）应用开发中，检索增强生成（Retrival-Augmented Generation, RAG）已经成为提升模型输出质量的关键技术。本文将深入探讨在LangChain框架中如何优化RAG应用，帮助开发者构建更智能、更准确的AI应用。</p><h2 id="RAG的基本概念"><a href="#RAG的基本概念" class="headerlink" title="RAG的基本概念"></a>RAG的基本概念</h2><blockquote><p>📌 什么是RAG?<br>RAG是一种将外部知识检索与语言模型生成相结合的技术架构。它通过检索相关信息来增强LLM的知识储备，从而产生更准确、更可靠的输出。</p></blockquote><h3 id="为什么需要优化RAG？"><a href="#为什么需要优化RAG？" class="headerlink" title="为什么需要优化RAG？"></a>为什么需要优化RAG？</h3><p>在实际应用中，基础的RAG实现往往会遇到以下挑战：</p><ol><li>检索准确性不足</li><li>复杂问题处理能力有限</li><li>知识关联不够紧密</li><li>响应质量不够稳定</li></ol><p>这些问题促使我们需要采用多种优化策略来提升RAG的性能。</p><h3 id="第一部分：多查询检索优化策略"><a href="#第一部分：多查询检索优化策略" class="headerlink" title="第一部分：多查询检索优化策略"></a>第一部分：多查询检索优化策略</h3><h4 id="理解多查询检索的必要性"><a href="#理解多查询检索的必要性" class="headerlink" title="理解多查询检索的必要性"></a>理解多查询检索的必要性</h4><p>在RAG应用中，单一查询往往无法完整捕捉用户问题的所有方面。例如，当用户问”Python如何实现多线程并发控制？“时，我们可能需要同时检索：</p><ul><li>Python线程基础知识</li><li>并发控制机制</li><li>线程安全实现方法</li></ul><h4 id="多查询检索的工作原理"><a href="#多查询检索的工作原理" class="headerlink" title="多查询检索的工作原理"></a>多查询检索的工作原理</h4><blockquote><p>🔍 核心思路：利用LLM的理解能力，将一个复杂查询拆分或重写为多个相关查询，然后通过融合算法整合检索结果。</p></blockquote><p><strong>工作流程</strong>：</p><ol><li><strong>查询重写</strong>：LLM将原始查询转换为多个相关查询</li><li><strong>并行检索</strong>：对每个查询进行独立检索</li><li><strong>结果融合</strong>：使用RRF（Reciprocal Rank Fusion）算法融合检索结果</li><li><strong>内容生成</strong>：将融合后的结果输入LLM生成最终答案</li></ol><h3 id="代码实现示例"><a href="#代码实现示例" class="headerlink" title="代码实现示例"></a>代码实现示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> MultiQueryRetriever</span><br><span class="line"><span class="keyword">from</span> langchain.chains <span class="keyword">import</span> LLMChain</span><br><span class="line"><span class="keyword">from</span> langchain.prompts <span class="keyword">import</span> PromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 创建多查询检索器</span></span><br><span class="line">retriever = MultiQueryRetriever(</span><br><span class="line">    retriever=base_retriever,</span><br><span class="line">    llm=ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>, temperature=<span class="number">0</span>),</span><br><span class="line">    prompt_template=<span class="string">&quot;&quot;&quot;基于用户的问题，生成3个不同的相关查询：</span></span><br><span class="line"><span class="string">    原始问题: &#123;question&#125;</span></span><br><span class="line"><span class="string">    生成的查询应该探索问题的不同方面。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 使用RRF算法融合结果</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rrf_fusion</span>(<span class="params">results, k=<span class="number">60</span></span>):</span><br><span class="line">    fused_scores = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> rank, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(results):</span><br><span class="line">        doc_str = doc.page_content</span><br><span class="line">        <span class="keyword">if</span> doc_str <span class="keyword">not</span> <span class="keyword">in</span> fused_scores:</span><br><span class="line">            fused_scores[doc_str] = <span class="number">1.0</span> / (k + rank + <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            fused_scores[doc_str] += <span class="number">1.0</span> / (k + rank + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 排序并返回结果</span></span><br><span class="line">    sorted_results = <span class="built_in">sorted</span>(fused_scores.items(), </span><br><span class="line">                          key=<span class="keyword">lambda</span> x: x[<span class="number">1</span>], </span><br><span class="line">                          reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted_results</span><br></pre></td></tr></table></figure><p>RRF 算法原理如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">RRF (Reciprocal Rank Fusion) 算法的核心公式：</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">RRFscore(d ∈ D) = ∑ 1/(k + r(d))</span></span><br><span class="line"><span class="string">其中：</span></span><br><span class="line"><span class="string">- d 是文档</span></span><br><span class="line"><span class="string">- D 是所有文档集合</span></span><br><span class="line"><span class="string">- k 是一个常数(通常取60)</span></span><br><span class="line"><span class="string">- r(d)是文档d在排序中的位置</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">这个公式的特点：</span></span><br><span class="line"><span class="string">1. 对排名靠前的文档给予更高的权重</span></span><br><span class="line"><span class="string">2. k参数可以调节排名的影响程度</span></span><br><span class="line"><span class="string">3. 适合融合不同来源的排序结果</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><h4 id="优化效果分析"><a href="#优化效果分析" class="headerlink" title="优化效果分析"></a>优化效果分析</h4><p>多查询检索策略带来的主要优势：</p><ol><li><strong>提升召回率</strong><ul><li>通过多角度查询提高相关文档的覆盖率</li><li>减少因单一查询表达不当导致的漏检</li></ul></li><li><strong>提高准确性</strong><ul><li>RRF融合算法可以突出高质量的共同结果</li><li>降低单个查询的噪声影响</li></ul></li><li><strong>增强鲁棒性</strong><ul><li>对查询表达的变化更不敏感</li><li>能更好地处理复杂或模糊的问题</li></ul></li></ol><h4 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a>实践建议</h4><p>在实际应用中，需要注意以下几点：</p><ul><li><strong>查询数量选择</strong>：通常生成3-5个查询即可，过多查询可能引入噪声</li><li><strong>相似度阈值设置</strong>：建议在RRF融合时设置合适的相似度阈值，过滤低相关性结果</li><li><strong>资源消耗考虑</strong>：多查询会增加API调用和计算资源，需要在效果和成本间权衡</li></ul><blockquote><p>💡 实践小贴士：可以通过监控检索结果的diversity和relevance指标，来调整多查询策略的参数。</p></blockquote><h3 id="第二部分：问题分解策略优化"><a href="#第二部分：问题分解策略优化" class="headerlink" title="第二部分：问题分解策略优化"></a>第二部分：问题分解策略优化</h3><h4 id="复杂问题的分解处理"><a href="#复杂问题的分解处理" class="headerlink" title="复杂问题的分解处理"></a>复杂问题的分解处理</h4><p>在实际应用中，我们经常遇到复杂的多层次问题。例如：”请分析特斯拉近五年的财务状况，并评估其在电动汽车市场的竞争优势。”这类问题需要：</p><ul><li>处理大量相关信息</li><li>分析多个维度</li><li>综合多方面结论</li></ul><p><strong>并行分解模式</strong></p><blockquote><p>🔄 并行模式：将问题同时分解为多个独立子问题，分别获取答案后合并。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 并行分解示例</span></span><br><span class="line">decomposition_chain = &#123;</span><br><span class="line">    <span class="string">&quot;question&quot;</span>: RunnablePassthrough(),</span><br><span class="line">    | decomposition_prompt    <span class="comment"># 分解问题</span></span><br><span class="line">    | ChatOpenAI(temperature=<span class="number">0</span>)</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 并行处理子问题</span></span><br><span class="line">sub_questions = decomposition_chain.invoke(question)</span><br><span class="line">answers = <span class="keyword">await</span> asyncio.gather(*[</span><br><span class="line">    process_subquestion(q) <span class="keyword">for</span> q <span class="keyword">in</span> sub_questions</span><br><span class="line">])</span><br></pre></td></tr></table></figure><p><strong>串行分解模式</strong></p><blockquote><p>⛓️ 串行模式：按照逻辑顺序依次处理子问题，后面的问题依赖前面的答案。</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 串行分解示例</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">StepBackRetriever</span>(<span class="title class_ inherited__">BaseRetriever</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_relevant_documents</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self, query: <span class="built_in">str</span>, *, run_manager: CallbackManagerForRetrieverRun</span></span><br><span class="line"><span class="params">    </span>) -&gt; <span class="type">List</span>[Document]:</span><br><span class="line">        <span class="comment"># 1. 生成中间查询</span></span><br><span class="line">        intermediate_query = <span class="variable language_">self</span>.llm.predict(</span><br><span class="line">            <span class="string">f&quot;为了回答&#x27;<span class="subst">&#123;query&#125;</span>&#x27;，我们需要先了解什么？&quot;</span></span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 检索中间知识</span></span><br><span class="line">        intermediate_docs = <span class="variable language_">self</span>.retriever.get_relevant_documents(</span><br><span class="line">            intermediate_query</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 基于中间知识检索最终答案</span></span><br><span class="line">        final_docs = <span class="variable language_">self</span>.retriever.get_relevant_documents(query)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> intermediate_docs + final_docs</span><br></pre></td></tr></table></figure><h4 id="Step-Back-策略实现"><a href="#Step-Back-策略实现" class="headerlink" title="Step-Back 策略实现"></a>Step-Back 策略实现</h4><p>Step-Back策略是一种特殊的串行分解方法，它通过“后退一步”来获取更基础的知识背景。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">示例：用户问题&quot;量子计算机如何影响现代密码学？&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Step-Back分解：</span></span><br><span class="line"><span class="string">1. 基础知识查询：</span></span><br><span class="line"><span class="string">   - 什么是量子计算机的基本原理？</span></span><br><span class="line"><span class="string">   - 现代密码学的核心技术有哪些？</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">2. 关联分析：</span></span><br><span class="line"><span class="string">   - 量子计算对RSA等算法的影响</span></span><br><span class="line"><span class="string">   - 后量子密码学的发展</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">3. 最终综合：</span></span><br><span class="line"><span class="string">   基于以上知识形成完整答案</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure><p><strong>工作流程</strong>：</p><ol><li>分析原始问题</li><li>生成更基础的前置问题</li><li>获取基础知识</li><li>结合基础知识回答原问题</li></ol><h4 id="Step-Back-代码实现"><a href="#Step-Back-代码实现" class="headerlink" title="Step-Back 代码实现"></a>Step-Back 代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># </span></span><br><span class="line">system_prompt = <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">你是一位专业的助手，需要：</span></span><br><span class="line"><span class="string">1. 理解用户的具体问题</span></span><br><span class="line"><span class="string">2. 思考需要哪些基础知识</span></span><br><span class="line"><span class="string">3. 生成相关的基础问题</span></span><br><span class="line"><span class="string">4. 基于基础知识回答原问题</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">few_shot_prompt = FewShotChatMessagePromptTemplate(</span><br><span class="line">    example_prompt=example_prompt,</span><br><span class="line">    examples=examples,</span><br><span class="line">    suffix=<span class="string">&quot;现在，请帮我回答：&#123;question&#125;&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="优化效果对比"><a href="#优化效果对比" class="headerlink" title="优化效果对比"></a>优化效果对比</h4><table><thead><tr><th>分解策略</th><th>适用场景</th><th>优势</th><th>劣势</th></tr></thead><tbody><tr><td>并行分解</td><td>独立子问题</td><td>处理速度快，资源利用高</td><td>结果整合可能不够连贯</td></tr><tr><td>串行分解</td><td>逻辑依赖性强</td><td>答案更连贯，逻辑性强</td><td>处理时间较长</td></tr><tr><td>Step-Back</td><td>需要深入理解</td><td>回答更全面，准确度高</td><td>资源消耗较大</td></tr></tbody></table><h4 id="实践建议-1"><a href="#实践建议-1" class="headerlink" title="实践建议"></a>实践建议</h4><ol><li>选择策略时考虑因素: <ul><li>问题的复杂度</li><li>子问题间的依赖关系</li><li>响应时间要求</li><li>资源限制</li></ul></li><li>优化建议：<ul><li>对于并行模式，注意结果融合的质量</li><li>串行模式要控制分解的层级深度</li><li>Step-Back策略要平衡基础知识的范围</li></ul></li></ol><blockquote><p>🌟 最佳实践：可以根据问题类型动态选择分解策略，甚至组合使用多种策略。</p></blockquote><h3 id="第三部分：混合检索策略实现"><a href="#第三部分：混合检索策略实现" class="headerlink" title="第三部分：混合检索策略实现"></a>第三部分：混合检索策略实现</h3><h4 id="理解混合检索的价值"><a href="#理解混合检索的价值" class="headerlink" title="理解混合检索的价值"></a>理解混合检索的价值</h4><p>在实际应用中，单一的检索方法往往难以应对所有场景。例如：</p><ul><li>语义检索擅长理解上下文，但可能错过关键词</li><li>关键词检索准确度高，但缺乏语义理解</li><li>密集检索和稀疏检索各有优势</li></ul><p>因此，将多种检索方法结合起来，可以取长补短，提升整体检索效果。</p><h4 id="混合检索器的架构设计"><a href="#混合检索器的架构设计" class="headerlink" title="混合检索器的架构设计"></a>混合检索器的架构设计</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain.retrievers <span class="keyword">import</span> EnsembleRetriever</span><br><span class="line"><span class="keyword">from</span> langchain_community.retrievers <span class="keyword">import</span> BM25Retriever</span><br><span class="line"><span class="keyword">from</span> langchain_community.vectorstores <span class="keyword">import</span> FAISS</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 创建不同类型的检索器</span></span><br><span class="line"><span class="comment"># BM25检索器（基于关键词）</span></span><br><span class="line">bm25_retriever = BM25Retriever.from_documents(</span><br><span class="line">    documents, k=<span class="number">4</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># FAISS检索器（基于向量）</span></span><br><span class="line">faiss_retriever = FAISS.from_documents(</span><br><span class="line">    documents,</span><br><span class="line">    embedding=OpenAIEmbeddings(model=<span class="string">&quot;text-embedding-3-small&quot;</span>)</span><br><span class="line">).as_retriever(search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">4</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 创建集成检索器</span></span><br><span class="line">ensemble_retriever = EnsembleRetriever(</span><br><span class="line">    retrievers=[bm25_retriever, faiss_retriever],</span><br><span class="line">    weights=[<span class="number">0.5</span>, <span class="number">0.5</span>]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h4 id="主要检索方法的特点"><a href="#主要检索方法的特点" class="headerlink" title="主要检索方法的特点"></a>主要检索方法的特点</h4><p>下面是几种常用检索方法的对比：</p><table><thead><tr><th>检索方法</th><th>优势</th><th>适用场景</th><th>注意事项</th></tr></thead><tbody><tr><td>BM25</td><td>精确匹配，速度快</td><td>关键词搜索</td><td>不理解语义变化</td></tr><tr><td>向量检索</td><td>理解语义相似</td><td>概念搜索</td><td>计算资源消耗大</td></tr><tr><td>混合检索</td><td>综合优势</td><td>复杂查询</td><td>需要调整权重</td></tr></tbody></table><h4 id="实现细节和优化"><a href="#实现细节和优化" class="headerlink" title="实现细节和优化"></a>实现细节和优化</h4><p><strong>检索器配置</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置检索参数</span></span><br><span class="line">faiss_retriever = faiss_db.as_retriever(</span><br><span class="line">    search_kwargs=&#123;<span class="string">&quot;k&quot;</span>: <span class="number">4</span>&#125;</span><br><span class="line">).configurable_fields(</span><br><span class="line">    search_kwargs=ConfigurableField(</span><br><span class="line">        <span class="built_in">id</span>=<span class="string">&quot;search_kwargs_faiss&quot;</span>,</span><br><span class="line">        name=<span class="string">&quot;检索参数&quot;</span>,</span><br><span class="line">        description=<span class="string">&quot;设置检索的参数&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置运行时配置</span></span><br><span class="line">config = &#123;<span class="string">&quot;configurable&quot;</span>: &#123;<span class="string">&quot;search_kwargs_faiss&quot;</span>: &#123;<span class="string">&quot;k&quot;</span>: <span class="number">4</span>&#125;&#125;&#125;</span><br><span class="line">docs = ensemble_retriever.invoke(<span class="string">&quot;查询&quot;</span>, config=config)</span><br></pre></td></tr></table></figure><p><strong>权重调整策略</strong></p><ol><li><strong>初始设置</strong>：开始时可以给各检索器相同权重</li><li><strong>动态调整</strong>：根据查询类型动态调整权重</li><li><strong>性能监控</strong>：跟踪各检索器的表现，定期优化权重</li><li><strong>场景适配</strong>：针对不同领域调整最优权重组合</li></ol><h4 id="应用效果优化"><a href="#应用效果优化" class="headerlink" title="应用效果优化"></a>应用效果优化</h4><p>为了获得最佳检索效果，建议：</p><ol><li>检索器选择<ul><li>根据数据特点选择合适的检索器组合</li><li>考虑计算资源和响应时间的平衡</li><li>评估检索器的互补性</li></ul></li><li>参数优化<ul><li>使用验证集调整检索参数</li><li>监控检索质量指标</li><li>定期更新检索模型</li></ul></li><li>结果融合<ul><li>采用多样化的融合策略</li><li>考虑结果的去重和排序</li><li>平衡相关性和多样性</li></ul></li></ol><h4 id="性能监控与改进"><a href="#性能监控与改进" class="headerlink" title="性能监控与改进"></a>性能监控与改进</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 性能监控示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">evaluate_retrieval</span>(<span class="params">retriever, test_queries, ground_truth</span>):</span><br><span class="line">    metrics = &#123;</span><br><span class="line">        <span class="string">&#x27;precision&#x27;</span>: [],</span><br><span class="line">        <span class="string">&#x27;recall&#x27;</span>: [],</span><br><span class="line">        <span class="string">&#x27;latency&#x27;</span>: []</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> query, truth <span class="keyword">in</span> <span class="built_in">zip</span>(test_queries, ground_truth):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        results = retriever.get_relevant_documents(query)</span><br><span class="line">        latency = time.time() - start_time</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算评估指标</span></span><br><span class="line">        metrics[<span class="string">&#x27;latency&#x27;</span>].append(latency)</span><br><span class="line">        <span class="comment"># ... 计算precision和recall</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> metrics</span><br></pre></td></tr></table></figure><h3 id="总结：RAG优化策略的实践指南"><a href="#总结：RAG优化策略的实践指南" class="headerlink" title="总结：RAG优化策略的实践指南"></a>总结：RAG优化策略的实践指南</h3><h4 id="优化策略的综合比较"><a href="#优化策略的综合比较" class="headerlink" title="优化策略的综合比较"></a>优化策略的综合比较</h4><p>以下是我们讨论过的主要优化策略的特点对比：</p><table><thead><tr><th>优化策略</th><th>主要优势</th><th>实现复杂度</th><th>资源消耗</th><th>适用场景</th></tr></thead><tbody><tr><td>多查询检索</td><td>提高召回率</td><td>中等</td><td>中等</td><td>复杂查询、模糊问题</td></tr><tr><td>问题分解</td><td>提升理解深度</td><td>较高</td><td>较高</td><td>多维度分析问题</td></tr><tr><td>Step-Back</td><td>增强理解准确性</td><td>高</td><td>高</td><td>需要深入理解的问题</td></tr><tr><td>混合检索</td><td>综合性能提升</td><td>中等</td><td>较高</td><td>通用场景</td></tr></tbody></table><h4 id="优化路径建议"><a href="#优化路径建议" class="headerlink" title="优化路径建议"></a>优化路径建议</h4><ol><li><strong>基础阶段</strong><ul><li>实现基本的RAG流程</li><li>优化向量检索参数</li><li>改进提示词设计</li></ul></li><li><strong>进阶阶段</strong><ul><li>引入多查询策略</li><li>实现基本的问题分解</li><li>尝试混合检索方法</li></ul></li><li><strong>高级阶段</strong><ul><li>实现完整的Step-Back策略</li><li>优化多检索器集成</li><li>构建自适应检索系统</li></ul></li></ol><p><strong>场景选择指南</strong></p><p>根据不同的应用场景，推荐以下优化组合：</p><ol><li><strong>知识问答系统</strong><ul><li>多查询检索 + 混合检索</li><li>重点优化检索准确性</li></ul></li><li><strong>文档分析系统</strong><ul><li>问题分解 + Step-Back</li><li>强化深度理解能力</li></ul></li><li><strong>通用对话系统</strong><ul><li>混合检索 + 多查询</li><li>平衡效率和准确性</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Langchain </tag>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LangChain初入门</title>
      <link href="/2025/02/09/langchain-chu-ru-men/"/>
      <url>/2025/02/09/langchain-chu-ru-men/</url>
      
        <content type="html"><![CDATA[<h2 id="为什么选择LangChain"><a href="#为什么选择LangChain" class="headerlink" title="为什么选择LangChain"></a>为什么选择LangChain</h2><p>LangChain作为一个强大的框架，具有以下优势：</p><ul><li><p><strong>组件化和标准化</strong>：提供了标准化的接口来处理各种LLM，使开发更加灵活和可维护。</p></li><li><p><strong>丰富的工具集成</strong>：内置了大量工具和集成，可以轻松连接数据库、搜索引擎等外部服务。</p></li><li><p><strong>链式处理能力</strong>：可以将多个组件组合成链，实现复杂的处理流程。</p></li><li><p><strong>内存管理</strong>：提供了多种记忆组件，使应用能够保持上下文连贯性。</p></li></ul><h2 id="LangChain简介"><a href="#LangChain简介" class="headerlink" title="LangChain简介"></a>LangChain简介</h2><p>LangChain是一个用于开发由语言模型驱动的应用程序的框架。</p><h3 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h3><ul><li><p><strong>Models (模型)</strong>：提供与大语言模型的统一交互接口，支持各类LLM、聊天模型和文本嵌入模型的调用</p></li><li><p><strong>Prompts (提示)</strong>：专门用于管理和优化提示模板，提供标准化的提示工程工具</p></li><li><p><strong>Indexes (索引)</strong>：提供高效的文档加载、分割和向量存储系统，支持大规模文本处理和检索</p></li><li><p><strong>Memory (记忆)</strong>：用于在交互过程中管理和存储状态信息，确保对话的连贯性和上下文理解</p></li><li><p><strong>Chains (链)</strong>：能将多个组件组合成端到端应用的核心机制，实现复杂的处理流程</p></li><li><p><strong>Agents (代理)</strong>：赋予LLM使用工具的能力，支持自主推理和行动决策</p></li></ul><p><img src="/medias/featureimages/blog/langchain-chu-ru-men/image1.png" alt="image"></p><h4 id="Prompts组件"><a href="#Prompts组件" class="headerlink" title="Prompts组件"></a>Prompts组件</h4><p><strong>概念与作用</strong></p><p>在LLM应用开发中,我们通常不会直接将用户输入传递给大模型,而是会将用户输入添加到一个更大的文本片段中,这个文本片段被称为Prompt。Prompt为大模型提供了任务相关的上下文和指令,帮助模型更好地理解和执行任务。</p><p>LangChain中的Prompts组件提供了一系列工具来管理和优化这些提示模板。主要包含两大类:</p><ul><li><p>PromptTemplate: 将Prompt按照template进行格式化,处理变量和组合</p></li><li><p>Selectors: 根据不同条件选择不同的提示词</p></li></ul><p><strong>基本构成</strong></p><p>在LangChain中,Prompts组件包含多个子组件:</p><p>角色提示模板:</p><ul><li><p>SystemMessagePromptTemplate: 系统角色消息模板</p></li><li><p>HumanMessagePromptTemplate: 人类角色消息模板</p></li><li><p>AIMessagePromptTemplate: AI角色消息模板</p></li></ul><p>提示模板类型:</p><ul><li><p>PromptTemplate: 文本提示模板</p></li><li><p>ChatPromptTemplate: 聊天消息提示模板</p></li><li><p>MessagePlaceholder: 消息占位符</p></li></ul><p><strong>关键操作</strong></p><p>格式化LangChain支持两种格式化方式</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># f-string方式</span></span><br><span class="line">prompt = PromptTemplate.from_template(<span class="string">&quot;请将一个关于&#123;subject&#125;的笑话&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># jinja2方式</span></span><br><span class="line">prompt = PromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;请将一个关于&#123;&#123;subject&#125;&#125;的笑话&quot;</span>,</span><br><span class="line">    template_format=<span class="string">&quot;jinja2&quot;</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>提示模板拼接</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 字符串提示拼接</span></span><br><span class="line">prompt = (</span><br><span class="line">    PromptTemplate.from_template(<span class="string">&quot;请将一个关于&#123;subject&#125;的冷笑话&quot;</span>)</span><br><span class="line">    + <span class="string">&quot;，让我开心下&quot;</span></span><br><span class="line">    + <span class="string">&quot;\n使用&#123;language&#125;语言。&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 聊天提示拼接</span></span><br><span class="line">system_prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是OpenAI开发的聊天机器人，请根据用户的提问进行回复，我叫&#123;username&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line">human_prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;query&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line">prompt = system_prompt + human_prompt</span><br></pre></td></tr></table></figure><p><strong>模板复用</strong></p><p>对于复杂的提示模板,LangChain提供了PipelinePromptTemplate来实现模板的复用:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 描述提示模板</span></span><br><span class="line">instruction_template = <span class="string">&quot;你正在模拟&#123;person&#125;。&quot;</span></span><br><span class="line">instruction_prompt = PromptTemplate.from_template(instruction_template)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例提示模板</span></span><br><span class="line">example_template = <span class="string">&quot;&quot;&quot;下面是一个交互例子:</span></span><br><span class="line"><span class="string">Q: &#123;example_q&#125;</span></span><br><span class="line"><span class="string">A: &#123;example_a&#125;&quot;&quot;&quot;</span></span><br><span class="line">example_prompt = PromptTemplate.from_template(example_template)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 开始提示模板</span></span><br><span class="line">start_template = <span class="string">&quot;&quot;&quot;现在开始对话:</span></span><br><span class="line"><span class="string">Q: &#123;input&#125;</span></span><br><span class="line"><span class="string">A:&quot;&quot;&quot;</span></span><br><span class="line">start_prompt = PromptTemplate.from_template(start_template)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 组合模板</span></span><br><span class="line">pipeline_prompt = PipelinePromptTemplate(</span><br><span class="line">    final_prompt=full_prompt,</span><br><span class="line">    pipeline_prompts=[</span><br><span class="line">        (<span class="string">&quot;instruction&quot;</span>, instruction_prompt),</span><br><span class="line">        (<span class="string">&quot;example&quot;</span>, example_prompt),</span><br><span class="line">        (<span class="string">&quot;start&quot;</span>, start_prompt),</span><br><span class="line">    ]</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>最佳实践</strong></p><p>选择合适的格式化方式</p><ol><li><p>简单变量替换使用f-string</p></li><li><p>需要条件判断等复杂逻辑时使用jinja2</p></li></ol><p>提示模板设计</p><ol><li><p>保持模板的清晰和可维护性</p></li><li><p>合理使用系统消息和示例</p></li><li><p>避免过于复杂的嵌套结构</p></li></ol><p>错误处理</p><ol><li><p>验证必要的变量是否存在</p></li><li><p>处理格式化可能出现的异常</p></li></ol><p>性能优化</p><ol><li><p>重复使用的模板要缓存</p></li><li><p>避免不必要的模板拼接操作</p></li></ol><h4 id="Model组件"><a href="#Model组件" class="headerlink" title="Model组件"></a>Model组件</h4><p><strong>基本概念</strong></p><p>Models是LangChain的核心组件，提供了一个标准接口来封装不同类型的LLM进行交互，LangChain本身不提供LLM,而是提供了接口来集成各种模型。</p><p>LangChain支持两种类型的模型:</p><ul><li><p>LLM: 使用纯文本作为输入和输出的大语言模型</p></li><li><p>Chat Model: 使用聊天消息列表作为输入并返回聊天消息的聊天模型</p></li></ul><p><strong>组件架构</strong></p><p>LangChain中Models组件的基类结构如下:</p><p>BaseLanguageModel(基类)</p><ul><li><p>BaseLLM(大语言模型基类)</p><ul><li>SimpleLLM(简化大语言模型)</li><li>第三方LLM集成(OpenAI、百度文心等)</li></ul></li><li><p>BaseChatModel(聊天模型基类)</p><ul><li>SimpleChatModel(简化聊天模型)</li><li>第三方Chat Model集成</li></ul></li></ul><h4 id="Message组件"><a href="#Message组件" class="headerlink" title="Message组件"></a>Message组件</h4><ul><li><p>SystemMessage: 系统消息</p></li><li><p>HumanMessage: 人类消息</p></li><li><p>AIMessage: AI消息</p></li><li><p>FunctionMessage: 函数调用消息</p></li><li><p>ToolMessage: 工具调用消息</p></li></ul><p><strong>核心办法</strong></p><p>Models组件提供了几个关键方法:</p><p>invoke&#x2F;invoke_sync: 调用模型生成内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基本调用</span></span><br><span class="line">llm = ChatOpenAI(model=<span class="string">&quot;gpt-3.5-turbo-16k&quot;</span>)</span><br><span class="line">response = llm.invoke(<span class="string">&quot;你好!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 异步调用</span></span><br><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">generate</span>():</span><br><span class="line">    response = <span class="keyword">await</span> llm.ainvoke(<span class="string">&quot;你好!&quot;</span>)</span><br></pre></td></tr></table></figure><p>batch&#x2F;abatch: 批量调用处理多个输入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">messages = [</span><br><span class="line">    <span class="string">&quot;请讲一个关于程序员的笑话&quot;</span>,</span><br><span class="line">    <span class="string">&quot;请讲一个关于Python的笑话&quot;</span></span><br><span class="line">]</span><br><span class="line">responses = llm.batch(messages)</span><br></pre></td></tr></table></figure><p>stream&#x2F;astream: 流式返回生成内容</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">response = llm.stream(<span class="string">&quot;请介绍下LLM和LLMOps&quot;</span>)</span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> response:</span><br><span class="line">    <span class="built_in">print</span>(chunk.content, end=<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>Message组件使用</strong></p><p>消息组件用于构建与聊天模型的交互:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.messages <span class="keyword">import</span> SystemMessage, HumanMessage, AIMessage</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建消息</span></span><br><span class="line">system_msg = SystemMessage(content=<span class="string">&quot;你是一个AI助手&quot;</span>)</span><br><span class="line">human_msg = HumanMessage(content=<span class="string">&quot;你好!&quot;</span>)</span><br><span class="line">ai_msg = AIMessage(content=<span class="string">&quot;你好!我是AI助手&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建消息列表</span></span><br><span class="line">messages = [system_msg, human_msg, ai_msg]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用消息与模型交互</span></span><br><span class="line">response = chat_model.invoke(messages)</span><br></pre></td></tr></table></figure><p><strong>实践示例</strong></p><p>基本对话示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建聊天模型</span></span><br><span class="line">chat = ChatOpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建提示模板</span></span><br><span class="line">prompt = ChatPromptTemplate.from_messages([</span><br><span class="line">    (<span class="string">&quot;system&quot;</span>, <span class="string">&quot;你是一位&#123;role&#125;&quot;</span>),</span><br><span class="line">    (<span class="string">&quot;human&quot;</span>, <span class="string">&quot;&#123;query&#125;&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用模型</span></span><br><span class="line">response = chat.invoke(</span><br><span class="line">    prompt.format_messages(</span><br><span class="line">        role=<span class="string">&quot;Python专家&quot;</span>,</span><br><span class="line">        query=<span class="string">&quot;什么是装饰器?&quot;</span></span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>流式输出示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建提示模板</span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;&#123;subject&#125;的发展历史是什么?&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建模型</span></span><br><span class="line">llm = ChatOpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 流式生成</span></span><br><span class="line">response = llm.stream(</span><br><span class="line">    prompt.format_messages(subject=<span class="string">&quot;人工智能&quot;</span>)</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理输出</span></span><br><span class="line"><span class="keyword">for</span> chunk <span class="keyword">in</span> response:</span><br><span class="line">    <span class="built_in">print</span>(chunk.content, end=<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>最佳实践</strong></p><p>选择合适的模型类型</p><ol><li><p>简单文本生成任务使用LLM</p></li><li><p>对话类任务使用Chat Model</p></li></ol><p>正确处理异步操作</p><ol><li><p>在异步环境中使用ainvoke&#x2F;astream</p></li><li><p>批量处理时考虑使用batch</p></li></ol><p>异常处理</p><ol><li><p>处理模型调用可能的超时</p></li><li><p>捕获API错误并适当处理</p></li></ol><p>性能优化</p><ol><li><p>合理使用批处理</p></li><li><p>适时使用流式输出</p></li></ol><p><strong>OutputParser 解析器组件</strong></p><p>为什么需要输出解析器</p><p>在使用大模型时,我们经常会遇到输出解析的问题。比如:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">llm = ChatOpenAI()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例1: 返回的是自然语言</span></span><br><span class="line">llm.invoke(<span class="string">&quot;1+1等于几?&quot;</span>)  <span class="comment"># 输出: 1 + 1 等于 2。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例2: 包含多余信息</span></span><br><span class="line">llm.invoke(<span class="string">&quot;告诉我3个动物的名字。&quot;</span>)  <span class="comment"># 输出: 好的，这里有三种动物的名字：\n1. 狮子\n2. 大熊猫\n3. 斑马</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例3: 格式不统一</span></span><br><span class="line">llm.invoke(<span class="string">&quot;给我一个json数据,键为a和b&quot;</span>)  <span class="comment"># 输出: &#123;\n &quot;a&quot;: 10,\n &quot;b&quot;: 20\n&#125;</span></span><br></pre></td></tr></table></figure><p>OutputParser就是为了解决这些问题而设计的。它通过:</p><ol><li><p>预设提示 - 告诉LLM需要的输出格式</p></li><li><p>解析功能 - 将输出转换成指定格式</p></li></ol><p><strong>Parser类型详解</strong></p><p>Langchain 提供了多种Parser：</p><ol><li><p>基础Parser：</p><ul><li>StrOutputParser: 最简单的Parser,原样返回文本</li><li>BaseOutputParser: 所有Parser的基类</li><li>BaseLLMOutputParser: 专门用于LLM输出的基类</li></ul></li><li><p>格式化Parser：</p><ul><li>JsonOutputParser: 解析JSON格式输出</li><li>XMLOutputParser: 解析XML格式输出</li><li>PydanticOutputParser: 使用Pydantic模型解析输出</li></ul></li><li><p>列表类Parser：</p><ul><li>CommaSeparatedListOutputParser: 解析逗号分隔的列表</li><li>NumberedListOutputParser: 解析数字编号的列表</li></ul></li></ol><p><strong>实践示例</strong></p><ol><li>StrOutputParser使用：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建链</span></span><br><span class="line">chain = (</span><br><span class="line">    ChatPromptTemplate.from_template(<span class="string">&quot;&#123;query&#125;&quot;</span>)</span><br><span class="line">    | ChatOpenAI()</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用</span></span><br><span class="line">response = chain.invoke(&#123;<span class="string">&quot;query&quot;</span>: <span class="string">&quot;你好!&quot;</span>&#125;)</span><br></pre></td></tr></table></figure><ol start="2"><li>JsonOutputParser使用：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> JsonOutputParser</span><br><span class="line"><span class="keyword">from</span> langchain_core.pydantic_v1 <span class="keyword">import</span> BaseModel, Field</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义输出结构</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Joke</span>(<span class="title class_ inherited__">BaseModel</span>):</span><br><span class="line">    joke: <span class="built_in">str</span> = Field(description=<span class="string">&quot;回答用户的冷笑话&quot;</span>)</span><br><span class="line">    punchline: <span class="built_in">str</span> = Field(description=<span class="string">&quot;冷笑话的笑点&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Parser</span></span><br><span class="line">parser = JsonOutputParser(pydantic_object=Joke)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建提示模板</span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(</span><br><span class="line">    <span class="string">&quot;回答用户的问题。\n&#123;format_instructions&#125;\n&#123;query&#125;\n&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加格式说明</span></span><br><span class="line">prompt = prompt.partial(format_instructions=parser.get_format_instructions())</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建链</span></span><br><span class="line">chain = prompt | ChatOpenAI() | parser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line">response = chain.invoke(&#123;<span class="string">&quot;query&quot;</span>: <span class="string">&quot;请讲一个关于程序员的冷笑话&quot;</span>&#125;)</span><br></pre></td></tr></table></figure><p><strong>错误处理</strong></p><ol><li>解析失败的处理：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> OutputParserException</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    result = parser.parse(llm_output)</span><br><span class="line"><span class="keyword">except</span> OutputParserException <span class="keyword">as</span> e:</span><br><span class="line">    <span class="comment"># 处理解析错误</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;解析错误: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 可以选择重试或使用默认值</span></span><br></pre></td></tr></table></figure><ol start="2"><li>使用重试机制：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 可以配置回调来处理重试</span></span><br><span class="line"><span class="keyword">from</span> langchain_core.callbacks <span class="keyword">import</span> BaseCallbackHandler</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">RetryHandler</span>(<span class="title class_ inherited__">BaseCallbackHandler</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_retry</span>(<span class="params">self, retry_state</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;重试次数: <span class="subst">&#123;retry_state.attempt_number&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>最佳实践</strong></p><ol><li><p>选择合适的Parser</p><ul><li>简单文本使用StrOutputParser</li><li>结构化数据使用JsonOutputParser或PydanticOutputParser</li><li>列表数据使用专门的列表Parser</li></ul></li><li><p>提示设计</p><ul><li>在提示中明确指定输出格式</li><li>使用Parser提供的format_instructions</li></ul></li><li><p>异常处理</p><ul><li>总是处理可能的解析错误</li><li>考虑添加重试机制</li><li>提供合理的默认值</li></ul></li><li><p>性能优化</p><ul><li>避免过于复杂的解析逻辑</li><li>合理使用缓存</li></ul></li></ol><h4 id="LCEL表达式与Runnable协议"><a href="#LCEL表达式与Runnable协议" class="headerlink" title="LCEL表达式与Runnable协议"></a>LCEL表达式与Runnable协议</h4><p><strong>为什么需要LCEL</strong></p><p>传统的链式调用方式存在嵌套问题：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">content = parser.invoke(</span><br><span class="line">    llm.invoke(</span><br><span class="line">        prompt.invoke(</span><br><span class="line">            &#123;<span class="string">&quot;query&quot;</span>: req.query.data&#125;</span><br><span class="line">        )</span><br><span class="line">    )</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>LCEL 提供了更优雅的方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chain = prompt | llm | parser</span><br><span class="line">content = chain.invoke(&#123;<span class="string">&quot;query&quot;</span>: req.query.data&#125;)</span><br></pre></td></tr></table></figure><p><strong>Runnable协议核心方法</strong></p><ul><li><p>invoke&#x2F;ainvoke: 调用组件</p></li><li><p>batch&#x2F;abatch: 批量处理</p></li><li><p>stream&#x2F;astream: 流式输出</p></li><li><p>transform: 转换输入输出</p></li></ul><p><strong>两个核心类</strong></p><ol><li>RunnableParallel - 并行执行多个Runnable</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnableParallel</span><br><span class="line"></span><br><span class="line"><span class="comment"># 并行执行多个链</span></span><br><span class="line">chain = RunnableParallel(</span><br><span class="line">    joke=joke_chain,</span><br><span class="line">    poem=poem_chain</span><br><span class="line">)</span><br><span class="line">resp = chain.invoke(&#123;<span class="string">&quot;subject&quot;</span>: <span class="string">&quot;程序员&quot;</span>&#125;)</span><br></pre></td></tr></table></figure><ol start="2"><li>RunnablePassthrough - 传递数据</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.runnables <span class="keyword">import</span> RunnablePassthrough</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建检索链</span></span><br><span class="line">chain = (</span><br><span class="line">    RunnablePassthrough.assign(</span><br><span class="line">        context=<span class="keyword">lambda</span> query: retrieval(query)</span><br><span class="line">    )</span><br><span class="line">    | prompt </span><br><span class="line">    | llm </span><br><span class="line">    | parser</span><br><span class="line">)</span><br></pre></td></tr></table></figure><p><strong>实践示例</strong></p><ol><li>基础链构建：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.prompts <span class="keyword">import</span> ChatPromptTemplate</span><br><span class="line"><span class="keyword">from</span> langchain_openai <span class="keyword">import</span> ChatOpenAI</span><br><span class="line"><span class="keyword">from</span> langchain_core.output_parsers <span class="keyword">import</span> StrOutputParser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建组件</span></span><br><span class="line">prompt = ChatPromptTemplate.from_template(<span class="string">&quot;&#123;input&#125;&quot;</span>)</span><br><span class="line">llm = ChatOpenAI()</span><br><span class="line">parser = StrOutputParser()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建链</span></span><br><span class="line">chain = prompt | llm | parser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">response = chain.invoke(&#123;<span class="string">&quot;input&quot;</span>: <span class="string">&quot;Hello!&quot;</span>&#125;)</span><br></pre></td></tr></table></figure><ol start="2"><li>带检索的链：</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">retrieval</span>(<span class="params">query: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;相关文档内容...&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建链</span></span><br><span class="line">chain = (</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="string">&quot;context&quot;</span>: retrieval,</span><br><span class="line">        <span class="string">&quot;question&quot;</span>: RunnablePassthrough()</span><br><span class="line">    &#125;</span><br><span class="line">    | prompt</span><br><span class="line">    | llm</span><br><span class="line">    | StrOutputParser()</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 执行</span></span><br><span class="line">response = chain.invoke(<span class="string">&quot;问题&quot;</span>)</span><br></pre></td></tr></table></figure><p><strong>最佳实践</strong></p><ol><li><p>链的设计</p><ul><li>使用管道操作符(|)构建简单链</li><li>复杂逻辑使用RunnableParallel</li><li>数据传递用RunnablePassthrough</li></ul></li><li><p>错误处理</p><ul><li>合理使用try&#x2F;except</li><li>实现错误回调处理</li></ul></li><li><p>性能优化</p><ul><li>合适场景使用并行执行</li><li>批处理代替单个处理</li></ul></li><li><p>代码可维护性</p><ul><li>链结构保持清晰</li><li>适当拆分复杂链</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Langchain </tag>
            
            <tag> LLM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LangChain RAG 应用开发组件深度解析</title>
      <link href="/2025/02/09/langchain-rag-ying-yong-kai-fa-zu-jian-shen-du-jie-xi/"/>
      <url>/2025/02/09/langchain-rag-ying-yong-kai-fa-zu-jian-shen-du-jie-xi/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>在当今的AI应用开发中，检索增强生成(Retrieval-Augmented Generation, RAG)已经成为一种重要的技术范式。它通过将大语言模型与外部知识库结合，极大地提升了AI系统的知识获取能力和输出质量。而LangChain作为一个强大的框架，为RAG应用的开发提供了丰富的组件支持。本文将深入剖析LangChain中RAG应用开发的核心组件，帮助你更好地理解和使用这些工具。</p><h2 id="核心组件概览"><a href="#核心组件概览" class="headerlink" title="核心组件概览"></a>核心组件概览</h2><p>在开始深入学习之前,我们先来了解LangChain中RAG应用开发涉及的主要组件:</p><ol><li>Document组件与文档加载器 - 负责文档的加载和基础处理</li><li>文档转换器与分割器 - 处理文档转换和分块</li><li>VectorStore组件 - 实现向量存储和检索</li><li>Blob相关组件 - 处理二进制大对象数据</li></ol><h3 id="Document-组件与文档加载器详解"><a href="#Document-组件与文档加载器详解" class="headerlink" title="Document 组件与文档加载器详解"></a>Document 组件与文档加载器详解</h3><h4 id="Document-组件基础"><a href="#Document-组件基础" class="headerlink" title="Document 组件基础"></a>Document 组件基础</h4><p>Document 是 LangChain 的核心组件之一，它定义了一个通用的文档结构，包含两个基本要素：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Document = page_content(页面内容) + metadata(元数据)</span><br></pre></td></tr></table></figure><p>这种结构允许我们统一处理各种类型的文档，同时保留文档的元信息。</p><h4 id="文档加载器类型"><a href="#文档加载器类型" class="headerlink" title="文档加载器类型"></a>文档加载器类型</h4><p>LangChain 提供了多种文档加载器：</p><ol><li>通用文本加载器</li><li>CSV文件加载器</li><li>HTML网页加载器</li><li>PDF文档加载器</li><li>Markdown文档加载器</li></ol><p><img src="/medias/featureimages/blog/langchain-rag-ying-yong-kai-fa-zu-jian-shen-du-jie-xi/image1.png" alt="image"></p><p>每种加载器都专门处理特定类型的文档，但它们都会将文档转换成统一的Document格式。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders <span class="keyword">import</span> TextLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载文本文件示例</span></span><br><span class="line">loader = TextLoader(<span class="string">&quot;./data.txt&quot;</span>, encoding=<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">documents = loader.load()</span><br></pre></td></tr></table></figure><h4 id="异步加载支持"><a href="#异步加载支持" class="headerlink" title="异步加载支持"></a>异步加载支持</h4><p>对于大型文档，LangChain提供了异步加载方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">load_documents</span>():</span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">with</span> aiofiles.<span class="built_in">open</span>(file_path, encoding=<span class="string">&quot;utf-8&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="comment"># 异步处理文档</span></span><br><span class="line">        <span class="keyword">yield</span> Document(</span><br><span class="line">            page_content=line,</span><br><span class="line">            metadata=&#123;<span class="string">&quot;source&quot;</span>: file_path&#125;</span><br><span class="line">        )</span><br></pre></td></tr></table></figure><h3 id="文档转换器与分割器"><a href="#文档转换器与分割器" class="headerlink" title="文档转换器与分割器"></a>文档转换器与分割器</h3><h4 id="DocumentTransformer-组件"><a href="#DocumentTransformer-组件" class="headerlink" title="DocumentTransformer 组件"></a>DocumentTransformer 组件</h4><p>文档转换器用于处理以下常见问题：</p><ol><li>文档太大导致的性能问题</li><li>原始文档格式不符合要求</li><li>文档内容需要标准化处理</li></ol><h4 id="文档转换器的工作原理"><a href="#文档转换器的工作原理" class="headerlink" title="文档转换器的工作原理"></a>文档转换器的工作原理</h4><p>DocumentTransformer组件的主要职责是对文档进行各种转换操作，包括：</p><ol><li>文档切割</li><li>文档层级提取</li><li>文档翻译</li><li>HTML标签处理</li><li>重排等多个功能</li></ol><p>在LangChain中，所有文档转换器都继承自BaseDocumentTransformer基类，它提供了两个核心方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BaseDocumentTransformer</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">transform_documents</span>(<span class="params">self</span>): </span><br><span class="line">        <span class="comment"># 转换文档列表</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">async</span> <span class="keyword">def</span> <span class="title function_">atransform_documents</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 异步转换处理</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><h3 id="文档分割器详解"><a href="#文档分割器详解" class="headerlink" title="文档分割器详解"></a>文档分割器详解</h3><h4 id="字符分割器（CharacterTextSplitter）"><a href="#字符分割器（CharacterTextSplitter）" class="headerlink" title="字符分割器（CharacterTextSplitter）"></a>字符分割器（CharacterTextSplitter）</h4><p>CharacterTextSplitter 是基础的分割器，它有以下重要参数：</p><ol><li><code>separator</code>: 分割符,默认为’\n\n’</li><li><code>chunk_size</code>: 每块文本的最大大小,默认4000</li><li><code>chunk_overlap</code>: 块与块之间的重叠大小,默认200</li><li><code>length_function</code>: 计算文本长度的函数,默认len</li><li><code>keep_separator</code>: 是否在分割的块中保留分隔符</li></ol><p>使用示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_text_splitters <span class="keyword">import</span> CharacterTextSplitter</span><br><span class="line"></span><br><span class="line">text_splitter = CharacterTextSplitter(</span><br><span class="line">    separator=<span class="string">&quot;\n\n&quot;</span>,</span><br><span class="line">    chunk_size=<span class="number">500</span>,</span><br><span class="line">    chunk_overlap=<span class="number">50</span>,</span><br><span class="line">    add_start_index=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用分割器处理文档</span></span><br><span class="line">splits = text_splitter.split_documents(documents)</span><br></pre></td></tr></table></figure><h4 id="实践建议"><a href="#实践建议" class="headerlink" title="实践建议"></a>实践建议</h4><p>在实际应用中，有以下几点建议：</p><ol><li>选择合适的chunk_size<ul><li>太大会影响处理效率</li><li>太小可能破坏语义完整性</li><li>建议根据实际需求在400-1000之间调整</li></ul></li><li>合理设置overlap<ul><li>设置适当的重叠可以保持上下文连贯</li><li>通常设置为chunk_size的10%-20%</li></ul></li><li>注意分隔符的选择<ul><li>根据文档类型选择合适的分隔符</li><li>可以使用多级分隔符策略</li></ul></li></ol><h3 id="VectorStore组件与检索器"><a href="#VectorStore组件与检索器" class="headerlink" title="VectorStore组件与检索器"></a>VectorStore组件与检索器</h3><h4 id="VectorStore基础概念"><a href="#VectorStore基础概念" class="headerlink" title="VectorStore基础概念"></a>VectorStore基础概念</h4><p>VectorStore组件负责：</p><ol><li>存储文档的向量表示</li><li>提供相似性检索功能</li><li>支持不同的向量检索策略</li></ol><h4 id="检索器的使用"><a href="#检索器的使用" class="headerlink" title="检索器的使用"></a>检索器的使用</h4><p>LangChain 提供了多种检索策略：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain <span class="keyword">import</span> VectorStore</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基础相似性检索</span></span><br><span class="line">results = vectorstore.similarity_search(query)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 带相似度分数的检索</span></span><br><span class="line">results = vectorstore.similarity_search_with_score(query)</span><br><span class="line"></span><br><span class="line"><span class="comment"># MMR检索策略</span></span><br><span class="line">results = vectorstore.max_marginal_relevance_search(query)</span><br></pre></td></tr></table></figure><h3 id="VectorStore实现细节"><a href="#VectorStore实现细节" class="headerlink" title="VectorStore实现细节"></a>VectorStore实现细节</h3><h4 id="支持的向量数据库"><a href="#支持的向量数据库" class="headerlink" title="支持的向量数据库"></a>支持的向量数据库</h4><ul><li>Chroma</li><li>FAISS</li><li>Pinecone</li><li>Milvus</li></ul><h4 id="检索策略详解"><a href="#检索策略详解" class="headerlink" title="检索策略详解"></a>检索策略详解</h4><ol><li>相似度检索(Similarity Search)<ul><li>基于余弦相似度</li><li>支持Top-K检索</li></ul></li><li>MMR检索(Maximum Marginal Relevance)<ul><li>平衡相关性和多样性</li><li>可配置lambda参数调整权重</li></ul></li><li>混合检索策略<ul><li>关键词+语义检索</li><li>支持自定义评分函数</li></ul></li></ol><h3 id="Blob与BlobParser组件"><a href="#Blob与BlobParser组件" class="headerlink" title="Blob与BlobParser组件"></a>Blob与BlobParser组件</h3><h4 id="Blob方案介绍"><a href="#Blob方案介绍" class="headerlink" title="Blob方案介绍"></a>Blob方案介绍</h4><p>Blob是LangChain处理二进制数据的解决方案，它具有以下特点：</p><ol><li>支持存储字节流数据</li><li>提供统一的数据访问接口</li><li>灵活的元数据管理</li></ol><p>基本使用示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.document_loaders <span class="keyword">import</span> Blob</span><br><span class="line"><span class="keyword">from</span> langchain_core.document_loaders.base <span class="keyword">import</span> BaseBlobParser</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建Blob对象</span></span><br><span class="line">blob = Blob.from_path(<span class="string">&quot;./data.txt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用解析器</span></span><br><span class="line">parser = CustomParser()</span><br><span class="line">documents = <span class="built_in">list</span>(parser.lazy_parse(blob))</span><br></pre></td></tr></table></figure><h4 id="Blob数据存储类详解"><a href="#Blob数据存储类详解" class="headerlink" title="Blob数据存储类详解"></a>Blob数据存储类详解</h4><p>LangChain中的Blob数据存储提供了丰富的属性和方法，让我们详细了解一下：</p><p><strong>核心属性</strong></p><ol><li><strong>data</strong>: 原始数据，支持存储字节，字符串数据</li><li><strong>mimetype</strong>: 文件的mimetype类型</li><li><strong>encoding</strong>: 文件的编码，默认utf-8</li><li><strong>path</strong>: 文件的原始路径</li><li><strong>metadata</strong>: 存储的元数据，通常包含source字段</li></ol><p><strong>常用方法</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 字符串转换</span></span><br><span class="line">as_string(): <span class="comment"># 将数据转换为字符串</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 字节转换</span></span><br><span class="line">as_bytes(): <span class="comment"># 将数据转换为字节数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 字节流操作</span></span><br><span class="line">as_bytes_io(): <span class="comment"># 将数据转换为字节流</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从路径加载</span></span><br><span class="line">from_path(): <span class="comment"># 从文件路径加载Blob数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 从原始数据加载</span></span><br><span class="line">from_data(): <span class="comment"># 从原始数据加载Blob数据</span></span><br></pre></td></tr></table></figure><h4 id="BlobLoader实现"><a href="#BlobLoader实现" class="headerlink" title="BlobLoader实现"></a>BlobLoader实现</h4><p>BlobLoader是一个抽象接口，用于实现二进制数据的加载。以下是一个自定义BlobLoader的示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_core.document_loaders <span class="keyword">import</span> Blob</span><br><span class="line"><span class="keyword">from</span> langchain_core.document_loaders.base <span class="keyword">import</span> BaseBlobParser</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomBlobLoader</span>(<span class="title class_ inherited__">ABC</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;自定义Blob加载器实现&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @abstractmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">yield_blobs</span>(<span class="params"></span></span><br><span class="line"><span class="params">        self,</span></span><br><span class="line"><span class="params">    </span>) -&gt; Iterable[Blob]:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;加载并返回Blob数据流&quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, file_path: <span class="built_in">str</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.file_path = file_path</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">lazy_load</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;延迟加载实现&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">for</span> blob <span class="keyword">in</span> <span class="variable language_">self</span>.yield_blobs():</span><br><span class="line">            <span class="keyword">yield</span> Document(</span><br><span class="line">                page_content=blob.as_string(),</span><br><span class="line">                metadata=&#123;<span class="string">&quot;source&quot;</span>: blob.source&#125;</span><br><span class="line">            )</span><br></pre></td></tr></table></figure><h4 id="通用加载器使用最佳实践"><a href="#通用加载器使用最佳实践" class="headerlink" title="通用加载器使用最佳实践"></a>通用加载器使用最佳实践</h4><p>GenericLoader是LangChain提供的一个通用加载器，它结合了BlobLoader和BaseBlobParser的功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> langchain_community.document_loaders.generic <span class="keyword">import</span> GenericLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建通用加载器</span></span><br><span class="line">loader = GenericLoader.from_filesystem(</span><br><span class="line">    <span class="string">&quot;./&quot;</span>,  <span class="comment"># 文件系统路径</span></span><br><span class="line">    glob=<span class="string">&quot;*.txt&quot;</span>,  <span class="comment"># 文件匹配模式</span></span><br><span class="line">    show_progress=<span class="literal">True</span>  <span class="comment"># 显示进度</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用加载器</span></span><br><span class="line"><span class="keyword">for</span> idx, doc <span class="keyword">in</span> <span class="built_in">enumerate</span>(loader.lazy_load()):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;当前加载第<span class="subst">&#123;idx + <span class="number">1</span>&#125;</span>个文件, 文件信息:<span class="subst">&#123;doc.metadata&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="性能优化建议"><a href="#性能优化建议" class="headerlink" title="性能优化建议"></a>性能优化建议</h4><ol><li>使用延迟加载<ul><li>对于大文件优先使用lazy_load()</li><li>避免一次性加载全部内容</li></ul></li><li>合理配置缓存<ul><li>利用缓存减少重复加载</li><li>及时清理不需要的缓存</li></ul></li><li>错误处理<ul><li>实现适当的错误处理机制</li><li>记录加载过程中的异常</li></ul></li><li>进度监控<ul><li>对大规模数据处理添加进度显示</li><li>实现断点续传机制</li></ul></li></ol>]]></content>
      
      
      <categories>
          
          <category> AI </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Langchain </tag>
            
            <tag> RAG </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
